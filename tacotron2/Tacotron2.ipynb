{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tacotron2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1O0VCf7SiWZ1VeM7Ain8pEEPfI1yS05Sb","authorship_tag":"ABX9TyPEeiEaZrEEXXh9mUg3WnuF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwco3Qsdteon","executionInfo":{"status":"ok","timestamp":1612831591045,"user_tz":-540,"elapsed":1064,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"e8ef9bd5-692b-47c2-9437-ffcca298d062"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"2RlHglHEtpHN","executionInfo":{"status":"ok","timestamp":1612831591880,"user_tz":-540,"elapsed":1891,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"d391995e-69c3-4b41-b3ee-aafa355791ed"},"source":["import os \r\n","\r\n","os.getcwd()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/AI대학원/tacotron2'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFkeYe1f7DaM","executionInfo":{"status":"ok","timestamp":1612831591881,"user_tz":-540,"elapsed":1886,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"f4fb4926-7772-47ea-849a-8f6d5ae1ad61"},"source":["ls"],"execution_count":13,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mdata\u001b[0m/        \u001b[01;34mlogdir-tacotron2\u001b[0m/  \u001b[01;34msamples\u001b[0m/           \u001b[01;34mtext\u001b[0m/               \u001b[01;34mwavenet\u001b[0m/\n","\u001b[01;34mdatasets\u001b[0m/    \u001b[01;34mlogdir-wavenet\u001b[0m/    synthesizer.py     train_tacotron2.py\n","generate.py  preprocess.py      \u001b[01;34mtacotron2\u001b[0m/         train_vocoder.py\n","hparams.py   \u001b[01;34m__pycache__\u001b[0m/       Tacotron2_1.ipynb  명령어모음.txt\n","LICENSE      ReadMe.md          Tacotron2.ipynb    \u001b[01;34mutils\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YD9QwJARtsoM","executionInfo":{"status":"ok","timestamp":1612831591881,"user_tz":-540,"elapsed":1879,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"9293eeae-1213-4fd0-c514-41170f9f8072"},"source":["cd drive/MyDrive/AI대학원/tacotron2"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'drive/MyDrive/AI대학원/tacotron2'\n","/content/drive/MyDrive/AI대학원/tacotron2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":607},"id":"UvtxCfjVtxfK","executionInfo":{"status":"ok","timestamp":1612831596062,"user_tz":-540,"elapsed":5933,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"899f1217-ebb2-4494-b3c8-715bc8589f85"},"source":["pip install tensorflow-gpu==1.8"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-gpu==1.8 in /usr/local/lib/python3.6/dist-packages (1.8.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (0.3.3)\n","Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (3.12.4)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (0.36.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (1.32.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (1.19.5)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (0.8.1)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (0.10.0)\n","Collecting tensorboard<1.9.0,>=1.8.0\n","  Using cached https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (1.1.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-gpu==1.8) (53.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (3.3.3)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (1.0.1)\n","Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (1.5.0)\n","Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (0.9999999)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (3.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (3.7.4.3)\n","\u001b[31mERROR: tensorflow 1.15.2 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 1.15.2 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.8.0 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboard\n","  Found existing installation: tensorboard 1.15.0\n","    Uninstalling tensorboard-1.15.0:\n","      Successfully uninstalled tensorboard-1.15.0\n","Successfully installed tensorboard-1.8.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["tensorboard"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"k8LgUO8Tt2Cx","executionInfo":{"status":"ok","timestamp":1612831608508,"user_tz":-540,"elapsed":1866,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}}},"source":["%tensorflow_version 1.x"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"veQz2fH2t3nP","executionInfo":{"status":"ok","timestamp":1612831608510,"user_tz":-540,"elapsed":1609,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"7ec67dee-c391-4578-ed4c-f805b245cc2e"},"source":["import tensorflow as tf\r\n","print(tf.__version__)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["1.15.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUMPQ1ydt5aw","executionInfo":{"status":"ok","timestamp":1612831614791,"user_tz":-540,"elapsed":7095,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"f28840f9-966f-49c7-a53f-21583499bd81"},"source":["!pip install jamo\r\n","!pip install Unidecode"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: jamo in /usr/local/lib/python3.6/dist-packages (0.4.1)\n","Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1WMBMnH5t7fN","executionInfo":{"status":"ok","timestamp":1612655319170,"user_tz":-540,"elapsed":29647,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"c176ad1d-e0d3-4e7d-9652-87a67ced796f"},"source":["!python preprocess.py --num_workers 8 --name son --in_dir ./datasets/son --out_dir ./data/son"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","Hyperparameters:\n","  adam_beta1: 0.9\n","  adam_beta2: 0.999\n","  allow_clipping_in_normalization: True\n","  attention_dim: 128\n","  attention_filters: 32\n","  attention_kernel: (31,)\n","  attention_size: 128\n","  attention_type: bah_mon_norm\n","  attention_win_size: 7\n","  cleaners: korean_cleaners\n","  clip_mels_length: True\n","  cumulative_weights: True\n","  dec_prenet_sizes: [256, 256]\n","  decoder_layers: 2\n","  decoder_lstm_units: 1024\n","  dilation_channels: 256\n","  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n","  dropout_prob: 0.5\n","  embedding_size: 512\n","  enc_conv_channels: 512\n","  enc_conv_kernel_size: 5\n","  enc_conv_num_layers: 3\n","  encoder_lstm_units: 256\n","  fft_size: 2048\n","  filter_width: 3\n","  gc_channels: 32\n","  griffin_lim_iters: 60\n","  hop_size: 300\n","  inference_prenet_dropout: True\n","  initial_data_greedy: True\n","  initial_phase_step: 8000\n","  input_type: raw\n","  l2_regularization_strength: 0\n","  legacy: True\n","  main_data: ['']\n","  main_data_greedy_factor: 0\n","  mask_encoder: True\n","  max_abs_value: 4.0\n","  max_checkpoints: 3\n","  max_mel_frames: 1000\n","  max_n_frame: 1000\n","  min_level_db: -100\n","  min_n_frame: 150\n","  min_tokens: 30\n","  model_type: multi-speaker\n","  momentum: 0.9\n","  name: Tacotron-2\n","  num_mels: 80\n","  num_steps: 1000000\n","  optimizer: adam\n","  out_channels: 30\n","  post_bank_channel_size: 128\n","  post_bank_size: 8\n","  post_highway_depth: 4\n","  post_maxpool_width: 2\n","  post_proj_sizes: [256, 80]\n","  post_proj_width: 3\n","  post_rnn_size: 128\n","  postnet_channels: 512\n","  postnet_kernel_size: (5,)\n","  postnet_num_layers: 5\n","  power: 1.5\n","  preemphasis: 0.97\n","  preemphasize: True\n","  prenet_layers: [256, 256]\n","  prioritize_loss: False\n","  quantization_channels: 256\n","  reduction_factor: 2\n","  ref_level_db: 20\n","  rescaling: True\n","  rescaling_max: 0.999\n","  residual_channels: 128\n","  residual_legacy: True\n","  sample_rate: 24000\n","  sample_size: 9000\n","  scalar_input: True\n","  signal_normalization: True\n","  silence_threshold: 0\n","  skip_channels: 128\n","  skip_inadequate: False\n","  skip_path_filter: False\n","  smoothing: False\n","  speaker_embedding_size: 16\n","  store_metadata: False\n","  symmetric_mels: True\n","  synthesis_constraint: False\n","  synthesis_constraint_type: window\n","  tacotron_decay_learning_rate: True\n","  tacotron_decay_rate: 0.5\n","  tacotron_decay_steps: 18000\n","  tacotron_final_learning_rate: 0.0001\n","  tacotron_initial_learning_rate: 0.001\n","  tacotron_reg_weight: 1e-06\n","  tacotron_start_decay: 40000\n","  tacotron_zoneout_rate: 0.1\n","  trim_fft_size: 512\n","  trim_hop_size: 128\n","  trim_silence: True\n","  trim_top_db: 23\n","  upsample_factor: [12, 25]\n","  upsample_type: SubPixel\n","  use_biases: True\n","  use_lws: False\n","  wavenet_batch_size: 2\n","  wavenet_clip_gradients: True\n","  wavenet_decay_rate: 0.5\n","  wavenet_decay_steps: 300000\n","  wavenet_dropout: 0.05\n","  wavenet_learning_rate: 0.001\n","  win_size: 1200\n","Sampling frequency: 24000\n","100% 50/50 [00:15<00:00,  3.16it/s]\n","Write 50 utterances, 15036 mel frames, 4510800 audio timesteps, (0.05 hours)\n","Max input length (text chars): 83\n","Max mel frames length: 827\n","Max audio timesteps length: 248100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFClNf58uWTR","executionInfo":{"status":"ok","timestamp":1612655397332,"user_tz":-540,"elapsed":29851,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"631f3e8d-01b3-4bbc-d298-f1277443a1cb"},"source":["!python preprocess.py --num_workers 8 --name moon --in_dir ./datasets/moon --out_dir ./data/moon"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","Hyperparameters:\n","  adam_beta1: 0.9\n","  adam_beta2: 0.999\n","  allow_clipping_in_normalization: True\n","  attention_dim: 128\n","  attention_filters: 32\n","  attention_kernel: (31,)\n","  attention_size: 128\n","  attention_type: bah_mon_norm\n","  attention_win_size: 7\n","  cleaners: korean_cleaners\n","  clip_mels_length: True\n","  cumulative_weights: True\n","  dec_prenet_sizes: [256, 256]\n","  decoder_layers: 2\n","  decoder_lstm_units: 1024\n","  dilation_channels: 256\n","  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n","  dropout_prob: 0.5\n","  embedding_size: 512\n","  enc_conv_channels: 512\n","  enc_conv_kernel_size: 5\n","  enc_conv_num_layers: 3\n","  encoder_lstm_units: 256\n","  fft_size: 2048\n","  filter_width: 3\n","  gc_channels: 32\n","  griffin_lim_iters: 60\n","  hop_size: 300\n","  inference_prenet_dropout: True\n","  initial_data_greedy: True\n","  initial_phase_step: 8000\n","  input_type: raw\n","  l2_regularization_strength: 0\n","  legacy: True\n","  main_data: ['']\n","  main_data_greedy_factor: 0\n","  mask_encoder: True\n","  max_abs_value: 4.0\n","  max_checkpoints: 3\n","  max_mel_frames: 1000\n","  max_n_frame: 1000\n","  min_level_db: -100\n","  min_n_frame: 150\n","  min_tokens: 30\n","  model_type: multi-speaker\n","  momentum: 0.9\n","  name: Tacotron-2\n","  num_mels: 80\n","  num_steps: 1000000\n","  optimizer: adam\n","  out_channels: 30\n","  post_bank_channel_size: 128\n","  post_bank_size: 8\n","  post_highway_depth: 4\n","  post_maxpool_width: 2\n","  post_proj_sizes: [256, 80]\n","  post_proj_width: 3\n","  post_rnn_size: 128\n","  postnet_channels: 512\n","  postnet_kernel_size: (5,)\n","  postnet_num_layers: 5\n","  power: 1.5\n","  preemphasis: 0.97\n","  preemphasize: True\n","  prenet_layers: [256, 256]\n","  prioritize_loss: False\n","  quantization_channels: 256\n","  reduction_factor: 2\n","  ref_level_db: 20\n","  rescaling: True\n","  rescaling_max: 0.999\n","  residual_channels: 128\n","  residual_legacy: True\n","  sample_rate: 24000\n","  sample_size: 9000\n","  scalar_input: True\n","  signal_normalization: True\n","  silence_threshold: 0\n","  skip_channels: 128\n","  skip_inadequate: False\n","  skip_path_filter: False\n","  smoothing: False\n","  speaker_embedding_size: 16\n","  store_metadata: False\n","  symmetric_mels: True\n","  synthesis_constraint: False\n","  synthesis_constraint_type: window\n","  tacotron_decay_learning_rate: True\n","  tacotron_decay_rate: 0.5\n","  tacotron_decay_steps: 18000\n","  tacotron_final_learning_rate: 0.0001\n","  tacotron_initial_learning_rate: 0.001\n","  tacotron_reg_weight: 1e-06\n","  tacotron_start_decay: 40000\n","  tacotron_zoneout_rate: 0.1\n","  trim_fft_size: 512\n","  trim_hop_size: 128\n","  trim_silence: True\n","  trim_top_db: 23\n","  upsample_factor: [12, 25]\n","  upsample_type: SubPixel\n","  use_biases: True\n","  use_lws: False\n","  wavenet_batch_size: 2\n","  wavenet_clip_gradients: True\n","  wavenet_decay_rate: 0.5\n","  wavenet_decay_steps: 300000\n","  wavenet_dropout: 0.05\n","  wavenet_learning_rate: 0.001\n","  win_size: 1200\n","Sampling frequency: 24000\n","100% 109/109 [00:24<00:00,  4.45it/s]\n","Write 109 utterances, 13624 mel frames, 4087200 audio timesteps, (0.05 hours)\n","Max input length (text chars): 25\n","Max mel frames length: 340\n","Max audio timesteps length: 102000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AqCliCTrt_Sh","executionInfo":{"status":"ok","timestamp":1612831720726,"user_tz":-540,"elapsed":70855,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"79cf24bf-3b27-43d7-aa88-33fb03514ca5"},"source":["!python train_tacotron2.py --data_paths ./data/son --batch_size 32"],"execution_count":19,"outputs":[{"output_type":"stream","text":["WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From train_tacotron2.py:27: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n","\n","WARNING:tensorflow:From train_tacotron2.py:27: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n","\n","WARNING:tensorflow:From train_tacotron2.py:27: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n","\n","WARNING:tensorflow:From train_tacotron2.py:27: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n","\n"," [*] MODEL dir: logdir-tacotron2/son_2021-02-09_00-47-34\n"," [*] PARAM path: logdir-tacotron2/son_2021-02-09_00-47-34/params.json\n","['./data/son']\n","==================================================\n","==================================================\n"," [*] Checkpoint path: logdir-tacotron2/son_2021-02-09_00-47-34/model.ckpt\n"," [*] Loading training data from: ['./data/son']\n"," [*] Using model: logdir-tacotron2/son_2021-02-09_00-47-34\n","Hyperparameters:\n","  adam_beta1: 0.9\n","  adam_beta2: 0.999\n","  allow_clipping_in_normalization: True\n","  attention_dim: 128\n","  attention_filters: 32\n","  attention_kernel: (31,)\n","  attention_size: 128\n","  attention_type: bah_mon_norm\n","  attention_win_size: 7\n","  cleaners: korean_cleaners\n","  clip_mels_length: True\n","  cumulative_weights: True\n","  dec_prenet_sizes: [256, 256]\n","  decoder_layers: 2\n","  decoder_lstm_units: 1024\n","  dilation_channels: 256\n","  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n","  dropout_prob: 0.5\n","  embedding_size: 512\n","  enc_conv_channels: 512\n","  enc_conv_kernel_size: 5\n","  enc_conv_num_layers: 3\n","  encoder_lstm_units: 256\n","  fft_size: 2048\n","  filter_width: 3\n","  gc_channels: 32\n","  griffin_lim_iters: 60\n","  hop_size: 300\n","  inference_prenet_dropout: True\n","  initial_data_greedy: True\n","  initial_phase_step: 8000\n","  input_type: raw\n","  l2_regularization_strength: 0\n","  legacy: True\n","  main_data: ['']\n","  main_data_greedy_factor: 0\n","  mask_encoder: True\n","  max_abs_value: 4.0\n","  max_checkpoints: 3\n","  max_mel_frames: 1000\n","  max_n_frame: 1000\n","  min_level_db: -100\n","  min_n_frame: 150\n","  min_tokens: 30\n","  model_type: single\n","  momentum: 0.9\n","  name: Tacotron-2\n","  num_mels: 80\n","  num_steps: 1000000\n","  optimizer: adam\n","  out_channels: 30\n","  post_bank_channel_size: 128\n","  post_bank_size: 8\n","  post_highway_depth: 4\n","  post_maxpool_width: 2\n","  post_proj_sizes: [256, 80]\n","  post_proj_width: 3\n","  post_rnn_size: 128\n","  postnet_channels: 512\n","  postnet_kernel_size: (5,)\n","  postnet_num_layers: 5\n","  power: 1.5\n","  preemphasis: 0.97\n","  preemphasize: True\n","  prenet_layers: [256, 256]\n","  prioritize_loss: False\n","  quantization_channels: 256\n","  reduction_factor: 2\n","  ref_level_db: 20\n","  rescaling: True\n","  rescaling_max: 0.999\n","  residual_channels: 128\n","  residual_legacy: True\n","  sample_rate: 24000\n","  sample_size: 9000\n","  scalar_input: True\n","  signal_normalization: True\n","  silence_threshold: 0\n","  skip_channels: 128\n","  skip_inadequate: False\n","  skip_path_filter: False\n","  smoothing: False\n","  speaker_embedding_size: 16\n","  store_metadata: False\n","  symmetric_mels: True\n","  synthesis_constraint: False\n","  synthesis_constraint_type: window\n","  tacotron_decay_learning_rate: True\n","  tacotron_decay_rate: 0.5\n","  tacotron_decay_steps: 18000\n","  tacotron_final_learning_rate: 0.0001\n","  tacotron_initial_learning_rate: 0.001\n","  tacotron_reg_weight: 1e-06\n","  tacotron_start_decay: 40000\n","  tacotron_zoneout_rate: 0.1\n","  trim_fft_size: 512\n","  trim_hop_size: 128\n","  trim_silence: True\n","  trim_top_db: 23\n","  upsample_factor: [12, 25]\n","  upsample_type: SubPixel\n","  use_biases: True\n","  use_lws: False\n","  wavenet_batch_size: 2\n","  wavenet_clip_gradients: True\n","  wavenet_decay_rate: 0.5\n","  wavenet_decay_steps: 300000\n","  wavenet_dropout: 0.05\n","  wavenet_learning_rate: 0.001\n","  win_size: 1200\n","filter_by_min_max_frame_batch: 100% 50/50 [00:00<00:00, 322.55it/s]\n"," [./data/son] Loaded metadata for 38 examples (0.05 hours)\n"," [./data/son] Max length: 827\n"," [./data/son] Min length: 176\n","========================================\n","Data Amount:\n","{'./data/son': 1.0}\n","========================================\n","filter_by_min_max_frame_batch: 100% 50/50 [00:00<00:00, 314.81it/s]\n"," [./data/son] Loaded metadata for 38 examples (0.05 hours)\n"," [./data/son] Max length: 827\n"," [./data/son] Min length: 176\n","========================================\n","Data Amount:\n","{'./data/son': 1.0}\n","========================================\n","========================================\n"," model_type: single\n","========================================\n","Initialized Tacotron model. Dimensions: \n","    embedding:                512\n","    encoder conv out:               512\n","    encoder out:              512\n","    attention out:            1024\n","    decoder prenet lstm concat out :        1536\n","    decoder cell out:         162\n","    decoder out (2 frames):  162\n","    decoder mel out:    80\n","    mel out:    80\n","    postnet out:              256\n","    linear out:               1025\n","  Tacotron Parameters       29.142 Million.\n","========================================\n"," model_type: single\n","========================================\n","Initialized Tacotron model. Dimensions: \n","    embedding:                512\n","    encoder conv out:               512\n","    encoder out:              512\n","    attention out:            1024\n","    decoder prenet lstm concat out :        1536\n","    decoder cell out:         162\n","    decoder out (2 frames):  162\n","    decoder mel out:    80\n","    mel out:    80\n","    postnet out:              256\n","    linear out:               1025\n","  Tacotron Parameters       29.142 Million.\n","2021-02-09 00:47:48.353648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-02-09 00:47:48.372044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:47:48.373099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2021-02-09 00:47:48.373473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-02-09 00:47:48.376136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-02-09 00:47:48.378109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-02-09 00:47:48.378476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-02-09 00:47:48.382218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-02-09 00:47:48.395061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-02-09 00:47:48.399986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-02-09 00:47:48.400188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:47:48.401148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:47:48.402033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-02-09 00:47:48.408347: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n","2021-02-09 00:47:48.408856: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xd4ae000 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-02-09 00:47:48.408901: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-02-09 00:47:48.514295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:47:48.515782: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xd4ae1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-02-09 00:47:48.515833: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2021-02-09 00:47:48.516053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:47:48.516886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2021-02-09 00:47:48.516987: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-02-09 00:47:48.517068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-02-09 00:47:48.517141: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-02-09 00:47:48.517171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-02-09 00:47:48.517206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-02-09 00:47:48.517267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-02-09 00:47:48.517297: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-02-09 00:47:48.517374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:47:48.518328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:47:48.519136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-02-09 00:47:48.519210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-02-09 00:47:48.520966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-02-09 00:47:48.520998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-02-09 00:47:48.521009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-02-09 00:47:48.521145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:47:48.522270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:47:48.523094: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2021-02-09 00:47:48.523146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","Starting new training run at commit: None\n","Generated 8 batches of size 2 in 0.000 sec\n","2021-02-09 00:48:02.214450: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","Generated 32 batches of size 32 in 9.351 sec\n","2021-02-09 00:48:02.944162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","Step 1       [12.824 sec/step, loss=9.29908, avg_loss=9.29908]\n","Step 2       [7.067 sec/step, loss=7.79570, avg_loss=8.54739]\n","Step 3       [5.213 sec/step, loss=7.61357, avg_loss=8.23612]\n","Step 4       [4.299 sec/step, loss=8.44239, avg_loss=8.28768]\n","Step 5       [4.112 sec/step, loss=8.64600, avg_loss=8.35935]\n","Step 6       [3.620 sec/step, loss=6.33415, avg_loss=8.02181]\n","Step 7       [3.316 sec/step, loss=5.99077, avg_loss=7.73167]\n","Step 8       [3.046 sec/step, loss=5.81049, avg_loss=7.49152]\n","Step 9       [3.049 sec/step, loss=4.94549, avg_loss=7.20863]\n","Step 10      [2.844 sec/step, loss=4.34447, avg_loss=6.92221]\n","Step 11      [2.692 sec/step, loss=4.68068, avg_loss=6.71844]\n","Step 12      [2.719 sec/step, loss=4.96126, avg_loss=6.57200]\n","Step 13      [2.642 sec/step, loss=4.12945, avg_loss=6.38412]\n","Step 14      [2.524 sec/step, loss=3.71976, avg_loss=6.19380]\n","Step 15      [2.479 sec/step, loss=3.94324, avg_loss=6.04377]\n","Step 16      [2.422 sec/step, loss=3.69052, avg_loss=5.89669]\n","Step 17      [2.370 sec/step, loss=3.98472, avg_loss=5.78422]\n","Step 18      [2.325 sec/step, loss=3.80378, avg_loss=5.67420]\n","2021-02-09 00:48:38.190284: W tensorflow/core/kernels/queue_base.cc:277] _0_datafeeder/input_queue_1: Skipping cancelled enqueue attempt with queue not closed\n","2021-02-09 00:48:38.190426: W tensorflow/core/kernels/queue_base.cc:277] _1_datafeeder/input_queue: Skipping cancelled enqueue attempt with queue not closed\n","Traceback (most recent call last):\n","  File \"train_tacotron2.py\", line 287, in <module>\n","    main()\n","  File \"train_tacotron2.py\", line 283, in main\n","    train(config.model_dir, config)\n","  File \"train_tacotron2.py\", line 190, in train\n","    step, loss, opt = sess.run([global_step, model.loss_without_coeff, model.optimize])\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 956, in run\n","    run_metadata_ptr)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1180, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n","    run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n","    return fn(*args)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n","    target_list, run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n","    return fn(*args)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n","    target_list, run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n","    run_metadata)\n","tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n","\t [[{{node datafeeder/input_queue_enqueue}}]]\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/drive/My Drive/AI대학원/tacotron2/datasets/datafeeder_tacotron2.py\", line 189, in run\n","    self._enqueue_next_group()\n","  File \"/content/drive/My Drive/AI대학원/tacotron2/datasets/datafeeder_tacotron2.py\", line 225, in _enqueue_next_group\n","    self._session.run(self._enqueue_op, feed_dict=feed_dict)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 956, in run\n","    run_metadata_ptr)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1180, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n","    run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1384, in _do_call\n","    raise type(e)(node_def, op, message)\n","tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n","\t [[node datafeeder/input_queue_enqueue (defined at /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]\n","\n","Original stack trace for 'datafeeder/input_queue_enqueue':\n","  File \"train_tacotron2.py\", line 287, in <module>\n","    main()\n","  File \"train_tacotron2.py\", line 283, in main\n","    train(config.model_dir, config)\n","  File \"train_tacotron2.py\", line 121, in train\n","    train_feeder = DataFeederTacotron2(coord, data_dirs, hparams, config, 32,data_type='train', batch_size=config.batch_size)\n","  File \"/content/drive/My Drive/AI대학원/tacotron2/datasets/datafeeder_tacotron2.py\", line 144, in __init__\n","    self._enqueue_op = queue.enqueue(self._placeholders)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/data_flow_ops.py\", line 346, in enqueue\n","    self._queue_ref, vals, name=scope)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/gen_data_flow_ops.py\", line 4410, in queue_enqueue_v2\n","    timeout_ms=timeout_ms, name=name)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n","    op_def=op_def)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n","    return func(*args, **kwargs)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n","    attrs, op_def, compute_device)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n","    op_def=op_def)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n","    self._traceback = tf_stack.extract_stack()\n","\n","Traceback (most recent call last):\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n","    return fn(*args)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n","    target_list, run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n","    run_metadata)\n","tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n","\t [[{{node datafeeder/input_queue_1_enqueue}}]]\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/drive/My Drive/AI대학원/tacotron2/datasets/datafeeder_tacotron2.py\", line 189, in run\n","    self._enqueue_next_group()\n","  File \"/content/drive/My Drive/AI대학원/tacotron2/datasets/datafeeder_tacotron2.py\", line 225, in _enqueue_next_group\n","    self._session.run(self._enqueue_op, feed_dict=feed_dict)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 956, in run\n","    run_metadata_ptr)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1180, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n","    run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1384, in _do_call\n","    raise type(e)(node_def, op, message)\n","tensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\n","\t [[node datafeeder/input_queue_1_enqueue (defined at /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]\n","\n","Original stack trace for 'datafeeder/input_queue_1_enqueue':\n","  File \"train_tacotron2.py\", line 287, in <module>\n","    main()\n","  File \"train_tacotron2.py\", line 283, in main\n","    train(config.model_dir, config)\n","  File \"train_tacotron2.py\", line 122, in train\n","    test_feeder = DataFeederTacotron2(coord, data_dirs, hparams, config, 8, data_type='test', batch_size=config.num_test)\n","  File \"/content/drive/My Drive/AI대학원/tacotron2/datasets/datafeeder_tacotron2.py\", line 144, in __init__\n","    self._enqueue_op = queue.enqueue(self._placeholders)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/data_flow_ops.py\", line 346, in enqueue\n","    self._queue_ref, vals, name=scope)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/gen_data_flow_ops.py\", line 4410, in queue_enqueue_v2\n","    timeout_ms=timeout_ms, name=name)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n","    op_def=op_def)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n","    return func(*args, **kwargs)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n","    attrs, op_def, compute_device)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n","    op_def=op_def)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n","    self._traceback = tf_stack.extract_stack()\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mAEjvczYwNSA","outputId":"5ddce749-2bdb-4b05-8074-fda67e9c5ccb"},"source":["!python train_tacotron2.py --data_paths ./data/son --batch_size 32 --load_path logdir-tacotron2/son_2021-02-06_23-59-17"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n","Step 44495   [1.467 sec/step, loss=0.21170, avg_loss=0.20533]\n","Step 44496   [1.463 sec/step, loss=0.19036, avg_loss=0.20506]\n","Step 44497   [1.461 sec/step, loss=0.18503, avg_loss=0.20496]\n","Step 44498   [1.461 sec/step, loss=0.17961, avg_loss=0.20496]\n","Step 44499   [1.461 sec/step, loss=0.21217, avg_loss=0.20495]\n","Step 44500   [1.464 sec/step, loss=0.18886, avg_loss=0.20500]\n","Writing summary at step: 44500\n","Saving audio and alignment...\n","Generated 32 batches of size 32 in 9.310 sec\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n","WARNING:matplotlib.font_manager:findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12593 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12623 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12616 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12643 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12610 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12596 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12601 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12594 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12641 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12620 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12615 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12628 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12629 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12636 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12618 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12624 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12599 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12634 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12609 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12631 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12613 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12622 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12614 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","WARNING:matplotlib.font_manager:findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44054 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44032 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51648 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48156 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45149 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44208 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44397 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52292 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53469 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46104 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47803 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54664 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49845 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45768 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12593 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12623 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12616 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12643 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12610 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12596 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12601 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12594 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12641 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12620 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12615 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12628 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12629 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12636 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12618 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12624 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12599 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12634 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12609 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12631 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12613 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12622 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12614 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44054 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44032 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51648 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48156 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45149 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44208 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44397 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52292 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53469 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46104 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47803 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54664 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49845 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45768 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/train-step-000044500-align000.png\n","100% 1/1 [00:03<00:00,  3.83s/it]\n","Test finished for step 44500.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12627 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12632 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12642 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12637 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12625 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54924 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49440 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51652 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54868 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48277 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51008 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51116 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51201 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51032 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50896 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51473 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44284 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50500 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45772 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50724 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48516 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49340 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49345 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52268 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49457 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54644 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50556 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47564 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12627 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12632 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12642 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12637 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12625 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54924 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49440 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51652 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54868 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48277 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51008 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51116 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51201 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51032 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50896 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51473 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44284 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50500 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45772 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50724 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48516 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49340 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49345 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52268 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49457 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54644 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50556 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47564 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000044500-align000.png\n"," 50% 1/2 [00:11<00:11, 11.68s/it]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48124 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46308 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44172 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49888 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47280 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48155 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47484 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50612 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51208 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 55176 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48148 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46989 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48124 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46308 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44172 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49888 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47280 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48155 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47484 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50612 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51208 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 55176 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48148 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46989 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000044500-align001.png\n","100% 2/2 [00:23<00:00, 11.52s/it]\n","Test finished for step 44500.\n","Step 44501   [1.464 sec/step, loss=0.20724, avg_loss=0.20495]\n","Step 44502   [1.467 sec/step, loss=0.20613, avg_loss=0.20522]\n","Step 44503   [1.468 sec/step, loss=0.21891, avg_loss=0.20537]\n","Step 44504   [1.466 sec/step, loss=0.19800, avg_loss=0.20544]\n","Step 44505   [1.462 sec/step, loss=0.21281, avg_loss=0.20549]\n","Step 44506   [1.455 sec/step, loss=0.19990, avg_loss=0.20552]\n","Step 44507   [1.457 sec/step, loss=0.20618, avg_loss=0.20579]\n","Step 44508   [1.453 sec/step, loss=0.19553, avg_loss=0.20600]\n","Step 44509   [1.455 sec/step, loss=0.20226, avg_loss=0.20616]\n","Step 44510   [1.453 sec/step, loss=0.21493, avg_loss=0.20628]\n","Step 44511   [1.454 sec/step, loss=0.22221, avg_loss=0.20635]\n","Step 44512   [1.453 sec/step, loss=0.21011, avg_loss=0.20641]\n","Step 44513   [1.451 sec/step, loss=0.18249, avg_loss=0.20635]\n","Step 44514   [1.445 sec/step, loss=0.17789, avg_loss=0.20606]\n","Step 44515   [1.445 sec/step, loss=0.20864, avg_loss=0.20604]\n","Step 44516   [1.445 sec/step, loss=0.20521, avg_loss=0.20608]\n","Step 44517   [1.445 sec/step, loss=0.18144, avg_loss=0.20608]\n","Step 44518   [1.443 sec/step, loss=0.21817, avg_loss=0.20616]\n","Step 44519   [1.441 sec/step, loss=0.19154, avg_loss=0.20622]\n","Step 44520   [1.442 sec/step, loss=0.21544, avg_loss=0.20663]\n","Step 44521   [1.442 sec/step, loss=0.21291, avg_loss=0.20665]\n","Step 44522   [1.438 sec/step, loss=0.18764, avg_loss=0.20645]\n","Step 44523   [1.433 sec/step, loss=0.18503, avg_loss=0.20627]\n","Step 44524   [1.434 sec/step, loss=0.21468, avg_loss=0.20643]\n","Step 44525   [1.435 sec/step, loss=0.21934, avg_loss=0.20662]\n","Step 44526   [1.429 sec/step, loss=0.17839, avg_loss=0.20641]\n","Step 44527   [1.427 sec/step, loss=0.20210, avg_loss=0.20631]\n","Step 44528   [1.429 sec/step, loss=0.21072, avg_loss=0.20645]\n","Step 44529   [1.432 sec/step, loss=0.20500, avg_loss=0.20655]\n","Step 44530   [1.433 sec/step, loss=0.19790, avg_loss=0.20645]\n","Step 44531   [1.433 sec/step, loss=0.19566, avg_loss=0.20629]\n","Step 44532   [1.441 sec/step, loss=0.18387, avg_loss=0.20625]\n","Step 44533   [1.448 sec/step, loss=0.19615, avg_loss=0.20642]\n","Step 44534   [1.448 sec/step, loss=0.19485, avg_loss=0.20626]\n","Step 44535   [1.448 sec/step, loss=0.17728, avg_loss=0.20594]\n","Generated 32 batches of size 32 in 10.398 sec\n","Step 44536   [1.450 sec/step, loss=0.22064, avg_loss=0.20623]\n","Step 44537   [1.453 sec/step, loss=0.22699, avg_loss=0.20668]\n","Step 44538   [1.448 sec/step, loss=0.21877, avg_loss=0.20677]\n","Step 44539   [1.445 sec/step, loss=0.21257, avg_loss=0.20678]\n","Step 44540   [1.444 sec/step, loss=0.20737, avg_loss=0.20674]\n","Step 44541   [1.447 sec/step, loss=0.20447, avg_loss=0.20675]\n","Step 44542   [1.453 sec/step, loss=0.22208, avg_loss=0.20705]\n","Step 44543   [1.453 sec/step, loss=0.18325, avg_loss=0.20683]\n","Step 44544   [1.455 sec/step, loss=0.20925, avg_loss=0.20693]\n","Step 44545   [1.458 sec/step, loss=0.22212, avg_loss=0.20730]\n","Step 44546   [1.461 sec/step, loss=0.21792, avg_loss=0.20744]\n","Step 44547   [1.461 sec/step, loss=0.21632, avg_loss=0.20743]\n","Step 44548   [1.461 sec/step, loss=0.21643, avg_loss=0.20748]\n","Step 44549   [1.455 sec/step, loss=0.17980, avg_loss=0.20721]\n","Step 44550   [1.455 sec/step, loss=0.19211, avg_loss=0.20695]\n","Step 44551   [1.449 sec/step, loss=0.17746, avg_loss=0.20666]\n","Step 44552   [1.453 sec/step, loss=0.20420, avg_loss=0.20667]\n","Step 44553   [1.449 sec/step, loss=0.18653, avg_loss=0.20639]\n","Step 44554   [1.450 sec/step, loss=0.17684, avg_loss=0.20603]\n","Step 44555   [1.445 sec/step, loss=0.18557, avg_loss=0.20554]\n","Step 44556   [1.450 sec/step, loss=0.20296, avg_loss=0.20525]\n","Step 44557   [1.446 sec/step, loss=0.18387, avg_loss=0.20491]\n","Step 44558   [1.446 sec/step, loss=0.18474, avg_loss=0.20466]\n","Step 44559   [1.451 sec/step, loss=0.23659, avg_loss=0.20487]\n","Step 44560   [1.450 sec/step, loss=0.21021, avg_loss=0.20473]\n","Step 44561   [1.447 sec/step, loss=0.18533, avg_loss=0.20437]\n","Step 44562   [1.447 sec/step, loss=0.17551, avg_loss=0.20389]\n","Step 44563   [1.448 sec/step, loss=0.19714, avg_loss=0.20356]\n","Step 44564   [1.445 sec/step, loss=0.17257, avg_loss=0.20304]\n","Step 44565   [1.448 sec/step, loss=0.19362, avg_loss=0.20290]\n","Step 44566   [1.458 sec/step, loss=0.21084, avg_loss=0.20287]\n","Step 44567   [1.463 sec/step, loss=0.21031, avg_loss=0.20259]\n","Generated 32 batches of size 32 in 10.210 sec\n","Step 44568   [1.464 sec/step, loss=0.20784, avg_loss=0.20259]\n","Step 44569   [1.467 sec/step, loss=0.20719, avg_loss=0.20273]\n","Step 44570   [1.460 sec/step, loss=0.19786, avg_loss=0.20230]\n","Step 44571   [1.457 sec/step, loss=0.17754, avg_loss=0.20219]\n","Step 44572   [1.456 sec/step, loss=0.18150, avg_loss=0.20219]\n","Step 44573   [1.452 sec/step, loss=0.19257, avg_loss=0.20186]\n","Step 44574   [1.451 sec/step, loss=0.21499, avg_loss=0.20174]\n","Step 44575   [1.454 sec/step, loss=0.21263, avg_loss=0.20178]\n","Step 44576   [1.450 sec/step, loss=0.18497, avg_loss=0.20150]\n","Step 44577   [1.452 sec/step, loss=0.18318, avg_loss=0.20129]\n","Step 44578   [1.446 sec/step, loss=0.17399, avg_loss=0.20095]\n","Step 44579   [1.447 sec/step, loss=0.21344, avg_loss=0.20091]\n","Step 44580   [1.446 sec/step, loss=0.20579, avg_loss=0.20069]\n","Step 44581   [1.452 sec/step, loss=0.20921, avg_loss=0.20092]\n","Step 44582   [1.453 sec/step, loss=0.20816, avg_loss=0.20087]\n","Step 44583   [1.450 sec/step, loss=0.19799, avg_loss=0.20069]\n","Step 44584   [1.454 sec/step, loss=0.21240, avg_loss=0.20082]\n","Step 44585   [1.455 sec/step, loss=0.19214, avg_loss=0.20081]\n","Step 44586   [1.454 sec/step, loss=0.18143, avg_loss=0.20040]\n","Step 44587   [1.451 sec/step, loss=0.19279, avg_loss=0.20024]\n","Step 44588   [1.452 sec/step, loss=0.18646, avg_loss=0.20029]\n","Step 44589   [1.455 sec/step, loss=0.21043, avg_loss=0.20035]\n","Step 44590   [1.449 sec/step, loss=0.17451, avg_loss=0.19987]\n","Step 44591   [1.444 sec/step, loss=0.17180, avg_loss=0.19954]\n","Step 44592   [1.447 sec/step, loss=0.20709, avg_loss=0.19962]\n","Step 44593   [1.447 sec/step, loss=0.20449, avg_loss=0.19950]\n","Step 44594   [1.455 sec/step, loss=0.20670, avg_loss=0.19958]\n","Step 44595   [1.453 sec/step, loss=0.17711, avg_loss=0.19923]\n","Step 44596   [1.461 sec/step, loss=0.20306, avg_loss=0.19936]\n","Step 44597   [1.469 sec/step, loss=0.20484, avg_loss=0.19956]\n","Step 44598   [1.474 sec/step, loss=0.19673, avg_loss=0.19973]\n","Generated 32 batches of size 32 in 10.485 sec\n","Step 44599   [1.479 sec/step, loss=0.22216, avg_loss=0.19983]\n","Step 44600   [1.478 sec/step, loss=0.20497, avg_loss=0.19999]\n","Writing summary at step: 44600\n","Step 44601   [1.478 sec/step, loss=0.20350, avg_loss=0.19995]\n","Step 44602   [1.481 sec/step, loss=0.20625, avg_loss=0.19995]\n","Step 44603   [1.475 sec/step, loss=0.17699, avg_loss=0.19953]\n","Step 44604   [1.478 sec/step, loss=0.20880, avg_loss=0.19964]\n","Step 44605   [1.472 sec/step, loss=0.17127, avg_loss=0.19923]\n","Step 44606   [1.473 sec/step, loss=0.19028, avg_loss=0.19913]\n","Step 44607   [1.468 sec/step, loss=0.16979, avg_loss=0.19877]\n","Step 44608   [1.472 sec/step, loss=0.20920, avg_loss=0.19890]\n","Step 44609   [1.468 sec/step, loss=0.18372, avg_loss=0.19872]\n","Step 44610   [1.468 sec/step, loss=0.21091, avg_loss=0.19868]\n","Step 44611   [1.468 sec/step, loss=0.21371, avg_loss=0.19859]\n","Step 44612   [1.464 sec/step, loss=0.18419, avg_loss=0.19833]\n","Step 44613   [1.470 sec/step, loss=0.20855, avg_loss=0.19859]\n","Step 44614   [1.475 sec/step, loss=0.20652, avg_loss=0.19888]\n","Step 44615   [1.474 sec/step, loss=0.20124, avg_loss=0.19881]\n","Step 44616   [1.472 sec/step, loss=0.18108, avg_loss=0.19856]\n","Step 44617   [1.478 sec/step, loss=0.21069, avg_loss=0.19886]\n","Step 44618   [1.478 sec/step, loss=0.19960, avg_loss=0.19867]\n","Step 44619   [1.479 sec/step, loss=0.19516, avg_loss=0.19871]\n","Step 44620   [1.473 sec/step, loss=0.18561, avg_loss=0.19841]\n","Step 44621   [1.473 sec/step, loss=0.20019, avg_loss=0.19828]\n","Step 44622   [1.475 sec/step, loss=0.19726, avg_loss=0.19838]\n","Step 44623   [1.475 sec/step, loss=0.17252, avg_loss=0.19825]\n","Step 44624   [1.475 sec/step, loss=0.20879, avg_loss=0.19819]\n","Step 44625   [1.479 sec/step, loss=0.20633, avg_loss=0.19806]\n","Step 44626   [1.485 sec/step, loss=0.18698, avg_loss=0.19815]\n","Step 44627   [1.491 sec/step, loss=0.20054, avg_loss=0.19813]\n","Step 44628   [1.496 sec/step, loss=0.21236, avg_loss=0.19815]\n","Step 44629   [1.498 sec/step, loss=0.19918, avg_loss=0.19809]\n","Generated 32 batches of size 32 in 10.423 sec\n","Step 44630   [1.495 sec/step, loss=0.17323, avg_loss=0.19785]\n","Step 44631   [1.496 sec/step, loss=0.20778, avg_loss=0.19797]\n","Step 44632   [1.491 sec/step, loss=0.19199, avg_loss=0.19805]\n","Step 44633   [1.490 sec/step, loss=0.19997, avg_loss=0.19809]\n","Step 44634   [1.490 sec/step, loss=0.20695, avg_loss=0.19821]\n","Step 44635   [1.485 sec/step, loss=0.17641, avg_loss=0.19820]\n","Step 44636   [1.483 sec/step, loss=0.20573, avg_loss=0.19805]\n","Step 44637   [1.483 sec/step, loss=0.20466, avg_loss=0.19783]\n","Step 44638   [1.482 sec/step, loss=0.21842, avg_loss=0.19782]\n","Step 44639   [1.481 sec/step, loss=0.21193, avg_loss=0.19782]\n","Step 44640   [1.478 sec/step, loss=0.18766, avg_loss=0.19762]\n","Step 44641   [1.475 sec/step, loss=0.19952, avg_loss=0.19757]\n","Step 44642   [1.469 sec/step, loss=0.17535, avg_loss=0.19710]\n","Step 44643   [1.473 sec/step, loss=0.21055, avg_loss=0.19738]\n","Step 44644   [1.475 sec/step, loss=0.20351, avg_loss=0.19732]\n","Step 44645   [1.469 sec/step, loss=0.17128, avg_loss=0.19681]\n","Step 44646   [1.466 sec/step, loss=0.18171, avg_loss=0.19645]\n","Step 44647   [1.464 sec/step, loss=0.19191, avg_loss=0.19620]\n","Step 44648   [1.464 sec/step, loss=0.21433, avg_loss=0.19618]\n","Step 44649   [1.470 sec/step, loss=0.20042, avg_loss=0.19639]\n","Step 44650   [1.473 sec/step, loss=0.21508, avg_loss=0.19662]\n","Step 44651   [1.475 sec/step, loss=0.18539, avg_loss=0.19670]\n","Step 44652   [1.470 sec/step, loss=0.17720, avg_loss=0.19643]\n","Step 44653   [1.473 sec/step, loss=0.20081, avg_loss=0.19657]\n","Step 44654   [1.471 sec/step, loss=0.19501, avg_loss=0.19675]\n","Step 44655   [1.471 sec/step, loss=0.17270, avg_loss=0.19662]\n","Step 44656   [1.471 sec/step, loss=0.16726, avg_loss=0.19627]\n","Step 44657   [1.481 sec/step, loss=0.21028, avg_loss=0.19653]\n","Step 44658   [1.490 sec/step, loss=0.20814, avg_loss=0.19676]\n","Step 44659   [1.489 sec/step, loss=0.18434, avg_loss=0.19624]\n","Step 44660   [1.494 sec/step, loss=0.20787, avg_loss=0.19622]\n","Step 44661   [1.496 sec/step, loss=0.18192, avg_loss=0.19618]\n","Generated 32 batches of size 32 in 10.399 sec\n","Step 44662   [1.501 sec/step, loss=0.20178, avg_loss=0.19645]\n","Step 44663   [1.499 sec/step, loss=0.19899, avg_loss=0.19647]\n","Step 44664   [1.496 sec/step, loss=0.17599, avg_loss=0.19650]\n","Step 44665   [1.492 sec/step, loss=0.20072, avg_loss=0.19657]\n","Step 44666   [1.488 sec/step, loss=0.20030, avg_loss=0.19647]\n","Step 44667   [1.479 sec/step, loss=0.19000, avg_loss=0.19626]\n","Step 44668   [1.473 sec/step, loss=0.17267, avg_loss=0.19591]\n","Step 44669   [1.474 sec/step, loss=0.21131, avg_loss=0.19595]\n","Step 44670   [1.476 sec/step, loss=0.20792, avg_loss=0.19605]\n","Step 44671   [1.477 sec/step, loss=0.17268, avg_loss=0.19600]\n","Step 44672   [1.479 sec/step, loss=0.20322, avg_loss=0.19622]\n","Step 44673   [1.477 sec/step, loss=0.18482, avg_loss=0.19614]\n","Step 44674   [1.474 sec/step, loss=0.19620, avg_loss=0.19596]\n","Step 44675   [1.473 sec/step, loss=0.21405, avg_loss=0.19597]\n","Step 44676   [1.478 sec/step, loss=0.21067, avg_loss=0.19623]\n","Step 44677   [1.481 sec/step, loss=0.20037, avg_loss=0.19640]\n","Step 44678   [1.486 sec/step, loss=0.19827, avg_loss=0.19664]\n","Step 44679   [1.483 sec/step, loss=0.19350, avg_loss=0.19644]\n","Step 44680   [1.484 sec/step, loss=0.21712, avg_loss=0.19656]\n","Step 44681   [1.478 sec/step, loss=0.17919, avg_loss=0.19626]\n","Step 44682   [1.474 sec/step, loss=0.18118, avg_loss=0.19599]\n","Step 44683   [1.473 sec/step, loss=0.17237, avg_loss=0.19573]\n","Step 44684   [1.473 sec/step, loss=0.20726, avg_loss=0.19568]\n","Step 44685   [1.469 sec/step, loss=0.17127, avg_loss=0.19547]\n","Step 44686   [1.473 sec/step, loss=0.21174, avg_loss=0.19577]\n","Step 44687   [1.476 sec/step, loss=0.20207, avg_loss=0.19587]\n","Step 44688   [1.477 sec/step, loss=0.19561, avg_loss=0.19596]\n","Step 44689   [1.482 sec/step, loss=0.22280, avg_loss=0.19608]\n","Step 44690   [1.487 sec/step, loss=0.19323, avg_loss=0.19627]\n","Step 44691   [1.492 sec/step, loss=0.18681, avg_loss=0.19642]\n","Step 44692   [1.497 sec/step, loss=0.21058, avg_loss=0.19645]\n","Step 44693   [1.497 sec/step, loss=0.19675, avg_loss=0.19638]\n","Generated 32 batches of size 32 in 10.254 sec\n","Step 44694   [1.496 sec/step, loss=0.21380, avg_loss=0.19645]\n","Step 44695   [1.500 sec/step, loss=0.20950, avg_loss=0.19677]\n","Step 44696   [1.493 sec/step, loss=0.19211, avg_loss=0.19666]\n","Step 44697   [1.486 sec/step, loss=0.17975, avg_loss=0.19641]\n","Step 44698   [1.487 sec/step, loss=0.20787, avg_loss=0.19652]\n","Step 44699   [1.483 sec/step, loss=0.20274, avg_loss=0.19633]\n","Step 44700   [1.483 sec/step, loss=0.20499, avg_loss=0.19633]\n","Writing summary at step: 44700\n","Step 44701   [1.483 sec/step, loss=0.21375, avg_loss=0.19643]\n","Step 44702   [1.483 sec/step, loss=0.21001, avg_loss=0.19647]\n","Step 44703   [1.486 sec/step, loss=0.19793, avg_loss=0.19668]\n","Step 44704   [1.480 sec/step, loss=0.17917, avg_loss=0.19638]\n","Step 44705   [1.484 sec/step, loss=0.19479, avg_loss=0.19662]\n","Step 44706   [1.482 sec/step, loss=0.17149, avg_loss=0.19643]\n","Step 44707   [1.482 sec/step, loss=0.16956, avg_loss=0.19643]\n","Step 44708   [1.482 sec/step, loss=0.20339, avg_loss=0.19637]\n","Step 44709   [1.481 sec/step, loss=0.17172, avg_loss=0.19625]\n","Step 44710   [1.481 sec/step, loss=0.21478, avg_loss=0.19629]\n","Step 44711   [1.477 sec/step, loss=0.19386, avg_loss=0.19609]\n","Step 44712   [1.479 sec/step, loss=0.19529, avg_loss=0.19620]\n","Step 44713   [1.476 sec/step, loss=0.19212, avg_loss=0.19603]\n","Step 44714   [1.472 sec/step, loss=0.18547, avg_loss=0.19582]\n","Step 44715   [1.474 sec/step, loss=0.21788, avg_loss=0.19599]\n","Step 44716   [1.476 sec/step, loss=0.20823, avg_loss=0.19626]\n","Step 44717   [1.476 sec/step, loss=0.20934, avg_loss=0.19625]\n","Step 44718   [1.474 sec/step, loss=0.18605, avg_loss=0.19611]\n","Step 44719   [1.473 sec/step, loss=0.18385, avg_loss=0.19600]\n","Step 44720   [1.483 sec/step, loss=0.20787, avg_loss=0.19622]\n","Step 44721   [1.483 sec/step, loss=0.17328, avg_loss=0.19595]\n","Step 44722   [1.489 sec/step, loss=0.20601, avg_loss=0.19604]\n","Step 44723   [1.499 sec/step, loss=0.20261, avg_loss=0.19634]\n","Step 44724   [1.501 sec/step, loss=0.20928, avg_loss=0.19635]\n","Generated 32 batches of size 32 in 10.383 sec\n","Step 44725   [1.499 sec/step, loss=0.19903, avg_loss=0.19627]\n","Step 44726   [1.494 sec/step, loss=0.17773, avg_loss=0.19618]\n","Step 44727   [1.490 sec/step, loss=0.20610, avg_loss=0.19624]\n","Step 44728   [1.484 sec/step, loss=0.16918, avg_loss=0.19580]\n","Step 44729   [1.477 sec/step, loss=0.16998, avg_loss=0.19551]\n","Step 44730   [1.472 sec/step, loss=0.16787, avg_loss=0.19546]\n","Step 44731   [1.468 sec/step, loss=0.19666, avg_loss=0.19535]\n","Step 44732   [1.471 sec/step, loss=0.20504, avg_loss=0.19548]\n","Step 44733   [1.468 sec/step, loss=0.19495, avg_loss=0.19543]\n","Step 44734   [1.463 sec/step, loss=0.18417, avg_loss=0.19520]\n","Step 44735   [1.468 sec/step, loss=0.19079, avg_loss=0.19534]\n","Step 44736   [1.463 sec/step, loss=0.17203, avg_loss=0.19501]\n","Step 44737   [1.463 sec/step, loss=0.21835, avg_loss=0.19514]\n","Step 44738   [1.463 sec/step, loss=0.20989, avg_loss=0.19506]\n","Step 44739   [1.459 sec/step, loss=0.18349, avg_loss=0.19477]\n","Step 44740   [1.462 sec/step, loss=0.20717, avg_loss=0.19497]\n","Step 44741   [1.462 sec/step, loss=0.19644, avg_loss=0.19494]\n","Step 44742   [1.468 sec/step, loss=0.20662, avg_loss=0.19525]\n","Step 44743   [1.467 sec/step, loss=0.20235, avg_loss=0.19517]\n","Step 44744   [1.462 sec/step, loss=0.18354, avg_loss=0.19497]\n","Step 44745   [1.468 sec/step, loss=0.20908, avg_loss=0.19535]\n","Step 44746   [1.465 sec/step, loss=0.17068, avg_loss=0.19524]\n","Step 44747   [1.468 sec/step, loss=0.20500, avg_loss=0.19537]\n","Step 44748   [1.469 sec/step, loss=0.20806, avg_loss=0.19531]\n","Step 44749   [1.469 sec/step, loss=0.20510, avg_loss=0.19535]\n","Step 44750   [1.470 sec/step, loss=0.20315, avg_loss=0.19523]\n","Step 44751   [1.468 sec/step, loss=0.17583, avg_loss=0.19514]\n","Step 44752   [1.476 sec/step, loss=0.18251, avg_loss=0.19519]\n","Step 44753   [1.481 sec/step, loss=0.20132, avg_loss=0.19520]\n","Step 44754   [1.487 sec/step, loss=0.19964, avg_loss=0.19524]\n","Step 44755   [1.492 sec/step, loss=0.19364, avg_loss=0.19545]\n","Step 44756   [1.494 sec/step, loss=0.19017, avg_loss=0.19568]\n","Generated 32 batches of size 32 in 10.499 sec\n","Step 44757   [1.492 sec/step, loss=0.21354, avg_loss=0.19571]\n","Step 44758   [1.484 sec/step, loss=0.18378, avg_loss=0.19547]\n","Step 44759   [1.485 sec/step, loss=0.20320, avg_loss=0.19566]\n","Step 44760   [1.480 sec/step, loss=0.20795, avg_loss=0.19566]\n","Step 44761   [1.477 sec/step, loss=0.20193, avg_loss=0.19586]\n","Step 44762   [1.473 sec/step, loss=0.19478, avg_loss=0.19579]\n","Step 44763   [1.474 sec/step, loss=0.20778, avg_loss=0.19588]\n","Step 44764   [1.474 sec/step, loss=0.18427, avg_loss=0.19596]\n","Step 44765   [1.474 sec/step, loss=0.19449, avg_loss=0.19590]\n","Step 44766   [1.475 sec/step, loss=0.21525, avg_loss=0.19605]\n","Step 44767   [1.480 sec/step, loss=0.20734, avg_loss=0.19622]\n","Step 44768   [1.485 sec/step, loss=0.21045, avg_loss=0.19660]\n","Step 44769   [1.482 sec/step, loss=0.19401, avg_loss=0.19643]\n","Step 44770   [1.479 sec/step, loss=0.19109, avg_loss=0.19626]\n","Step 44771   [1.484 sec/step, loss=0.20594, avg_loss=0.19659]\n","Step 44772   [1.485 sec/step, loss=0.20221, avg_loss=0.19658]\n","Step 44773   [1.489 sec/step, loss=0.20220, avg_loss=0.19675]\n","Step 44774   [1.486 sec/step, loss=0.18710, avg_loss=0.19666]\n","Step 44775   [1.487 sec/step, loss=0.21193, avg_loss=0.19664]\n","Step 44776   [1.487 sec/step, loss=0.20734, avg_loss=0.19661]\n","Step 44777   [1.481 sec/step, loss=0.17222, avg_loss=0.19633]\n","Step 44778   [1.476 sec/step, loss=0.17083, avg_loss=0.19605]\n","Step 44779   [1.479 sec/step, loss=0.20576, avg_loss=0.19617]\n","Step 44780   [1.474 sec/step, loss=0.19198, avg_loss=0.19592]\n","Step 44781   [1.474 sec/step, loss=0.17219, avg_loss=0.19585]\n","Step 44782   [1.473 sec/step, loss=0.18472, avg_loss=0.19589]\n","Step 44783   [1.476 sec/step, loss=0.20928, avg_loss=0.19626]\n","Step 44784   [1.480 sec/step, loss=0.20599, avg_loss=0.19625]\n","Step 44785   [1.489 sec/step, loss=0.20167, avg_loss=0.19655]\n","Step 44786   [1.489 sec/step, loss=0.18882, avg_loss=0.19632]\n","Step 44787   [1.494 sec/step, loss=0.20982, avg_loss=0.19640]\n","Step 44788   [1.501 sec/step, loss=0.20721, avg_loss=0.19651]\n","Generated 32 batches of size 32 in 10.392 sec\n","Step 44789   [1.494 sec/step, loss=0.18519, avg_loss=0.19614]\n","Step 44790   [1.493 sec/step, loss=0.18728, avg_loss=0.19608]\n","Step 44791   [1.493 sec/step, loss=0.20417, avg_loss=0.19625]\n","Step 44792   [1.484 sec/step, loss=0.17164, avg_loss=0.19586]\n","Step 44793   [1.478 sec/step, loss=0.17462, avg_loss=0.19564]\n","Step 44794   [1.470 sec/step, loss=0.18313, avg_loss=0.19533]\n","Step 44795   [1.470 sec/step, loss=0.20797, avg_loss=0.19532]\n","Step 44796   [1.472 sec/step, loss=0.20534, avg_loss=0.19545]\n","Step 44797   [1.471 sec/step, loss=0.16990, avg_loss=0.19535]\n","Step 44798   [1.470 sec/step, loss=0.20466, avg_loss=0.19532]\n","Step 44799   [1.470 sec/step, loss=0.20379, avg_loss=0.19533]\n","Step 44800   [1.471 sec/step, loss=0.20358, avg_loss=0.19532]\n","Writing summary at step: 44800\n","Step 44801   [1.471 sec/step, loss=0.20234, avg_loss=0.19520]\n","Step 44802   [1.470 sec/step, loss=0.20723, avg_loss=0.19517]\n","Step 44803   [1.472 sec/step, loss=0.20222, avg_loss=0.19522]\n","Step 44804   [1.472 sec/step, loss=0.17340, avg_loss=0.19516]\n","Step 44805   [1.470 sec/step, loss=0.18850, avg_loss=0.19510]\n","Step 44806   [1.476 sec/step, loss=0.20764, avg_loss=0.19546]\n","Step 44807   [1.481 sec/step, loss=0.20273, avg_loss=0.19579]\n","Step 44808   [1.482 sec/step, loss=0.21302, avg_loss=0.19589]\n","Step 44809   [1.484 sec/step, loss=0.19941, avg_loss=0.19616]\n","Step 44810   [1.482 sec/step, loss=0.18063, avg_loss=0.19582]\n","Step 44811   [1.485 sec/step, loss=0.20208, avg_loss=0.19590]\n","Step 44812   [1.487 sec/step, loss=0.19899, avg_loss=0.19594]\n","Step 44813   [1.488 sec/step, loss=0.19220, avg_loss=0.19594]\n","Step 44814   [1.488 sec/step, loss=0.17082, avg_loss=0.19580]\n","Step 44815   [1.484 sec/step, loss=0.17454, avg_loss=0.19536]\n","Step 44816   [1.489 sec/step, loss=0.20741, avg_loss=0.19535]\n","Step 44817   [1.491 sec/step, loss=0.19445, avg_loss=0.19520]\n","Step 44818   [1.496 sec/step, loss=0.19221, avg_loss=0.19527]\n","Step 44819   [1.500 sec/step, loss=0.18391, avg_loss=0.19527]\n","Generated 32 batches of size 32 in 10.361 sec\n","Step 44820   [1.499 sec/step, loss=0.20548, avg_loss=0.19524]\n","Step 44821   [1.500 sec/step, loss=0.20431, avg_loss=0.19555]\n","Step 44822   [1.496 sec/step, loss=0.20305, avg_loss=0.19552]\n","Step 44823   [1.485 sec/step, loss=0.17946, avg_loss=0.19529]\n","Step 44824   [1.481 sec/step, loss=0.20251, avg_loss=0.19522]\n","Step 44825   [1.478 sec/step, loss=0.20634, avg_loss=0.19530]\n","Step 44826   [1.480 sec/step, loss=0.18506, avg_loss=0.19537]\n","Step 44827   [1.480 sec/step, loss=0.20531, avg_loss=0.19536]\n","Step 44828   [1.480 sec/step, loss=0.21211, avg_loss=0.19579]\n","Step 44829   [1.486 sec/step, loss=0.20521, avg_loss=0.19614]\n","Step 44830   [1.492 sec/step, loss=0.19893, avg_loss=0.19646]\n","Step 44831   [1.493 sec/step, loss=0.20100, avg_loss=0.19650]\n","Step 44832   [1.492 sec/step, loss=0.21251, avg_loss=0.19657]\n","Step 44833   [1.495 sec/step, loss=0.20498, avg_loss=0.19667]\n","Step 44834   [1.499 sec/step, loss=0.20920, avg_loss=0.19692]\n","Step 44835   [1.495 sec/step, loss=0.18199, avg_loss=0.19684]\n","Step 44836   [1.499 sec/step, loss=0.20744, avg_loss=0.19719]\n","Step 44837   [1.493 sec/step, loss=0.17241, avg_loss=0.19673]\n","Step 44838   [1.490 sec/step, loss=0.19706, avg_loss=0.19660]\n","Step 44839   [1.495 sec/step, loss=0.20786, avg_loss=0.19685]\n","Step 44840   [1.495 sec/step, loss=0.20662, avg_loss=0.19684]\n","Step 44841   [1.493 sec/step, loss=0.19785, avg_loss=0.19685]\n","Step 44842   [1.487 sec/step, loss=0.19326, avg_loss=0.19672]\n","Step 44843   [1.485 sec/step, loss=0.19153, avg_loss=0.19661]\n","Step 44844   [1.490 sec/step, loss=0.20454, avg_loss=0.19682]\n","Step 44845   [1.484 sec/step, loss=0.17339, avg_loss=0.19647]\n","Step 44846   [1.485 sec/step, loss=0.19342, avg_loss=0.19669]\n","Step 44847   [1.488 sec/step, loss=0.21415, avg_loss=0.19679]\n","Step 44848   [1.487 sec/step, loss=0.18444, avg_loss=0.19655]\n","Step 44849   [1.483 sec/step, loss=0.17486, avg_loss=0.19625]\n","Step 44850   [1.485 sec/step, loss=0.17950, avg_loss=0.19601]\n","Step 44851   [1.496 sec/step, loss=0.21672, avg_loss=0.19642]\n","Generated 32 batches of size 32 in 10.210 sec\n","Step 44852   [1.499 sec/step, loss=0.20436, avg_loss=0.19664]\n","Step 44853   [1.494 sec/step, loss=0.20859, avg_loss=0.19671]\n","Step 44854   [1.488 sec/step, loss=0.19222, avg_loss=0.19664]\n","Step 44855   [1.482 sec/step, loss=0.17611, avg_loss=0.19646]\n","Step 44856   [1.481 sec/step, loss=0.19912, avg_loss=0.19655]\n","Step 44857   [1.471 sec/step, loss=0.17178, avg_loss=0.19613]\n","Step 44858   [1.475 sec/step, loss=0.20441, avg_loss=0.19634]\n","Step 44859   [1.471 sec/step, loss=0.18298, avg_loss=0.19614]\n","Step 44860   [1.471 sec/step, loss=0.21916, avg_loss=0.19625]\n","Step 44861   [1.471 sec/step, loss=0.19476, avg_loss=0.19618]\n","Step 44862   [1.471 sec/step, loss=0.18990, avg_loss=0.19613]\n","Step 44863   [1.471 sec/step, loss=0.20727, avg_loss=0.19612]\n","Step 44864   [1.473 sec/step, loss=0.18878, avg_loss=0.19617]\n","Step 44865   [1.473 sec/step, loss=0.19263, avg_loss=0.19615]\n","Step 44866   [1.472 sec/step, loss=0.20625, avg_loss=0.19606]\n","Step 44867   [1.471 sec/step, loss=0.20074, avg_loss=0.19599]\n","Step 44868   [1.471 sec/step, loss=0.20186, avg_loss=0.19591]\n","Step 44869   [1.473 sec/step, loss=0.20176, avg_loss=0.19599]\n","Step 44870   [1.470 sec/step, loss=0.17705, avg_loss=0.19584]\n","Step 44871   [1.468 sec/step, loss=0.19565, avg_loss=0.19574]\n","Step 44872   [1.468 sec/step, loss=0.21459, avg_loss=0.19587]\n","Step 44873   [1.468 sec/step, loss=0.20351, avg_loss=0.19588]\n","Step 44874   [1.468 sec/step, loss=0.17186, avg_loss=0.19573]\n","Step 44875   [1.461 sec/step, loss=0.16812, avg_loss=0.19529]\n","Step 44876   [1.458 sec/step, loss=0.18916, avg_loss=0.19511]\n","Step 44877   [1.460 sec/step, loss=0.17735, avg_loss=0.19516]\n","Step 44878   [1.460 sec/step, loss=0.17176, avg_loss=0.19517]\n","Step 44879   [1.464 sec/step, loss=0.19940, avg_loss=0.19510]\n","Step 44880   [1.473 sec/step, loss=0.20745, avg_loss=0.19526]\n","Step 44881   [1.478 sec/step, loss=0.18270, avg_loss=0.19536]\n","Step 44882   [1.485 sec/step, loss=0.19912, avg_loss=0.19551]\n","Step 44883   [1.487 sec/step, loss=0.17679, avg_loss=0.19518]\n","Generated 32 batches of size 32 in 10.287 sec\n","Step 44884   [1.486 sec/step, loss=0.20371, avg_loss=0.19516]\n","Step 44885   [1.483 sec/step, loss=0.20527, avg_loss=0.19520]\n","Step 44886   [1.484 sec/step, loss=0.19844, avg_loss=0.19529]\n","Step 44887   [1.475 sec/step, loss=0.18241, avg_loss=0.19502]\n","Step 44888   [1.470 sec/step, loss=0.20326, avg_loss=0.19498]\n","Step 44889   [1.470 sec/step, loss=0.19035, avg_loss=0.19503]\n","Step 44890   [1.470 sec/step, loss=0.18717, avg_loss=0.19503]\n","Step 44891   [1.471 sec/step, loss=0.20172, avg_loss=0.19500]\n","Step 44892   [1.469 sec/step, loss=0.17698, avg_loss=0.19506]\n","Step 44893   [1.473 sec/step, loss=0.19015, avg_loss=0.19521]\n","Step 44894   [1.471 sec/step, loss=0.16609, avg_loss=0.19504]\n","Step 44895   [1.471 sec/step, loss=0.19933, avg_loss=0.19496]\n","Step 44896   [1.471 sec/step, loss=0.19722, avg_loss=0.19487]\n","Step 44897   [1.471 sec/step, loss=0.16866, avg_loss=0.19486]\n","Step 44898   [1.469 sec/step, loss=0.18356, avg_loss=0.19465]\n","Step 44899   [1.469 sec/step, loss=0.20443, avg_loss=0.19466]\n","Step 44900   [1.468 sec/step, loss=0.20382, avg_loss=0.19466]\n","Writing summary at step: 44900\n","Step 44901   [1.464 sec/step, loss=0.18003, avg_loss=0.19444]\n","Step 44902   [1.465 sec/step, loss=0.20411, avg_loss=0.19441]\n","Step 44903   [1.460 sec/step, loss=0.16974, avg_loss=0.19408]\n","Step 44904   [1.460 sec/step, loss=0.16563, avg_loss=0.19400]\n","Step 44905   [1.463 sec/step, loss=0.20590, avg_loss=0.19418]\n","Step 44906   [1.463 sec/step, loss=0.21005, avg_loss=0.19420]\n","Step 44907   [1.459 sec/step, loss=0.18780, avg_loss=0.19405]\n","Step 44908   [1.459 sec/step, loss=0.19734, avg_loss=0.19390]\n","Step 44909   [1.461 sec/step, loss=0.20387, avg_loss=0.19394]\n","Step 44910   [1.469 sec/step, loss=0.20640, avg_loss=0.19420]\n","Step 44911   [1.473 sec/step, loss=0.20302, avg_loss=0.19421]\n","Step 44912   [1.475 sec/step, loss=0.19493, avg_loss=0.19417]\n","Step 44913   [1.482 sec/step, loss=0.20003, avg_loss=0.19424]\n","Step 44914   [1.491 sec/step, loss=0.20031, avg_loss=0.19454]\n","Generated 32 batches of size 32 in 10.355 sec\n","Step 44915   [1.491 sec/step, loss=0.17629, avg_loss=0.19456]\n","Step 44916   [1.486 sec/step, loss=0.20481, avg_loss=0.19453]\n","Step 44917   [1.480 sec/step, loss=0.18025, avg_loss=0.19439]\n","Step 44918   [1.476 sec/step, loss=0.17590, avg_loss=0.19423]\n","Step 44919   [1.477 sec/step, loss=0.20104, avg_loss=0.19440]\n","Step 44920   [1.472 sec/step, loss=0.19681, avg_loss=0.19431]\n","Step 44921   [1.466 sec/step, loss=0.17839, avg_loss=0.19405]\n","Step 44922   [1.467 sec/step, loss=0.16714, avg_loss=0.19369]\n","Step 44923   [1.469 sec/step, loss=0.18889, avg_loss=0.19379]\n","Step 44924   [1.472 sec/step, loss=0.20901, avg_loss=0.19385]\n","Step 44925   [1.471 sec/step, loss=0.20442, avg_loss=0.19383]\n","Step 44926   [1.475 sec/step, loss=0.19850, avg_loss=0.19397]\n","Step 44927   [1.471 sec/step, loss=0.18148, avg_loss=0.19373]\n","Step 44928   [1.471 sec/step, loss=0.20157, avg_loss=0.19362]\n","Step 44929   [1.467 sec/step, loss=0.16832, avg_loss=0.19325]\n","Step 44930   [1.462 sec/step, loss=0.17233, avg_loss=0.19299]\n","Step 44931   [1.465 sec/step, loss=0.20668, avg_loss=0.19304]\n","Step 44932   [1.459 sec/step, loss=0.16720, avg_loss=0.19259]\n","Step 44933   [1.460 sec/step, loss=0.20266, avg_loss=0.19257]\n","Step 44934   [1.459 sec/step, loss=0.19935, avg_loss=0.19247]\n","Step 44935   [1.461 sec/step, loss=0.18611, avg_loss=0.19251]\n","Step 44936   [1.460 sec/step, loss=0.19382, avg_loss=0.19238]\n","Step 44937   [1.462 sec/step, loss=0.17841, avg_loss=0.19244]\n","Step 44938   [1.464 sec/step, loss=0.20421, avg_loss=0.19251]\n","Step 44939   [1.461 sec/step, loss=0.20142, avg_loss=0.19244]\n","Step 44940   [1.459 sec/step, loss=0.19545, avg_loss=0.19233]\n","Step 44941   [1.460 sec/step, loss=0.19072, avg_loss=0.19226]\n","Step 44942   [1.467 sec/step, loss=0.18157, avg_loss=0.19214]\n","Step 44943   [1.474 sec/step, loss=0.21472, avg_loss=0.19237]\n","Step 44944   [1.478 sec/step, loss=0.20754, avg_loss=0.19240]\n","Step 44945   [1.485 sec/step, loss=0.20196, avg_loss=0.19269]\n","Step 44946   [1.494 sec/step, loss=0.20309, avg_loss=0.19279]\n","Generated 32 batches of size 32 in 10.335 sec\n","Step 44947   [1.491 sec/step, loss=0.20375, avg_loss=0.19268]\n","Step 44948   [1.487 sec/step, loss=0.19364, avg_loss=0.19277]\n","Step 44949   [1.491 sec/step, loss=0.20691, avg_loss=0.19310]\n","Step 44950   [1.483 sec/step, loss=0.17295, avg_loss=0.19303]\n","Step 44951   [1.475 sec/step, loss=0.19590, avg_loss=0.19282]\n","Step 44952   [1.471 sec/step, loss=0.20175, avg_loss=0.19280]\n","Step 44953   [1.465 sec/step, loss=0.17623, avg_loss=0.19247]\n","Step 44954   [1.464 sec/step, loss=0.18607, avg_loss=0.19241]\n","Step 44955   [1.466 sec/step, loss=0.19151, avg_loss=0.19256]\n","Step 44956   [1.462 sec/step, loss=0.18086, avg_loss=0.19238]\n","Step 44957   [1.468 sec/step, loss=0.20953, avg_loss=0.19276]\n","Step 44958   [1.468 sec/step, loss=0.20499, avg_loss=0.19276]\n","Step 44959   [1.472 sec/step, loss=0.20133, avg_loss=0.19295]\n","Step 44960   [1.468 sec/step, loss=0.17479, avg_loss=0.19250]\n","Step 44961   [1.471 sec/step, loss=0.20095, avg_loss=0.19257]\n","Step 44962   [1.474 sec/step, loss=0.21330, avg_loss=0.19280]\n","Step 44963   [1.469 sec/step, loss=0.18228, avg_loss=0.19255]\n","Step 44964   [1.473 sec/step, loss=0.20158, avg_loss=0.19268]\n","Step 44965   [1.470 sec/step, loss=0.17866, avg_loss=0.19254]\n","Step 44966   [1.464 sec/step, loss=0.17150, avg_loss=0.19219]\n","Step 44967   [1.462 sec/step, loss=0.19696, avg_loss=0.19215]\n","Step 44968   [1.463 sec/step, loss=0.20806, avg_loss=0.19222]\n","Step 44969   [1.461 sec/step, loss=0.19018, avg_loss=0.19210]\n","Step 44970   [1.467 sec/step, loss=0.20535, avg_loss=0.19238]\n","Step 44971   [1.469 sec/step, loss=0.21935, avg_loss=0.19262]\n","Step 44972   [1.463 sec/step, loss=0.18892, avg_loss=0.19236]\n","Step 44973   [1.459 sec/step, loss=0.18953, avg_loss=0.19222]\n","Step 44974   [1.469 sec/step, loss=0.21261, avg_loss=0.19263]\n","Step 44975   [1.479 sec/step, loss=0.20400, avg_loss=0.19299]\n","Step 44976   [1.484 sec/step, loss=0.18068, avg_loss=0.19290]\n","Step 44977   [1.492 sec/step, loss=0.20655, avg_loss=0.19320]\n","Step 44978   [1.503 sec/step, loss=0.20680, avg_loss=0.19355]\n","Generated 32 batches of size 32 in 10.356 sec\n","Step 44979   [1.495 sec/step, loss=0.17518, avg_loss=0.19330]\n","Step 44980   [1.490 sec/step, loss=0.20292, avg_loss=0.19326]\n","Step 44981   [1.491 sec/step, loss=0.19954, avg_loss=0.19343]\n","Step 44982   [1.486 sec/step, loss=0.20173, avg_loss=0.19345]\n","Step 44983   [1.484 sec/step, loss=0.20404, avg_loss=0.19373]\n","Step 44984   [1.481 sec/step, loss=0.19945, avg_loss=0.19368]\n","Step 44985   [1.481 sec/step, loss=0.20051, avg_loss=0.19364]\n","Step 44986   [1.481 sec/step, loss=0.21386, avg_loss=0.19379]\n","Step 44987   [1.484 sec/step, loss=0.19713, avg_loss=0.19394]\n","Step 44988   [1.483 sec/step, loss=0.19553, avg_loss=0.19386]\n","Step 44989   [1.486 sec/step, loss=0.20295, avg_loss=0.19399]\n","Step 44990   [1.486 sec/step, loss=0.19451, avg_loss=0.19406]\n","Step 44991   [1.480 sec/step, loss=0.17727, avg_loss=0.19382]\n","Step 44992   [1.485 sec/step, loss=0.20457, avg_loss=0.19409]\n","Step 44993   [1.484 sec/step, loss=0.19543, avg_loss=0.19414]\n","Step 44994   [1.490 sec/step, loss=0.20055, avg_loss=0.19449]\n","Step 44995   [1.487 sec/step, loss=0.19029, avg_loss=0.19440]\n","Step 44996   [1.484 sec/step, loss=0.17710, avg_loss=0.19420]\n","Step 44997   [1.484 sec/step, loss=0.17337, avg_loss=0.19424]\n","Step 44998   [1.488 sec/step, loss=0.20437, avg_loss=0.19445]\n","Step 44999   [1.487 sec/step, loss=0.19565, avg_loss=0.19436]\n","Step 45000   [1.487 sec/step, loss=0.19683, avg_loss=0.19429]\n","Writing summary at step: 45000\n","Saving audio and alignment...\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12635 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45720 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47140 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49436 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47924 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47308 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50676 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48264 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45208 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44368 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52404 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50632 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12635 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45720 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47140 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49436 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47924 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47308 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50676 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48264 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45208 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44368 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52404 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50632 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/train-step-000045000-align000.png\n","100% 1/1 [00:02<00:00,  2.84s/it]\n","Test finished for step 45000.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000045000-align000.png\n"," 50% 1/2 [00:11<00:11, 11.71s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000045000-align001.png\n","100% 2/2 [00:23<00:00, 11.71s/it]\n","Test finished for step 45000.\n","Step 45001   [1.491 sec/step, loss=0.20610, avg_loss=0.19456]\n","Step 45002   [1.489 sec/step, loss=0.19276, avg_loss=0.19444]\n","Step 45003   [1.495 sec/step, loss=0.20166, avg_loss=0.19476]\n","Step 45004   [1.501 sec/step, loss=0.18743, avg_loss=0.19498]\n","Step 45005   [1.507 sec/step, loss=0.20404, avg_loss=0.19496]\n","Step 45006   [1.512 sec/step, loss=0.20210, avg_loss=0.19488]\n","Step 45007   [1.513 sec/step, loss=0.17382, avg_loss=0.19474]\n","Step 45008   [1.512 sec/step, loss=0.19186, avg_loss=0.19469]\n","Step 45009   [1.510 sec/step, loss=0.16714, avg_loss=0.19432]\n","Generated 32 batches of size 32 in 10.440 sec\n","Step 45010   [1.500 sec/step, loss=0.16597, avg_loss=0.19391]\n","Step 45011   [1.492 sec/step, loss=0.18222, avg_loss=0.19371]\n","Step 45012   [1.486 sec/step, loss=0.16829, avg_loss=0.19344]\n","Step 45013   [1.476 sec/step, loss=0.16727, avg_loss=0.19311]\n","Step 45014   [1.469 sec/step, loss=0.19942, avg_loss=0.19310]\n","Step 45015   [1.469 sec/step, loss=0.19285, avg_loss=0.19327]\n","Step 45016   [1.467 sec/step, loss=0.18672, avg_loss=0.19309]\n","Step 45017   [1.467 sec/step, loss=0.19664, avg_loss=0.19325]\n","Step 45018   [1.470 sec/step, loss=0.21380, avg_loss=0.19363]\n","Step 45019   [1.464 sec/step, loss=0.17374, avg_loss=0.19336]\n","Step 45020   [1.465 sec/step, loss=0.20227, avg_loss=0.19341]\n","Step 45021   [1.470 sec/step, loss=0.20507, avg_loss=0.19368]\n","Step 45022   [1.467 sec/step, loss=0.17110, avg_loss=0.19372]\n","Step 45023   [1.468 sec/step, loss=0.18466, avg_loss=0.19368]\n","Step 45024   [1.468 sec/step, loss=0.21144, avg_loss=0.19370]\n","Step 45025   [1.463 sec/step, loss=0.17027, avg_loss=0.19336]\n","Step 45026   [1.459 sec/step, loss=0.18248, avg_loss=0.19320]\n","Step 45027   [1.462 sec/step, loss=0.20550, avg_loss=0.19344]\n","Step 45028   [1.456 sec/step, loss=0.16677, avg_loss=0.19309]\n","Step 45029   [1.460 sec/step, loss=0.19696, avg_loss=0.19338]\n","Step 45030   [1.465 sec/step, loss=0.20371, avg_loss=0.19369]\n","Step 45031   [1.465 sec/step, loss=0.20286, avg_loss=0.19365]\n","Step 45032   [1.470 sec/step, loss=0.19922, avg_loss=0.19397]\n","Step 45033   [1.468 sec/step, loss=0.22046, avg_loss=0.19415]\n","Step 45034   [1.468 sec/step, loss=0.20750, avg_loss=0.19423]\n","Step 45035   [1.473 sec/step, loss=0.20541, avg_loss=0.19443]\n","Step 45036   [1.473 sec/step, loss=0.18466, avg_loss=0.19434]\n","Step 45037   [1.478 sec/step, loss=0.19600, avg_loss=0.19451]\n","Step 45038   [1.482 sec/step, loss=0.20065, avg_loss=0.19448]\n","Step 45039   [1.489 sec/step, loss=0.20541, avg_loss=0.19452]\n","Step 45040   [1.496 sec/step, loss=0.20598, avg_loss=0.19462]\n","Generated 32 batches of size 32 in 10.225 sec\n","Step 45041   [1.496 sec/step, loss=0.17465, avg_loss=0.19446]\n","Step 45042   [1.494 sec/step, loss=0.20342, avg_loss=0.19468]\n","Step 45043   [1.490 sec/step, loss=0.20349, avg_loss=0.19457]\n","Step 45044   [1.483 sec/step, loss=0.18728, avg_loss=0.19436]\n","Step 45045   [1.483 sec/step, loss=0.20159, avg_loss=0.19436]\n","Step 45046   [1.472 sec/step, loss=0.16827, avg_loss=0.19401]\n","Step 45047   [1.471 sec/step, loss=0.19698, avg_loss=0.19394]\n","Step 45048   [1.474 sec/step, loss=0.20161, avg_loss=0.19402]\n","Step 45049   [1.470 sec/step, loss=0.18253, avg_loss=0.19378]\n","Step 45050   [1.476 sec/step, loss=0.20410, avg_loss=0.19409]\n","Step 45051   [1.475 sec/step, loss=0.17790, avg_loss=0.19391]\n","Step 45052   [1.472 sec/step, loss=0.19066, avg_loss=0.19380]\n","Step 45053   [1.474 sec/step, loss=0.17869, avg_loss=0.19382]\n","Step 45054   [1.478 sec/step, loss=0.19745, avg_loss=0.19394]\n","Step 45055   [1.481 sec/step, loss=0.19901, avg_loss=0.19401]\n","Step 45056   [1.485 sec/step, loss=0.20815, avg_loss=0.19429]\n","Step 45057   [1.481 sec/step, loss=0.16977, avg_loss=0.19389]\n","Step 45058   [1.481 sec/step, loss=0.16838, avg_loss=0.19352]\n","Step 45059   [1.478 sec/step, loss=0.18943, avg_loss=0.19340]\n","Step 45060   [1.482 sec/step, loss=0.19592, avg_loss=0.19362]\n","Step 45061   [1.482 sec/step, loss=0.20293, avg_loss=0.19364]\n","Step 45062   [1.482 sec/step, loss=0.20090, avg_loss=0.19351]\n","Step 45063   [1.486 sec/step, loss=0.20150, avg_loss=0.19370]\n","Step 45064   [1.486 sec/step, loss=0.19444, avg_loss=0.19363]\n","Step 45065   [1.491 sec/step, loss=0.19350, avg_loss=0.19378]\n","Step 45066   [1.491 sec/step, loss=0.16961, avg_loss=0.19376]\n","Step 45067   [1.494 sec/step, loss=0.19237, avg_loss=0.19372]\n","Step 45068   [1.491 sec/step, loss=0.16520, avg_loss=0.19329]\n","Step 45069   [1.490 sec/step, loss=0.16421, avg_loss=0.19303]\n","Step 45070   [1.495 sec/step, loss=0.20096, avg_loss=0.19298]\n","Step 45071   [1.496 sec/step, loss=0.18258, avg_loss=0.19262]\n","Step 45072   [1.501 sec/step, loss=0.17967, avg_loss=0.19252]\n","Step 45073   [1.506 sec/step, loss=0.19783, avg_loss=0.19261]\n","Generated 32 batches of size 32 in 10.400 sec\n","Step 45074   [1.504 sec/step, loss=0.21605, avg_loss=0.19264]\n","Step 45075   [1.500 sec/step, loss=0.20107, avg_loss=0.19261]\n","Step 45076   [1.493 sec/step, loss=0.17356, avg_loss=0.19254]\n","Step 45077   [1.488 sec/step, loss=0.20323, avg_loss=0.19251]\n","Step 45078   [1.484 sec/step, loss=0.19913, avg_loss=0.19243]\n","Step 45079   [1.484 sec/step, loss=0.18612, avg_loss=0.19254]\n","Step 45080   [1.484 sec/step, loss=0.19896, avg_loss=0.19250]\n","Step 45081   [1.484 sec/step, loss=0.20185, avg_loss=0.19252]\n","Step 45082   [1.487 sec/step, loss=0.20414, avg_loss=0.19255]\n","Step 45083   [1.486 sec/step, loss=0.19877, avg_loss=0.19249]\n","Step 45084   [1.484 sec/step, loss=0.19363, avg_loss=0.19244]\n","Step 45085   [1.480 sec/step, loss=0.18407, avg_loss=0.19227]\n","Step 45086   [1.480 sec/step, loss=0.19784, avg_loss=0.19211]\n","Step 45087   [1.480 sec/step, loss=0.20153, avg_loss=0.19216]\n","Step 45088   [1.480 sec/step, loss=0.19349, avg_loss=0.19214]\n","Step 45089   [1.474 sec/step, loss=0.17424, avg_loss=0.19185]\n","Step 45090   [1.477 sec/step, loss=0.21081, avg_loss=0.19201]\n","Step 45091   [1.482 sec/step, loss=0.20246, avg_loss=0.19226]\n","Step 45092   [1.480 sec/step, loss=0.18472, avg_loss=0.19206]\n","Step 45093   [1.484 sec/step, loss=0.20187, avg_loss=0.19213]\n","Step 45094   [1.481 sec/step, loss=0.20634, avg_loss=0.19219]\n","Step 45095   [1.483 sec/step, loss=0.20368, avg_loss=0.19232]\n","Step 45096   [1.480 sec/step, loss=0.17734, avg_loss=0.19232]\n","Step 45097   [1.485 sec/step, loss=0.19575, avg_loss=0.19255]\n","Step 45098   [1.482 sec/step, loss=0.18172, avg_loss=0.19232]\n","Step 45099   [1.481 sec/step, loss=0.20469, avg_loss=0.19241]\n","Step 45100   [1.478 sec/step, loss=0.17082, avg_loss=0.19215]\n","Writing summary at step: 45100\n","Step 45101   [1.475 sec/step, loss=0.16798, avg_loss=0.19177]\n","Step 45102   [1.477 sec/step, loss=0.18091, avg_loss=0.19165]\n","Step 45103   [1.475 sec/step, loss=0.16689, avg_loss=0.19130]\n","Step 45104   [1.476 sec/step, loss=0.19811, avg_loss=0.19141]\n","Generated 32 batches of size 32 in 10.354 sec\n","Step 45105   [1.468 sec/step, loss=0.17515, avg_loss=0.19112]\n","Step 45106   [1.461 sec/step, loss=0.19115, avg_loss=0.19101]\n","Step 45107   [1.463 sec/step, loss=0.20409, avg_loss=0.19131]\n","Step 45108   [1.464 sec/step, loss=0.21480, avg_loss=0.19154]\n","Step 45109   [1.464 sec/step, loss=0.18988, avg_loss=0.19177]\n","Step 45110   [1.469 sec/step, loss=0.21025, avg_loss=0.19221]\n","Step 45111   [1.472 sec/step, loss=0.20227, avg_loss=0.19241]\n","Step 45112   [1.474 sec/step, loss=0.19304, avg_loss=0.19266]\n","Step 45113   [1.474 sec/step, loss=0.17261, avg_loss=0.19272]\n","Step 45114   [1.472 sec/step, loss=0.17689, avg_loss=0.19249]\n","Step 45115   [1.474 sec/step, loss=0.20543, avg_loss=0.19262]\n","Step 45116   [1.472 sec/step, loss=0.18493, avg_loss=0.19260]\n","Step 45117   [1.470 sec/step, loss=0.16786, avg_loss=0.19231]\n","Step 45118   [1.470 sec/step, loss=0.20890, avg_loss=0.19226]\n","Step 45119   [1.476 sec/step, loss=0.20446, avg_loss=0.19257]\n","Step 45120   [1.475 sec/step, loss=0.20162, avg_loss=0.19256]\n","Step 45121   [1.473 sec/step, loss=0.19545, avg_loss=0.19247]\n","Step 45122   [1.471 sec/step, loss=0.17386, avg_loss=0.19249]\n","Step 45123   [1.468 sec/step, loss=0.16788, avg_loss=0.19233]\n","Step 45124   [1.468 sec/step, loss=0.20334, avg_loss=0.19224]\n","Step 45125   [1.474 sec/step, loss=0.21155, avg_loss=0.19266]\n","Step 45126   [1.478 sec/step, loss=0.20853, avg_loss=0.19292]\n","Step 45127   [1.480 sec/step, loss=0.20499, avg_loss=0.19291]\n","Step 45128   [1.483 sec/step, loss=0.18817, avg_loss=0.19313]\n","Step 45129   [1.480 sec/step, loss=0.19247, avg_loss=0.19308]\n","Step 45130   [1.476 sec/step, loss=0.18325, avg_loss=0.19288]\n","Step 45131   [1.481 sec/step, loss=0.20832, avg_loss=0.19293]\n","Step 45132   [1.486 sec/step, loss=0.21185, avg_loss=0.19306]\n","Step 45133   [1.493 sec/step, loss=0.20463, avg_loss=0.19290]\n","Step 45134   [1.489 sec/step, loss=0.19845, avg_loss=0.19281]\n","Step 45135   [1.488 sec/step, loss=0.20447, avg_loss=0.19280]\n","Generated 32 batches of size 32 in 10.424 sec\n","Step 45136   [1.490 sec/step, loss=0.19681, avg_loss=0.19292]\n","Step 45137   [1.489 sec/step, loss=0.19909, avg_loss=0.19295]\n","Step 45138   [1.485 sec/step, loss=0.23044, avg_loss=0.19325]\n","Step 45139   [1.478 sec/step, loss=0.18902, avg_loss=0.19309]\n","Step 45140   [1.475 sec/step, loss=0.21774, avg_loss=0.19320]\n","Step 45141   [1.478 sec/step, loss=0.20917, avg_loss=0.19355]\n","Step 45142   [1.479 sec/step, loss=0.16744, avg_loss=0.19319]\n","Step 45143   [1.473 sec/step, loss=0.17561, avg_loss=0.19291]\n","Step 45144   [1.477 sec/step, loss=0.21196, avg_loss=0.19316]\n","Step 45145   [1.473 sec/step, loss=0.20267, avg_loss=0.19317]\n","Step 45146   [1.478 sec/step, loss=0.20481, avg_loss=0.19353]\n","Step 45147   [1.474 sec/step, loss=0.18565, avg_loss=0.19342]\n","Step 45148   [1.474 sec/step, loss=0.19653, avg_loss=0.19337]\n","Step 45149   [1.478 sec/step, loss=0.21205, avg_loss=0.19366]\n","Step 45150   [1.478 sec/step, loss=0.21432, avg_loss=0.19377]\n","Step 45151   [1.476 sec/step, loss=0.18116, avg_loss=0.19380]\n","Step 45152   [1.475 sec/step, loss=0.19073, avg_loss=0.19380]\n","Step 45153   [1.477 sec/step, loss=0.19413, avg_loss=0.19395]\n","Step 45154   [1.470 sec/step, loss=0.16921, avg_loss=0.19367]\n","Step 45155   [1.470 sec/step, loss=0.20963, avg_loss=0.19378]\n","Step 45156   [1.470 sec/step, loss=0.21953, avg_loss=0.19389]\n","Step 45157   [1.474 sec/step, loss=0.20342, avg_loss=0.19423]\n","Step 45158   [1.474 sec/step, loss=0.20457, avg_loss=0.19459]\n","Step 45159   [1.474 sec/step, loss=0.19503, avg_loss=0.19465]\n","Step 45160   [1.474 sec/step, loss=0.20012, avg_loss=0.19469]\n","Step 45161   [1.473 sec/step, loss=0.19737, avg_loss=0.19463]\n","Step 45162   [1.467 sec/step, loss=0.17431, avg_loss=0.19437]\n","Step 45163   [1.472 sec/step, loss=0.20080, avg_loss=0.19436]\n","Step 45164   [1.471 sec/step, loss=0.17280, avg_loss=0.19414]\n","Step 45165   [1.475 sec/step, loss=0.20240, avg_loss=0.19423]\n","Step 45166   [1.481 sec/step, loss=0.19560, avg_loss=0.19449]\n","Step 45167   [1.482 sec/step, loss=0.17759, avg_loss=0.19434]\n","Generated 32 batches of size 32 in 10.217 sec\n","Step 45168   [1.488 sec/step, loss=0.20744, avg_loss=0.19477]\n","Step 45169   [1.491 sec/step, loss=0.20623, avg_loss=0.19519]\n","Step 45170   [1.480 sec/step, loss=0.18071, avg_loss=0.19498]\n","Step 45171   [1.475 sec/step, loss=0.18870, avg_loss=0.19505]\n","Step 45172   [1.475 sec/step, loss=0.19704, avg_loss=0.19522]\n","Step 45173   [1.474 sec/step, loss=0.19699, avg_loss=0.19521]\n","Step 45174   [1.468 sec/step, loss=0.18170, avg_loss=0.19487]\n","Step 45175   [1.463 sec/step, loss=0.17205, avg_loss=0.19458]\n","Step 45176   [1.462 sec/step, loss=0.16539, avg_loss=0.19450]\n","Step 45177   [1.460 sec/step, loss=0.20350, avg_loss=0.19450]\n","Step 45178   [1.455 sec/step, loss=0.18564, avg_loss=0.19436]\n","Step 45179   [1.457 sec/step, loss=0.19267, avg_loss=0.19443]\n","Step 45180   [1.457 sec/step, loss=0.20713, avg_loss=0.19451]\n","Step 45181   [1.451 sec/step, loss=0.17519, avg_loss=0.19424]\n","Step 45182   [1.447 sec/step, loss=0.19713, avg_loss=0.19417]\n","Step 45183   [1.448 sec/step, loss=0.20743, avg_loss=0.19426]\n","Step 45184   [1.447 sec/step, loss=0.19333, avg_loss=0.19426]\n","Step 45185   [1.451 sec/step, loss=0.21056, avg_loss=0.19452]\n","Step 45186   [1.451 sec/step, loss=0.20552, avg_loss=0.19460]\n","Step 45187   [1.451 sec/step, loss=0.20347, avg_loss=0.19462]\n","Step 45188   [1.451 sec/step, loss=0.20434, avg_loss=0.19473]\n","Step 45189   [1.452 sec/step, loss=0.17465, avg_loss=0.19473]\n","Step 45190   [1.452 sec/step, loss=0.19817, avg_loss=0.19460]\n","Step 45191   [1.453 sec/step, loss=0.20581, avg_loss=0.19464]\n","Step 45192   [1.456 sec/step, loss=0.19785, avg_loss=0.19477]\n","Step 45193   [1.455 sec/step, loss=0.20263, avg_loss=0.19478]\n","Step 45194   [1.452 sec/step, loss=0.16932, avg_loss=0.19441]\n","Step 45195   [1.458 sec/step, loss=0.20079, avg_loss=0.19438]\n","Step 45196   [1.463 sec/step, loss=0.18278, avg_loss=0.19443]\n","Step 45197   [1.462 sec/step, loss=0.17860, avg_loss=0.19426]\n","Step 45198   [1.463 sec/step, loss=0.16496, avg_loss=0.19409]\n","Step 45199   [1.465 sec/step, loss=0.21104, avg_loss=0.19416]\n","Step 45200   [1.469 sec/step, loss=0.17809, avg_loss=0.19423]\n","Writing summary at step: 45200\n","Generated 32 batches of size 32 in 10.288 sec\n","Step 45201   [1.472 sec/step, loss=0.20400, avg_loss=0.19459]\n","Step 45202   [1.472 sec/step, loss=0.20108, avg_loss=0.19479]\n","Step 45203   [1.473 sec/step, loss=0.20114, avg_loss=0.19513]\n","Step 45204   [1.467 sec/step, loss=0.18051, avg_loss=0.19496]\n","Step 45205   [1.470 sec/step, loss=0.19949, avg_loss=0.19520]\n","Step 45206   [1.471 sec/step, loss=0.19433, avg_loss=0.19523]\n","Step 45207   [1.469 sec/step, loss=0.19392, avg_loss=0.19513]\n","Step 45208   [1.469 sec/step, loss=0.20899, avg_loss=0.19507]\n","Step 45209   [1.470 sec/step, loss=0.18646, avg_loss=0.19504]\n","Step 45210   [1.464 sec/step, loss=0.17051, avg_loss=0.19464]\n","Step 45211   [1.462 sec/step, loss=0.18102, avg_loss=0.19443]\n","Step 45212   [1.465 sec/step, loss=0.20266, avg_loss=0.19453]\n","Step 45213   [1.467 sec/step, loss=0.18208, avg_loss=0.19462]\n","Step 45214   [1.468 sec/step, loss=0.18935, avg_loss=0.19475]\n","Step 45215   [1.469 sec/step, loss=0.19708, avg_loss=0.19466]\n","Step 45216   [1.469 sec/step, loss=0.18134, avg_loss=0.19463]\n","Step 45217   [1.474 sec/step, loss=0.20616, avg_loss=0.19501]\n","Step 45218   [1.469 sec/step, loss=0.17654, avg_loss=0.19469]\n","Step 45219   [1.465 sec/step, loss=0.16673, avg_loss=0.19431]\n","Step 45220   [1.465 sec/step, loss=0.20507, avg_loss=0.19434]\n","Step 45221   [1.467 sec/step, loss=0.20083, avg_loss=0.19440]\n","Step 45222   [1.473 sec/step, loss=0.19656, avg_loss=0.19462]\n","Step 45223   [1.474 sec/step, loss=0.17823, avg_loss=0.19473]\n","Step 45224   [1.474 sec/step, loss=0.19774, avg_loss=0.19467]\n","Step 45225   [1.473 sec/step, loss=0.19945, avg_loss=0.19455]\n","Step 45226   [1.478 sec/step, loss=0.20916, avg_loss=0.19456]\n","Step 45227   [1.482 sec/step, loss=0.20287, avg_loss=0.19453]\n","Step 45228   [1.485 sec/step, loss=0.19144, avg_loss=0.19457]\n","Step 45229   [1.490 sec/step, loss=0.18930, avg_loss=0.19454]\n","Step 45230   [1.491 sec/step, loss=0.18872, avg_loss=0.19459]\n","Generated 32 batches of size 32 in 10.341 sec\n","Step 45231   [1.490 sec/step, loss=0.20489, avg_loss=0.19456]\n","Step 45232   [1.479 sec/step, loss=0.16975, avg_loss=0.19414]\n","Step 45233   [1.469 sec/step, loss=0.16882, avg_loss=0.19378]\n","Step 45234   [1.473 sec/step, loss=0.21521, avg_loss=0.19394]\n","Step 45235   [1.473 sec/step, loss=0.20767, avg_loss=0.19398]\n","Step 45236   [1.468 sec/step, loss=0.19170, avg_loss=0.19393]\n","Step 45237   [1.465 sec/step, loss=0.20858, avg_loss=0.19402]\n","Step 45238   [1.463 sec/step, loss=0.19670, avg_loss=0.19368]\n","Step 45239   [1.463 sec/step, loss=0.18917, avg_loss=0.19368]\n","Step 45240   [1.458 sec/step, loss=0.19441, avg_loss=0.19345]\n","Step 45241   [1.458 sec/step, loss=0.22310, avg_loss=0.19359]\n","Step 45242   [1.457 sec/step, loss=0.20944, avg_loss=0.19401]\n","Step 45243   [1.463 sec/step, loss=0.20574, avg_loss=0.19431]\n","Step 45244   [1.457 sec/step, loss=0.19773, avg_loss=0.19417]\n","Step 45245   [1.461 sec/step, loss=0.21205, avg_loss=0.19426]\n","Step 45246   [1.461 sec/step, loss=0.20433, avg_loss=0.19426]\n","Step 45247   [1.462 sec/step, loss=0.22556, avg_loss=0.19466]\n","Step 45248   [1.465 sec/step, loss=0.20731, avg_loss=0.19477]\n","Step 45249   [1.459 sec/step, loss=0.17385, avg_loss=0.19438]\n","Step 45250   [1.459 sec/step, loss=0.19870, avg_loss=0.19423]\n","Step 45251   [1.460 sec/step, loss=0.17615, avg_loss=0.19418]\n","Step 45252   [1.464 sec/step, loss=0.21360, avg_loss=0.19441]\n","Step 45253   [1.463 sec/step, loss=0.18332, avg_loss=0.19430]\n","Step 45254   [1.468 sec/step, loss=0.20455, avg_loss=0.19465]\n","Step 45255   [1.468 sec/step, loss=0.20248, avg_loss=0.19458]\n","Step 45256   [1.463 sec/step, loss=0.17829, avg_loss=0.19417]\n","Step 45257   [1.463 sec/step, loss=0.21768, avg_loss=0.19431]\n","Step 45258   [1.467 sec/step, loss=0.20854, avg_loss=0.19435]\n","Step 45259   [1.469 sec/step, loss=0.19110, avg_loss=0.19431]\n","Step 45260   [1.466 sec/step, loss=0.17380, avg_loss=0.19405]\n","Step 45261   [1.468 sec/step, loss=0.17878, avg_loss=0.19386]\n","Step 45262   [1.480 sec/step, loss=0.20363, avg_loss=0.19415]\n","Generated 32 batches of size 32 in 10.391 sec\n","Step 45263   [1.478 sec/step, loss=0.20390, avg_loss=0.19419]\n","Step 45264   [1.480 sec/step, loss=0.20801, avg_loss=0.19454]\n","Step 45265   [1.470 sec/step, loss=0.17342, avg_loss=0.19425]\n","Step 45266   [1.467 sec/step, loss=0.19900, avg_loss=0.19428]\n","Step 45267   [1.466 sec/step, loss=0.19691, avg_loss=0.19447]\n","Step 45268   [1.458 sec/step, loss=0.17320, avg_loss=0.19413]\n","Step 45269   [1.458 sec/step, loss=0.16498, avg_loss=0.19372]\n","Step 45270   [1.464 sec/step, loss=0.19576, avg_loss=0.19387]\n","Step 45271   [1.467 sec/step, loss=0.20472, avg_loss=0.19403]\n","Step 45272   [1.465 sec/step, loss=0.18765, avg_loss=0.19394]\n","Step 45273   [1.463 sec/step, loss=0.18599, avg_loss=0.19383]\n","Step 45274   [1.462 sec/step, loss=0.18445, avg_loss=0.19385]\n","Step 45275   [1.468 sec/step, loss=0.20155, avg_loss=0.19415]\n","Step 45276   [1.470 sec/step, loss=0.17738, avg_loss=0.19427]\n","Step 45277   [1.466 sec/step, loss=0.17056, avg_loss=0.19394]\n","Step 45278   [1.471 sec/step, loss=0.21149, avg_loss=0.19420]\n","Step 45279   [1.468 sec/step, loss=0.16368, avg_loss=0.19391]\n","Step 45280   [1.468 sec/step, loss=0.20198, avg_loss=0.19386]\n","Step 45281   [1.474 sec/step, loss=0.19912, avg_loss=0.19410]\n","Step 45282   [1.478 sec/step, loss=0.20244, avg_loss=0.19415]\n","Step 45283   [1.473 sec/step, loss=0.16679, avg_loss=0.19374]\n","Step 45284   [1.470 sec/step, loss=0.16411, avg_loss=0.19345]\n","Step 45285   [1.470 sec/step, loss=0.20277, avg_loss=0.19337]\n","Step 45286   [1.468 sec/step, loss=0.20035, avg_loss=0.19332]\n","Step 45287   [1.461 sec/step, loss=0.16394, avg_loss=0.19293]\n","Step 45288   [1.462 sec/step, loss=0.20419, avg_loss=0.19292]\n","Step 45289   [1.468 sec/step, loss=0.20221, avg_loss=0.19320]\n","Step 45290   [1.468 sec/step, loss=0.19131, avg_loss=0.19313]\n","Step 45291   [1.472 sec/step, loss=0.20192, avg_loss=0.19309]\n","Step 45292   [1.470 sec/step, loss=0.17987, avg_loss=0.19291]\n","Step 45293   [1.474 sec/step, loss=0.19874, avg_loss=0.19287]\n","Step 45294   [1.485 sec/step, loss=0.20239, avg_loss=0.19320]\n","Generated 32 batches of size 32 in 10.511 sec\n","Step 45295   [1.481 sec/step, loss=0.19312, avg_loss=0.19313]\n","Step 45296   [1.482 sec/step, loss=0.20654, avg_loss=0.19337]\n","Step 45297   [1.483 sec/step, loss=0.19491, avg_loss=0.19353]\n","Step 45298   [1.483 sec/step, loss=0.17971, avg_loss=0.19368]\n","Step 45299   [1.480 sec/step, loss=0.18570, avg_loss=0.19342]\n","Step 45300   [1.473 sec/step, loss=0.16982, avg_loss=0.19334]\n","Writing summary at step: 45300\n","Step 45301   [1.467 sec/step, loss=0.16484, avg_loss=0.19295]\n","Step 45302   [1.467 sec/step, loss=0.20319, avg_loss=0.19297]\n","Step 45303   [1.468 sec/step, loss=0.20565, avg_loss=0.19301]\n","Step 45304   [1.472 sec/step, loss=0.19803, avg_loss=0.19319]\n","Step 45305   [1.469 sec/step, loss=0.17712, avg_loss=0.19297]\n","Step 45306   [1.470 sec/step, loss=0.19372, avg_loss=0.19296]\n","Step 45307   [1.473 sec/step, loss=0.20150, avg_loss=0.19304]\n","Step 45308   [1.474 sec/step, loss=0.20226, avg_loss=0.19297]\n","Step 45309   [1.473 sec/step, loss=0.19029, avg_loss=0.19301]\n","Step 45310   [1.473 sec/step, loss=0.18059, avg_loss=0.19311]\n","Step 45311   [1.472 sec/step, loss=0.18568, avg_loss=0.19315]\n","Step 45312   [1.468 sec/step, loss=0.18229, avg_loss=0.19295]\n","Step 45313   [1.472 sec/step, loss=0.20120, avg_loss=0.19314]\n","Step 45314   [1.474 sec/step, loss=0.20081, avg_loss=0.19326]\n","Step 45315   [1.474 sec/step, loss=0.19361, avg_loss=0.19322]\n","Step 45316   [1.473 sec/step, loss=0.18071, avg_loss=0.19321]\n","Step 45317   [1.473 sec/step, loss=0.19990, avg_loss=0.19315]\n","Step 45318   [1.479 sec/step, loss=0.19789, avg_loss=0.19337]\n","Step 45319   [1.483 sec/step, loss=0.20722, avg_loss=0.19377]\n","Step 45320   [1.484 sec/step, loss=0.19559, avg_loss=0.19368]\n","Step 45321   [1.485 sec/step, loss=0.19627, avg_loss=0.19363]\n","Step 45322   [1.490 sec/step, loss=0.19867, avg_loss=0.19365]\n","Step 45323   [1.499 sec/step, loss=0.20063, avg_loss=0.19388]\n","Step 45324   [1.500 sec/step, loss=0.18493, avg_loss=0.19375]\n","Step 45325   [1.498 sec/step, loss=0.16879, avg_loss=0.19344]\n","Generated 32 batches of size 32 in 10.223 sec\n","Step 45326   [1.491 sec/step, loss=0.16872, avg_loss=0.19304]\n","Step 45327   [1.483 sec/step, loss=0.18024, avg_loss=0.19281]\n","Step 45328   [1.478 sec/step, loss=0.17641, avg_loss=0.19266]\n","Step 45329   [1.471 sec/step, loss=0.16640, avg_loss=0.19243]\n","Step 45330   [1.474 sec/step, loss=0.20216, avg_loss=0.19257]\n","Step 45331   [1.467 sec/step, loss=0.16666, avg_loss=0.19218]\n","Step 45332   [1.471 sec/step, loss=0.19518, avg_loss=0.19244]\n","Step 45333   [1.474 sec/step, loss=0.19043, avg_loss=0.19265]\n","Step 45334   [1.473 sec/step, loss=0.20709, avg_loss=0.19257]\n","Step 45335   [1.469 sec/step, loss=0.18501, avg_loss=0.19235]\n","Step 45336   [1.471 sec/step, loss=0.18579, avg_loss=0.19229]\n","Step 45337   [1.467 sec/step, loss=0.17333, avg_loss=0.19193]\n","Step 45338   [1.470 sec/step, loss=0.19699, avg_loss=0.19194]\n","Step 45339   [1.473 sec/step, loss=0.20157, avg_loss=0.19206]\n","Step 45340   [1.477 sec/step, loss=0.19621, avg_loss=0.19208]\n","Step 45341   [1.477 sec/step, loss=0.19905, avg_loss=0.19184]\n","Step 45342   [1.477 sec/step, loss=0.20153, avg_loss=0.19176]\n","Step 45343   [1.471 sec/step, loss=0.17165, avg_loss=0.19142]\n","Step 45344   [1.476 sec/step, loss=0.19641, avg_loss=0.19140]\n","Step 45345   [1.476 sec/step, loss=0.20175, avg_loss=0.19130]\n","Step 45346   [1.476 sec/step, loss=0.20082, avg_loss=0.19127]\n","Step 45347   [1.476 sec/step, loss=0.17995, avg_loss=0.19081]\n","Step 45348   [1.470 sec/step, loss=0.16533, avg_loss=0.19039]\n","Step 45349   [1.476 sec/step, loss=0.20329, avg_loss=0.19069]\n","Step 45350   [1.476 sec/step, loss=0.19335, avg_loss=0.19063]\n","Step 45351   [1.475 sec/step, loss=0.16190, avg_loss=0.19049]\n","Step 45352   [1.475 sec/step, loss=0.19159, avg_loss=0.19027]\n","Step 45353   [1.483 sec/step, loss=0.19834, avg_loss=0.19042]\n","Step 45354   [1.483 sec/step, loss=0.17990, avg_loss=0.19017]\n","Step 45355   [1.484 sec/step, loss=0.19368, avg_loss=0.19008]\n","Step 45356   [1.486 sec/step, loss=0.16443, avg_loss=0.18995]\n","Step 45357   [1.490 sec/step, loss=0.20410, avg_loss=0.18981]\n","Step 45358   [1.485 sec/step, loss=0.17914, avg_loss=0.18952]\n","Generated 32 batches of size 32 in 10.559 sec\n","Step 45359   [1.485 sec/step, loss=0.18438, avg_loss=0.18945]\n","Step 45360   [1.488 sec/step, loss=0.19662, avg_loss=0.18968]\n","Step 45361   [1.483 sec/step, loss=0.17568, avg_loss=0.18965]\n","Step 45362   [1.476 sec/step, loss=0.19075, avg_loss=0.18952]\n","Step 45363   [1.473 sec/step, loss=0.21748, avg_loss=0.18965]\n","Step 45364   [1.467 sec/step, loss=0.16549, avg_loss=0.18923]\n","Step 45365   [1.471 sec/step, loss=0.18742, avg_loss=0.18937]\n","Step 45366   [1.473 sec/step, loss=0.20007, avg_loss=0.18938]\n","Step 45367   [1.474 sec/step, loss=0.19608, avg_loss=0.18937]\n","Step 45368   [1.474 sec/step, loss=0.17948, avg_loss=0.18943]\n","Step 45369   [1.474 sec/step, loss=0.19734, avg_loss=0.18976]\n","Step 45370   [1.474 sec/step, loss=0.19354, avg_loss=0.18974]\n","Step 45371   [1.474 sec/step, loss=0.19769, avg_loss=0.18966]\n","Step 45372   [1.476 sec/step, loss=0.18996, avg_loss=0.18969]\n","Step 45373   [1.475 sec/step, loss=0.17679, avg_loss=0.18960]\n","Step 45374   [1.476 sec/step, loss=0.18721, avg_loss=0.18962]\n","Step 45375   [1.472 sec/step, loss=0.17376, avg_loss=0.18935]\n","Step 45376   [1.470 sec/step, loss=0.16923, avg_loss=0.18926]\n","Step 45377   [1.473 sec/step, loss=0.18300, avg_loss=0.18939]\n","Step 45378   [1.470 sec/step, loss=0.18253, avg_loss=0.18910]\n","Step 45379   [1.470 sec/step, loss=0.16131, avg_loss=0.18908]\n","Step 45380   [1.470 sec/step, loss=0.20752, avg_loss=0.18913]\n","Step 45381   [1.465 sec/step, loss=0.16606, avg_loss=0.18880]\n","Step 45382   [1.464 sec/step, loss=0.20109, avg_loss=0.18879]\n","Step 45383   [1.467 sec/step, loss=0.18620, avg_loss=0.18898]\n","Step 45384   [1.472 sec/step, loss=0.20093, avg_loss=0.18935]\n","Step 45385   [1.476 sec/step, loss=0.19547, avg_loss=0.18928]\n","Step 45386   [1.482 sec/step, loss=0.20310, avg_loss=0.18930]\n","Step 45387   [1.485 sec/step, loss=0.16835, avg_loss=0.18935]\n","Step 45388   [1.484 sec/step, loss=0.18088, avg_loss=0.18911]\n","Step 45389   [1.485 sec/step, loss=0.17320, avg_loss=0.18882]\n","Step 45390   [1.482 sec/step, loss=0.16238, avg_loss=0.18854]\n","Generated 32 batches of size 32 in 10.427 sec\n","Step 45391   [1.479 sec/step, loss=0.19902, avg_loss=0.18851]\n","Step 45392   [1.480 sec/step, loss=0.21174, avg_loss=0.18882]\n","Step 45393   [1.477 sec/step, loss=0.20744, avg_loss=0.18891]\n","Step 45394   [1.472 sec/step, loss=0.20175, avg_loss=0.18891]\n","Step 45395   [1.469 sec/step, loss=0.18697, avg_loss=0.18884]\n","Step 45396   [1.468 sec/step, loss=0.20230, avg_loss=0.18880]\n","Step 45397   [1.468 sec/step, loss=0.20132, avg_loss=0.18887]\n","Step 45398   [1.467 sec/step, loss=0.17493, avg_loss=0.18882]\n","Step 45399   [1.467 sec/step, loss=0.18362, avg_loss=0.18880]\n","Step 45400   [1.472 sec/step, loss=0.21543, avg_loss=0.18925]\n","Writing summary at step: 45400\n","Step 45401   [1.475 sec/step, loss=0.18246, avg_loss=0.18943]\n","Step 45402   [1.475 sec/step, loss=0.20264, avg_loss=0.18942]\n","Step 45403   [1.475 sec/step, loss=0.21579, avg_loss=0.18953]\n","Step 45404   [1.474 sec/step, loss=0.19743, avg_loss=0.18952]\n","Step 45405   [1.473 sec/step, loss=0.18289, avg_loss=0.18958]\n","Step 45406   [1.468 sec/step, loss=0.17197, avg_loss=0.18936]\n","Step 45407   [1.467 sec/step, loss=0.19874, avg_loss=0.18933]\n","Step 45408   [1.464 sec/step, loss=0.19010, avg_loss=0.18921]\n","Step 45409   [1.462 sec/step, loss=0.17847, avg_loss=0.18909]\n","Step 45410   [1.468 sec/step, loss=0.20408, avg_loss=0.18933]\n","Step 45411   [1.464 sec/step, loss=0.16712, avg_loss=0.18914]\n","Step 45412   [1.469 sec/step, loss=0.20172, avg_loss=0.18934]\n","Step 45413   [1.463 sec/step, loss=0.16231, avg_loss=0.18895]\n","Step 45414   [1.458 sec/step, loss=0.16145, avg_loss=0.18855]\n","Step 45415   [1.458 sec/step, loss=0.20224, avg_loss=0.18864]\n","Step 45416   [1.462 sec/step, loss=0.18101, avg_loss=0.18864]\n","Step 45417   [1.459 sec/step, loss=0.16454, avg_loss=0.18829]\n","Step 45418   [1.463 sec/step, loss=0.19788, avg_loss=0.18829]\n","Step 45419   [1.462 sec/step, loss=0.17461, avg_loss=0.18796]\n","Step 45420   [1.466 sec/step, loss=0.19563, avg_loss=0.18796]\n","Step 45421   [1.468 sec/step, loss=0.19642, avg_loss=0.18796]\n","Generated 32 batches of size 32 in 10.234 sec\n","Step 45422   [1.464 sec/step, loss=0.20130, avg_loss=0.18799]\n","Step 45423   [1.457 sec/step, loss=0.18757, avg_loss=0.18786]\n","Step 45424   [1.456 sec/step, loss=0.19199, avg_loss=0.18793]\n","Step 45425   [1.459 sec/step, loss=0.19761, avg_loss=0.18822]\n","Step 45426   [1.459 sec/step, loss=0.19709, avg_loss=0.18850]\n","Step 45427   [1.463 sec/step, loss=0.19521, avg_loss=0.18865]\n","Step 45428   [1.466 sec/step, loss=0.19084, avg_loss=0.18880]\n","Step 45429   [1.469 sec/step, loss=0.18409, avg_loss=0.18897]\n","Step 45430   [1.464 sec/step, loss=0.16907, avg_loss=0.18864]\n","Step 45431   [1.463 sec/step, loss=0.18079, avg_loss=0.18878]\n","Step 45432   [1.463 sec/step, loss=0.18216, avg_loss=0.18865]\n","Step 45433   [1.464 sec/step, loss=0.18157, avg_loss=0.18856]\n","Step 45434   [1.463 sec/step, loss=0.19940, avg_loss=0.18849]\n","Step 45435   [1.467 sec/step, loss=0.19235, avg_loss=0.18856]\n","Step 45436   [1.465 sec/step, loss=0.16360, avg_loss=0.18834]\n","Step 45437   [1.467 sec/step, loss=0.17501, avg_loss=0.18836]\n","Step 45438   [1.463 sec/step, loss=0.17286, avg_loss=0.18811]\n","Step 45439   [1.462 sec/step, loss=0.19032, avg_loss=0.18800]\n","Step 45440   [1.455 sec/step, loss=0.16228, avg_loss=0.18766]\n","Step 45441   [1.455 sec/step, loss=0.20363, avg_loss=0.18771]\n","Step 45442   [1.449 sec/step, loss=0.15918, avg_loss=0.18729]\n","Step 45443   [1.455 sec/step, loss=0.19868, avg_loss=0.18756]\n","Step 45444   [1.450 sec/step, loss=0.15912, avg_loss=0.18718]\n","Step 45445   [1.446 sec/step, loss=0.18419, avg_loss=0.18701]\n","Step 45446   [1.446 sec/step, loss=0.19161, avg_loss=0.18692]\n","Step 45447   [1.446 sec/step, loss=0.17213, avg_loss=0.18684]\n","Step 45448   [1.457 sec/step, loss=0.19333, avg_loss=0.18712]\n","Step 45449   [1.461 sec/step, loss=0.18950, avg_loss=0.18698]\n","Step 45450   [1.466 sec/step, loss=0.20354, avg_loss=0.18708]\n","Step 45451   [1.477 sec/step, loss=0.20062, avg_loss=0.18747]\n","Step 45452   [1.478 sec/step, loss=0.18893, avg_loss=0.18744]\n","Generated 32 batches of size 32 in 10.528 sec\n","Step 45453   [1.475 sec/step, loss=0.19734, avg_loss=0.18743]\n","Step 45454   [1.473 sec/step, loss=0.18312, avg_loss=0.18746]\n","Step 45455   [1.473 sec/step, loss=0.20347, avg_loss=0.18756]\n","Step 45456   [1.470 sec/step, loss=0.16896, avg_loss=0.18761]\n","Step 45457   [1.465 sec/step, loss=0.20007, avg_loss=0.18757]\n","Step 45458   [1.466 sec/step, loss=0.19771, avg_loss=0.18775]\n","Step 45459   [1.468 sec/step, loss=0.19736, avg_loss=0.18788]\n","Step 45460   [1.469 sec/step, loss=0.19666, avg_loss=0.18788]\n","Step 45461   [1.472 sec/step, loss=0.19635, avg_loss=0.18809]\n","Step 45462   [1.469 sec/step, loss=0.19054, avg_loss=0.18809]\n","Step 45463   [1.469 sec/step, loss=0.17201, avg_loss=0.18763]\n","Step 45464   [1.475 sec/step, loss=0.19789, avg_loss=0.18796]\n","Step 45465   [1.477 sec/step, loss=0.19666, avg_loss=0.18805]\n","Step 45466   [1.476 sec/step, loss=0.19240, avg_loss=0.18797]\n","Step 45467   [1.472 sec/step, loss=0.17761, avg_loss=0.18779]\n","Step 45468   [1.474 sec/step, loss=0.18605, avg_loss=0.18785]\n","Step 45469   [1.474 sec/step, loss=0.20617, avg_loss=0.18794]\n","Step 45470   [1.469 sec/step, loss=0.17321, avg_loss=0.18774]\n","Step 45471   [1.469 sec/step, loss=0.19370, avg_loss=0.18770]\n","Step 45472   [1.463 sec/step, loss=0.16272, avg_loss=0.18743]\n","Step 45473   [1.465 sec/step, loss=0.18570, avg_loss=0.18751]\n","Step 45474   [1.465 sec/step, loss=0.17595, avg_loss=0.18740]\n","Step 45475   [1.465 sec/step, loss=0.16824, avg_loss=0.18735]\n","Step 45476   [1.468 sec/step, loss=0.18280, avg_loss=0.18748]\n","Step 45477   [1.465 sec/step, loss=0.17321, avg_loss=0.18738]\n","Step 45478   [1.468 sec/step, loss=0.19986, avg_loss=0.18756]\n","Step 45479   [1.474 sec/step, loss=0.19441, avg_loss=0.18789]\n","Step 45480   [1.473 sec/step, loss=0.17954, avg_loss=0.18761]\n","Step 45481   [1.474 sec/step, loss=0.16314, avg_loss=0.18758]\n","Step 45482   [1.476 sec/step, loss=0.18710, avg_loss=0.18744]\n","Step 45483   [1.483 sec/step, loss=0.19193, avg_loss=0.18750]\n","Step 45484   [1.482 sec/step, loss=0.17587, avg_loss=0.18725]\n","Step 45485   [1.483 sec/step, loss=0.21017, avg_loss=0.18739]\n","Generated 32 batches of size 32 in 10.445 sec\n","Step 45486   [1.481 sec/step, loss=0.20325, avg_loss=0.18740]\n","Step 45487   [1.477 sec/step, loss=0.16736, avg_loss=0.18739]\n","Step 45488   [1.478 sec/step, loss=0.19541, avg_loss=0.18753]\n","Step 45489   [1.471 sec/step, loss=0.16137, avg_loss=0.18741]\n","Step 45490   [1.472 sec/step, loss=0.18615, avg_loss=0.18765]\n","Step 45491   [1.470 sec/step, loss=0.20646, avg_loss=0.18772]\n","Step 45492   [1.470 sec/step, loss=0.20328, avg_loss=0.18764]\n","Step 45493   [1.470 sec/step, loss=0.20041, avg_loss=0.18757]\n","Step 45494   [1.466 sec/step, loss=0.18313, avg_loss=0.18738]\n","Step 45495   [1.469 sec/step, loss=0.20188, avg_loss=0.18753]\n","Step 45496   [1.470 sec/step, loss=0.19605, avg_loss=0.18747]\n","Step 45497   [1.464 sec/step, loss=0.17684, avg_loss=0.18723]\n","Step 45498   [1.466 sec/step, loss=0.18443, avg_loss=0.18732]\n","Step 45499   [1.468 sec/step, loss=0.20056, avg_loss=0.18749]\n","Step 45500   [1.469 sec/step, loss=0.21659, avg_loss=0.18750]\n","Writing summary at step: 45500\n","Saving audio and alignment...\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12612 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47560 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50668 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45817 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54620 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49688 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51339 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49324 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46988 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48324 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47196 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50630 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51012 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44161 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12612 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47560 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50668 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45817 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54620 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49688 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51339 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49324 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46988 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48324 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47196 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50630 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51012 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44161 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/train-step-000045500-align000.png\n","100% 1/1 [00:03<00:00,  3.77s/it]\n","Test finished for step 45500.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000045500-align000.png\n"," 50% 1/2 [00:11<00:11, 11.80s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000045500-align001.png\n","100% 2/2 [00:23<00:00, 11.70s/it]\n","Test finished for step 45500.\n","Step 45501   [1.472 sec/step, loss=0.19707, avg_loss=0.18765]\n","Step 45502   [1.468 sec/step, loss=0.18210, avg_loss=0.18744]\n","Step 45503   [1.467 sec/step, loss=0.19593, avg_loss=0.18724]\n","Step 45504   [1.468 sec/step, loss=0.20076, avg_loss=0.18728]\n","Step 45505   [1.473 sec/step, loss=0.19843, avg_loss=0.18743]\n","Step 45506   [1.478 sec/step, loss=0.19255, avg_loss=0.18764]\n","Step 45507   [1.476 sec/step, loss=0.17324, avg_loss=0.18738]\n","Step 45508   [1.476 sec/step, loss=0.18644, avg_loss=0.18735]\n","Step 45509   [1.481 sec/step, loss=0.20078, avg_loss=0.18757]\n","Step 45510   [1.483 sec/step, loss=0.18118, avg_loss=0.18734]\n","Step 45511   [1.489 sec/step, loss=0.16789, avg_loss=0.18735]\n","Step 45512   [1.493 sec/step, loss=0.19832, avg_loss=0.18731]\n","Step 45513   [1.496 sec/step, loss=0.16678, avg_loss=0.18736]\n","Step 45514   [1.498 sec/step, loss=0.16224, avg_loss=0.18737]\n","Step 45515   [1.498 sec/step, loss=0.17853, avg_loss=0.18713]\n","Generated 32 batches of size 32 in 10.343 sec\n","Step 45516   [1.495 sec/step, loss=0.16332, avg_loss=0.18695]\n","Step 45517   [1.498 sec/step, loss=0.19205, avg_loss=0.18723]\n","Step 45518   [1.490 sec/step, loss=0.17506, avg_loss=0.18700]\n","Step 45519   [1.492 sec/step, loss=0.21081, avg_loss=0.18736]\n","Step 45520   [1.488 sec/step, loss=0.19783, avg_loss=0.18738]\n","Step 45521   [1.478 sec/step, loss=0.16960, avg_loss=0.18712]\n","Step 45522   [1.472 sec/step, loss=0.16426, avg_loss=0.18674]\n","Step 45523   [1.472 sec/step, loss=0.19571, avg_loss=0.18683]\n","Step 45524   [1.466 sec/step, loss=0.16095, avg_loss=0.18652]\n","Step 45525   [1.467 sec/step, loss=0.19983, avg_loss=0.18654]\n","Step 45526   [1.468 sec/step, loss=0.19672, avg_loss=0.18653]\n","Step 45527   [1.469 sec/step, loss=0.19405, avg_loss=0.18652]\n","Step 45528   [1.470 sec/step, loss=0.19947, avg_loss=0.18661]\n","Step 45529   [1.468 sec/step, loss=0.17101, avg_loss=0.18648]\n","Step 45530   [1.473 sec/step, loss=0.19535, avg_loss=0.18674]\n","Step 45531   [1.477 sec/step, loss=0.19130, avg_loss=0.18685]\n","Step 45532   [1.480 sec/step, loss=0.19802, avg_loss=0.18700]\n","Step 45533   [1.477 sec/step, loss=0.16142, avg_loss=0.18680]\n","Step 45534   [1.477 sec/step, loss=0.19545, avg_loss=0.18676]\n","Step 45535   [1.474 sec/step, loss=0.18230, avg_loss=0.18666]\n","Step 45536   [1.479 sec/step, loss=0.19727, avg_loss=0.18700]\n","Step 45537   [1.483 sec/step, loss=0.19355, avg_loss=0.18719]\n","Step 45538   [1.486 sec/step, loss=0.18992, avg_loss=0.18736]\n","Step 45539   [1.484 sec/step, loss=0.18597, avg_loss=0.18731]\n","Step 45540   [1.486 sec/step, loss=0.17900, avg_loss=0.18748]\n","Step 45541   [1.487 sec/step, loss=0.19147, avg_loss=0.18736]\n","Step 45542   [1.494 sec/step, loss=0.18610, avg_loss=0.18763]\n","Step 45543   [1.498 sec/step, loss=0.19635, avg_loss=0.18760]\n","Step 45544   [1.508 sec/step, loss=0.18990, avg_loss=0.18791]\n","Step 45545   [1.517 sec/step, loss=0.19862, avg_loss=0.18806]\n","Step 45546   [1.521 sec/step, loss=0.16342, avg_loss=0.18777]\n","Generated 32 batches of size 32 in 10.507 sec\n","Step 45547   [1.522 sec/step, loss=0.17958, avg_loss=0.18785]\n","Step 45548   [1.513 sec/step, loss=0.16612, avg_loss=0.18758]\n","Step 45549   [1.504 sec/step, loss=0.17424, avg_loss=0.18742]\n","Step 45550   [1.497 sec/step, loss=0.17047, avg_loss=0.18709]\n","Step 45551   [1.491 sec/step, loss=0.19166, avg_loss=0.18700]\n","Step 45552   [1.488 sec/step, loss=0.18343, avg_loss=0.18695]\n","Step 45553   [1.487 sec/step, loss=0.18812, avg_loss=0.18686]\n","Step 45554   [1.490 sec/step, loss=0.19817, avg_loss=0.18701]\n","Step 45555   [1.484 sec/step, loss=0.16638, avg_loss=0.18664]\n","Step 45556   [1.490 sec/step, loss=0.19764, avg_loss=0.18692]\n","Step 45557   [1.487 sec/step, loss=0.18290, avg_loss=0.18675]\n","Step 45558   [1.485 sec/step, loss=0.18067, avg_loss=0.18658]\n","Step 45559   [1.482 sec/step, loss=0.17881, avg_loss=0.18640]\n","Step 45560   [1.476 sec/step, loss=0.16538, avg_loss=0.18608]\n","Step 45561   [1.470 sec/step, loss=0.16087, avg_loss=0.18573]\n","Step 45562   [1.471 sec/step, loss=0.18715, avg_loss=0.18569]\n","Step 45563   [1.470 sec/step, loss=0.19496, avg_loss=0.18592]\n","Step 45564   [1.471 sec/step, loss=0.20018, avg_loss=0.18595]\n","Step 45565   [1.471 sec/step, loss=0.19953, avg_loss=0.18598]\n","Step 45566   [1.471 sec/step, loss=0.19310, avg_loss=0.18598]\n","Step 45567   [1.475 sec/step, loss=0.19783, avg_loss=0.18618]\n","Step 45568   [1.474 sec/step, loss=0.17915, avg_loss=0.18612]\n","Step 45569   [1.474 sec/step, loss=0.19735, avg_loss=0.18603]\n","Step 45570   [1.480 sec/step, loss=0.19098, avg_loss=0.18620]\n","Step 45571   [1.475 sec/step, loss=0.17974, avg_loss=0.18607]\n","Step 45572   [1.482 sec/step, loss=0.19919, avg_loss=0.18643]\n","Step 45573   [1.485 sec/step, loss=0.19864, avg_loss=0.18656]\n","Step 45574   [1.484 sec/step, loss=0.16122, avg_loss=0.18641]\n","Step 45575   [1.487 sec/step, loss=0.16777, avg_loss=0.18641]\n","Step 45576   [1.489 sec/step, loss=0.17942, avg_loss=0.18637]\n","Step 45577   [1.496 sec/step, loss=0.19906, avg_loss=0.18663]\n","Step 45578   [1.500 sec/step, loss=0.19306, avg_loss=0.18656]\n","Step 45579   [1.505 sec/step, loss=0.20365, avg_loss=0.18666]\n","Generated 32 batches of size 32 in 10.140 sec\n","Step 45580   [1.504 sec/step, loss=0.17697, avg_loss=0.18663]\n","Step 45581   [1.507 sec/step, loss=0.19808, avg_loss=0.18698]\n","Step 45582   [1.501 sec/step, loss=0.17999, avg_loss=0.18691]\n","Step 45583   [1.496 sec/step, loss=0.19421, avg_loss=0.18693]\n","Step 45584   [1.498 sec/step, loss=0.21178, avg_loss=0.18729]\n","Step 45585   [1.494 sec/step, loss=0.20269, avg_loss=0.18722]\n","Step 45586   [1.490 sec/step, loss=0.18901, avg_loss=0.18707]\n","Step 45587   [1.497 sec/step, loss=0.19783, avg_loss=0.18738]\n","Step 45588   [1.497 sec/step, loss=0.19558, avg_loss=0.18738]\n","Step 45589   [1.502 sec/step, loss=0.19193, avg_loss=0.18769]\n","Step 45590   [1.499 sec/step, loss=0.17854, avg_loss=0.18761]\n","Step 45591   [1.499 sec/step, loss=0.19118, avg_loss=0.18746]\n","Step 45592   [1.498 sec/step, loss=0.19165, avg_loss=0.18734]\n","Step 45593   [1.493 sec/step, loss=0.18639, avg_loss=0.18720]\n","Step 45594   [1.496 sec/step, loss=0.20200, avg_loss=0.18739]\n","Step 45595   [1.490 sec/step, loss=0.16510, avg_loss=0.18702]\n","Step 45596   [1.489 sec/step, loss=0.16625, avg_loss=0.18672]\n","Step 45597   [1.494 sec/step, loss=0.19763, avg_loss=0.18693]\n","Step 45598   [1.497 sec/step, loss=0.19878, avg_loss=0.18707]\n","Step 45599   [1.493 sec/step, loss=0.16679, avg_loss=0.18674]\n","Step 45600   [1.489 sec/step, loss=0.17690, avg_loss=0.18634]\n","Writing summary at step: 45600\n","Step 45601   [1.489 sec/step, loss=0.21432, avg_loss=0.18651]\n","Step 45602   [1.488 sec/step, loss=0.16704, avg_loss=0.18636]\n","Step 45603   [1.486 sec/step, loss=0.18722, avg_loss=0.18627]\n","Step 45604   [1.483 sec/step, loss=0.17327, avg_loss=0.18600]\n","Step 45605   [1.484 sec/step, loss=0.18295, avg_loss=0.18585]\n","Step 45606   [1.482 sec/step, loss=0.16467, avg_loss=0.18557]\n","Step 45607   [1.486 sec/step, loss=0.18193, avg_loss=0.18565]\n","Step 45608   [1.486 sec/step, loss=0.15955, avg_loss=0.18538]\n","Step 45609   [1.491 sec/step, loss=0.20034, avg_loss=0.18538]\n","Step 45610   [1.488 sec/step, loss=0.17915, avg_loss=0.18536]\n","Generated 32 batches of size 32 in 10.601 sec\n","Step 45611   [1.490 sec/step, loss=0.19764, avg_loss=0.18566]\n","Step 45612   [1.483 sec/step, loss=0.17385, avg_loss=0.18541]\n","Step 45613   [1.485 sec/step, loss=0.20528, avg_loss=0.18580]\n","Step 45614   [1.482 sec/step, loss=0.16559, avg_loss=0.18583]\n","Step 45615   [1.482 sec/step, loss=0.19858, avg_loss=0.18603]\n","Step 45616   [1.486 sec/step, loss=0.19781, avg_loss=0.18638]\n","Step 45617   [1.486 sec/step, loss=0.19576, avg_loss=0.18641]\n","Step 45618   [1.490 sec/step, loss=0.20485, avg_loss=0.18671]\n","Step 45619   [1.485 sec/step, loss=0.18863, avg_loss=0.18649]\n","Step 45620   [1.481 sec/step, loss=0.16650, avg_loss=0.18618]\n","Step 45621   [1.486 sec/step, loss=0.19669, avg_loss=0.18645]\n","Step 45622   [1.486 sec/step, loss=0.16570, avg_loss=0.18646]\n","Step 45623   [1.483 sec/step, loss=0.16049, avg_loss=0.18611]\n","Step 45624   [1.490 sec/step, loss=0.20265, avg_loss=0.18653]\n","Step 45625   [1.486 sec/step, loss=0.19648, avg_loss=0.18649]\n","Step 45626   [1.484 sec/step, loss=0.19009, avg_loss=0.18643]\n","Step 45627   [1.481 sec/step, loss=0.18522, avg_loss=0.18634]\n","Step 45628   [1.474 sec/step, loss=0.18949, avg_loss=0.18624]\n","Step 45629   [1.481 sec/step, loss=0.20615, avg_loss=0.18659]\n","Step 45630   [1.481 sec/step, loss=0.21509, avg_loss=0.18679]\n","Step 45631   [1.481 sec/step, loss=0.20783, avg_loss=0.18695]\n","Step 45632   [1.478 sec/step, loss=0.19256, avg_loss=0.18690]\n","Step 45633   [1.484 sec/step, loss=0.20014, avg_loss=0.18729]\n","Step 45634   [1.481 sec/step, loss=0.20784, avg_loss=0.18741]\n","Step 45635   [1.485 sec/step, loss=0.20296, avg_loss=0.18762]\n","Step 45636   [1.485 sec/step, loss=0.20044, avg_loss=0.18765]\n","Step 45637   [1.485 sec/step, loss=0.19254, avg_loss=0.18764]\n","Step 45638   [1.486 sec/step, loss=0.18664, avg_loss=0.18760]\n","Step 45639   [1.493 sec/step, loss=0.20032, avg_loss=0.18775]\n","Step 45640   [1.494 sec/step, loss=0.17262, avg_loss=0.18768]\n","Step 45641   [1.498 sec/step, loss=0.19357, avg_loss=0.18771]\n","Step 45642   [1.495 sec/step, loss=0.17877, avg_loss=0.18763]\n","Generated 32 batches of size 32 in 10.435 sec\n","Step 45643   [1.492 sec/step, loss=0.20990, avg_loss=0.18777]\n","Step 45644   [1.484 sec/step, loss=0.17677, avg_loss=0.18764]\n","Step 45645   [1.480 sec/step, loss=0.19836, avg_loss=0.18763]\n","Step 45646   [1.475 sec/step, loss=0.19617, avg_loss=0.18796]\n","Step 45647   [1.477 sec/step, loss=0.21484, avg_loss=0.18831]\n","Step 45648   [1.476 sec/step, loss=0.16586, avg_loss=0.18831]\n","Step 45649   [1.477 sec/step, loss=0.19856, avg_loss=0.18855]\n","Step 45650   [1.480 sec/step, loss=0.20171, avg_loss=0.18887]\n","Step 45651   [1.481 sec/step, loss=0.19903, avg_loss=0.18894]\n","Step 45652   [1.480 sec/step, loss=0.18612, avg_loss=0.18897]\n","Step 45653   [1.480 sec/step, loss=0.21063, avg_loss=0.18919]\n","Step 45654   [1.475 sec/step, loss=0.19051, avg_loss=0.18912]\n","Step 45655   [1.481 sec/step, loss=0.20441, avg_loss=0.18950]\n","Step 45656   [1.479 sec/step, loss=0.18912, avg_loss=0.18941]\n","Step 45657   [1.481 sec/step, loss=0.20289, avg_loss=0.18961]\n","Step 45658   [1.481 sec/step, loss=0.18588, avg_loss=0.18966]\n","Step 45659   [1.484 sec/step, loss=0.19747, avg_loss=0.18985]\n","Step 45660   [1.487 sec/step, loss=0.18120, avg_loss=0.19001]\n","Step 45661   [1.493 sec/step, loss=0.19731, avg_loss=0.19037]\n","Step 45662   [1.491 sec/step, loss=0.16952, avg_loss=0.19020]\n","Step 45663   [1.491 sec/step, loss=0.19617, avg_loss=0.19021]\n","Step 45664   [1.485 sec/step, loss=0.16131, avg_loss=0.18982]\n","Step 45665   [1.485 sec/step, loss=0.19122, avg_loss=0.18974]\n","Step 45666   [1.479 sec/step, loss=0.16182, avg_loss=0.18942]\n","Step 45667   [1.479 sec/step, loss=0.19991, avg_loss=0.18944]\n","Step 45668   [1.477 sec/step, loss=0.16134, avg_loss=0.18927]\n","Step 45669   [1.482 sec/step, loss=0.19262, avg_loss=0.18922]\n","Step 45670   [1.486 sec/step, loss=0.20634, avg_loss=0.18937]\n","Step 45671   [1.491 sec/step, loss=0.16759, avg_loss=0.18925]\n","Step 45672   [1.492 sec/step, loss=0.17731, avg_loss=0.18903]\n","Step 45673   [1.496 sec/step, loss=0.19794, avg_loss=0.18902]\n","Generated 32 batches of size 32 in 10.566 sec\n","Step 45674   [1.498 sec/step, loss=0.17558, avg_loss=0.18917]\n","Step 45675   [1.496 sec/step, loss=0.17496, avg_loss=0.18924]\n","Step 45676   [1.493 sec/step, loss=0.17388, avg_loss=0.18918]\n","Step 45677   [1.491 sec/step, loss=0.19288, avg_loss=0.18912]\n","Step 45678   [1.487 sec/step, loss=0.19537, avg_loss=0.18915]\n","Step 45679   [1.483 sec/step, loss=0.19996, avg_loss=0.18911]\n","Step 45680   [1.482 sec/step, loss=0.18957, avg_loss=0.18924]\n","Step 45681   [1.481 sec/step, loss=0.18999, avg_loss=0.18915]\n","Step 45682   [1.483 sec/step, loss=0.18203, avg_loss=0.18917]\n","Step 45683   [1.478 sec/step, loss=0.16773, avg_loss=0.18891]\n","Step 45684   [1.477 sec/step, loss=0.19636, avg_loss=0.18876]\n","Step 45685   [1.477 sec/step, loss=0.19187, avg_loss=0.18865]\n","Step 45686   [1.480 sec/step, loss=0.18958, avg_loss=0.18865]\n","Step 45687   [1.475 sec/step, loss=0.18413, avg_loss=0.18852]\n","Step 45688   [1.470 sec/step, loss=0.16081, avg_loss=0.18817]\n","Step 45689   [1.465 sec/step, loss=0.15898, avg_loss=0.18784]\n","Step 45690   [1.471 sec/step, loss=0.20203, avg_loss=0.18807]\n","Step 45691   [1.471 sec/step, loss=0.19124, avg_loss=0.18807]\n","Step 45692   [1.473 sec/step, loss=0.19816, avg_loss=0.18814]\n","Step 45693   [1.475 sec/step, loss=0.19475, avg_loss=0.18822]\n","Step 45694   [1.471 sec/step, loss=0.17819, avg_loss=0.18799]\n","Step 45695   [1.471 sec/step, loss=0.16794, avg_loss=0.18801]\n","Step 45696   [1.467 sec/step, loss=0.16156, avg_loss=0.18797]\n","Step 45697   [1.463 sec/step, loss=0.17241, avg_loss=0.18771]\n","Step 45698   [1.460 sec/step, loss=0.17165, avg_loss=0.18744]\n","Step 45699   [1.458 sec/step, loss=0.16000, avg_loss=0.18738]\n","Step 45700   [1.461 sec/step, loss=0.20286, avg_loss=0.18763]\n","Writing summary at step: 45700\n","Step 45701   [1.462 sec/step, loss=0.18547, avg_loss=0.18735]\n","Step 45702   [1.472 sec/step, loss=0.20115, avg_loss=0.18769]\n","Step 45703   [1.480 sec/step, loss=0.20718, avg_loss=0.18789]\n","Step 45704   [1.486 sec/step, loss=0.19681, avg_loss=0.18812]\n","Generated 32 batches of size 32 in 10.434 sec\n","Step 45705   [1.486 sec/step, loss=0.19469, avg_loss=0.18824]\n","Step 45706   [1.489 sec/step, loss=0.19132, avg_loss=0.18851]\n","Step 45707   [1.488 sec/step, loss=0.19963, avg_loss=0.18868]\n","Step 45708   [1.488 sec/step, loss=0.17942, avg_loss=0.18888]\n","Step 45709   [1.483 sec/step, loss=0.19359, avg_loss=0.18881]\n","Step 45710   [1.484 sec/step, loss=0.19878, avg_loss=0.18901]\n","Step 45711   [1.483 sec/step, loss=0.19700, avg_loss=0.18900]\n","Step 45712   [1.483 sec/step, loss=0.17802, avg_loss=0.18905]\n","Step 45713   [1.484 sec/step, loss=0.19502, avg_loss=0.18894]\n","Step 45714   [1.484 sec/step, loss=0.16727, avg_loss=0.18896]\n","Step 45715   [1.485 sec/step, loss=0.19344, avg_loss=0.18891]\n","Step 45716   [1.479 sec/step, loss=0.15955, avg_loss=0.18853]\n","Step 45717   [1.479 sec/step, loss=0.20680, avg_loss=0.18864]\n","Step 45718   [1.473 sec/step, loss=0.15843, avg_loss=0.18817]\n","Step 45719   [1.475 sec/step, loss=0.18630, avg_loss=0.18815]\n","Step 45720   [1.479 sec/step, loss=0.19146, avg_loss=0.18840]\n","Step 45721   [1.476 sec/step, loss=0.17886, avg_loss=0.18822]\n","Step 45722   [1.479 sec/step, loss=0.18269, avg_loss=0.18839]\n","Step 45723   [1.481 sec/step, loss=0.17273, avg_loss=0.18851]\n","Step 45724   [1.481 sec/step, loss=0.20789, avg_loss=0.18857]\n","Step 45725   [1.481 sec/step, loss=0.16827, avg_loss=0.18828]\n","Step 45726   [1.484 sec/step, loss=0.19966, avg_loss=0.18838]\n","Step 45727   [1.486 sec/step, loss=0.19079, avg_loss=0.18843]\n","Step 45728   [1.488 sec/step, loss=0.16541, avg_loss=0.18819]\n","Step 45729   [1.488 sec/step, loss=0.19697, avg_loss=0.18810]\n","Step 45730   [1.483 sec/step, loss=0.17368, avg_loss=0.18769]\n","Step 45731   [1.483 sec/step, loss=0.19515, avg_loss=0.18756]\n","Step 45732   [1.487 sec/step, loss=0.18599, avg_loss=0.18750]\n","Step 45733   [1.484 sec/step, loss=0.16644, avg_loss=0.18716]\n","Step 45734   [1.491 sec/step, loss=0.19541, avg_loss=0.18703]\n","Step 45735   [1.488 sec/step, loss=0.15912, avg_loss=0.18660]\n","Step 45736   [1.492 sec/step, loss=0.19049, avg_loss=0.18650]\n","Step 45737   [1.493 sec/step, loss=0.18224, avg_loss=0.18639]\n","Generated 32 batches of size 32 in 10.518 sec\n","Step 45738   [1.493 sec/step, loss=0.18125, avg_loss=0.18634]\n","Step 45739   [1.489 sec/step, loss=0.19177, avg_loss=0.18625]\n","Step 45740   [1.492 sec/step, loss=0.20041, avg_loss=0.18653]\n","Step 45741   [1.482 sec/step, loss=0.16535, avg_loss=0.18625]\n","Step 45742   [1.484 sec/step, loss=0.19612, avg_loss=0.18642]\n","Step 45743   [1.477 sec/step, loss=0.15976, avg_loss=0.18592]\n","Step 45744   [1.475 sec/step, loss=0.15727, avg_loss=0.18573]\n","Step 45745   [1.475 sec/step, loss=0.19674, avg_loss=0.18571]\n","Step 45746   [1.475 sec/step, loss=0.19116, avg_loss=0.18566]\n","Step 45747   [1.476 sec/step, loss=0.19015, avg_loss=0.18541]\n","Step 45748   [1.482 sec/step, loss=0.18841, avg_loss=0.18564]\n","Step 45749   [1.483 sec/step, loss=0.18984, avg_loss=0.18555]\n","Step 45750   [1.478 sec/step, loss=0.18426, avg_loss=0.18538]\n","Step 45751   [1.475 sec/step, loss=0.18300, avg_loss=0.18522]\n","Step 45752   [1.477 sec/step, loss=0.19953, avg_loss=0.18535]\n","Step 45753   [1.475 sec/step, loss=0.17418, avg_loss=0.18499]\n","Step 45754   [1.478 sec/step, loss=0.19442, avg_loss=0.18503]\n","Step 45755   [1.475 sec/step, loss=0.18353, avg_loss=0.18482]\n","Step 45756   [1.478 sec/step, loss=0.20910, avg_loss=0.18502]\n","Step 45757   [1.474 sec/step, loss=0.17137, avg_loss=0.18470]\n","Step 45758   [1.476 sec/step, loss=0.19038, avg_loss=0.18475]\n","Step 45759   [1.476 sec/step, loss=0.19682, avg_loss=0.18474]\n","Step 45760   [1.478 sec/step, loss=0.18757, avg_loss=0.18480]\n","Step 45761   [1.475 sec/step, loss=0.18348, avg_loss=0.18467]\n","Step 45762   [1.477 sec/step, loss=0.17738, avg_loss=0.18474]\n","Step 45763   [1.472 sec/step, loss=0.17839, avg_loss=0.18457]\n","Step 45764   [1.484 sec/step, loss=0.19927, avg_loss=0.18495]\n","Step 45765   [1.489 sec/step, loss=0.19758, avg_loss=0.18501]\n","Step 45766   [1.501 sec/step, loss=0.19611, avg_loss=0.18535]\n","Step 45767   [1.506 sec/step, loss=0.19796, avg_loss=0.18533]\n","Step 45768   [1.513 sec/step, loss=0.18430, avg_loss=0.18556]\n","Generated 32 batches of size 32 in 10.794 sec\n","Step 45769   [1.504 sec/step, loss=0.16244, avg_loss=0.18526]\n","Step 45770   [1.499 sec/step, loss=0.19494, avg_loss=0.18515]\n","Step 45771   [1.497 sec/step, loss=0.17157, avg_loss=0.18519]\n","Step 45772   [1.491 sec/step, loss=0.17599, avg_loss=0.18517]\n","Step 45773   [1.487 sec/step, loss=0.19561, avg_loss=0.18515]\n","Step 45774   [1.488 sec/step, loss=0.20864, avg_loss=0.18548]\n","Step 45775   [1.490 sec/step, loss=0.17047, avg_loss=0.18544]\n","Step 45776   [1.489 sec/step, loss=0.17617, avg_loss=0.18546]\n","Step 45777   [1.490 sec/step, loss=0.19738, avg_loss=0.18550]\n","Step 45778   [1.489 sec/step, loss=0.19487, avg_loss=0.18550]\n","Step 45779   [1.484 sec/step, loss=0.16623, avg_loss=0.18516]\n","Step 45780   [1.487 sec/step, loss=0.19190, avg_loss=0.18518]\n","Step 45781   [1.487 sec/step, loss=0.18997, avg_loss=0.18518]\n","Step 45782   [1.490 sec/step, loss=0.19992, avg_loss=0.18536]\n","Step 45783   [1.496 sec/step, loss=0.20152, avg_loss=0.18570]\n","Step 45784   [1.494 sec/step, loss=0.18396, avg_loss=0.18558]\n","Step 45785   [1.488 sec/step, loss=0.16359, avg_loss=0.18529]\n","Step 45786   [1.488 sec/step, loss=0.15870, avg_loss=0.18499]\n","Step 45787   [1.485 sec/step, loss=0.15969, avg_loss=0.18474]\n","Step 45788   [1.488 sec/step, loss=0.18116, avg_loss=0.18494]\n","Step 45789   [1.490 sec/step, loss=0.18227, avg_loss=0.18518]\n","Step 45790   [1.485 sec/step, loss=0.16406, avg_loss=0.18480]\n","Step 45791   [1.483 sec/step, loss=0.18130, avg_loss=0.18470]\n","Step 45792   [1.482 sec/step, loss=0.19835, avg_loss=0.18470]\n","Step 45793   [1.481 sec/step, loss=0.17571, avg_loss=0.18451]\n","Step 45794   [1.479 sec/step, loss=0.16421, avg_loss=0.18437]\n","Step 45795   [1.483 sec/step, loss=0.18042, avg_loss=0.18450]\n","Step 45796   [1.491 sec/step, loss=0.20211, avg_loss=0.18490]\n","Step 45797   [1.499 sec/step, loss=0.19599, avg_loss=0.18514]\n","Step 45798   [1.507 sec/step, loss=0.20222, avg_loss=0.18544]\n","Step 45799   [1.518 sec/step, loss=0.20054, avg_loss=0.18585]\n","Step 45800   [1.518 sec/step, loss=0.18192, avg_loss=0.18564]\n","Writing summary at step: 45800\n","Generated 32 batches of size 32 in 10.467 sec\n","Step 45801   [1.517 sec/step, loss=0.19753, avg_loss=0.18576]\n","Step 45802   [1.513 sec/step, loss=0.19614, avg_loss=0.18571]\n","Step 45803   [1.508 sec/step, loss=0.19734, avg_loss=0.18561]\n","Step 45804   [1.504 sec/step, loss=0.20114, avg_loss=0.18565]\n","Step 45805   [1.503 sec/step, loss=0.19625, avg_loss=0.18567]\n","Step 45806   [1.503 sec/step, loss=0.19802, avg_loss=0.18574]\n","Step 45807   [1.503 sec/step, loss=0.19530, avg_loss=0.18569]\n","Step 45808   [1.500 sec/step, loss=0.16763, avg_loss=0.18557]\n","Step 45809   [1.498 sec/step, loss=0.17184, avg_loss=0.18536]\n","Step 45810   [1.493 sec/step, loss=0.17496, avg_loss=0.18512]\n","Step 45811   [1.492 sec/step, loss=0.19376, avg_loss=0.18509]\n","Step 45812   [1.491 sec/step, loss=0.16225, avg_loss=0.18493]\n","Step 45813   [1.490 sec/step, loss=0.18924, avg_loss=0.18487]\n","Step 45814   [1.494 sec/step, loss=0.18311, avg_loss=0.18503]\n","Step 45815   [1.491 sec/step, loss=0.18056, avg_loss=0.18490]\n","Step 45816   [1.493 sec/step, loss=0.18080, avg_loss=0.18511]\n","Step 45817   [1.487 sec/step, loss=0.16500, avg_loss=0.18470]\n","Step 45818   [1.494 sec/step, loss=0.20164, avg_loss=0.18513]\n","Step 45819   [1.496 sec/step, loss=0.20012, avg_loss=0.18527]\n","Step 45820   [1.491 sec/step, loss=0.15995, avg_loss=0.18495]\n","Step 45821   [1.491 sec/step, loss=0.17467, avg_loss=0.18491]\n","Step 45822   [1.488 sec/step, loss=0.15768, avg_loss=0.18466]\n","Step 45823   [1.487 sec/step, loss=0.17109, avg_loss=0.18464]\n","Step 45824   [1.484 sec/step, loss=0.19334, avg_loss=0.18450]\n","Step 45825   [1.484 sec/step, loss=0.18751, avg_loss=0.18469]\n","Step 45826   [1.484 sec/step, loss=0.20028, avg_loss=0.18470]\n","Step 45827   [1.490 sec/step, loss=0.20021, avg_loss=0.18479]\n","Step 45828   [1.499 sec/step, loss=0.19484, avg_loss=0.18508]\n","Step 45829   [1.498 sec/step, loss=0.16776, avg_loss=0.18479]\n","Step 45830   [1.507 sec/step, loss=0.19662, avg_loss=0.18502]\n","Step 45831   [1.512 sec/step, loss=0.19246, avg_loss=0.18499]\n","Generated 32 batches of size 32 in 10.706 sec\n","Step 45832   [1.513 sec/step, loss=0.19879, avg_loss=0.18512]\n","Step 45833   [1.516 sec/step, loss=0.19491, avg_loss=0.18541]\n","Step 45834   [1.513 sec/step, loss=0.19557, avg_loss=0.18541]\n","Step 45835   [1.513 sec/step, loss=0.20594, avg_loss=0.18588]\n","Step 45836   [1.508 sec/step, loss=0.19557, avg_loss=0.18593]\n","Step 45837   [1.508 sec/step, loss=0.19602, avg_loss=0.18607]\n","Step 45838   [1.508 sec/step, loss=0.19329, avg_loss=0.18619]\n","Step 45839   [1.504 sec/step, loss=0.17497, avg_loss=0.18602]\n","Step 45840   [1.499 sec/step, loss=0.16342, avg_loss=0.18565]\n","Step 45841   [1.498 sec/step, loss=0.15994, avg_loss=0.18559]\n","Step 45842   [1.498 sec/step, loss=0.19682, avg_loss=0.18560]\n","Step 45843   [1.500 sec/step, loss=0.15983, avg_loss=0.18560]\n","Step 45844   [1.505 sec/step, loss=0.19475, avg_loss=0.18598]\n","Step 45845   [1.503 sec/step, loss=0.18611, avg_loss=0.18587]\n","Step 45846   [1.503 sec/step, loss=0.19659, avg_loss=0.18592]\n","Step 45847   [1.503 sec/step, loss=0.19520, avg_loss=0.18597]\n","Step 45848   [1.504 sec/step, loss=0.19348, avg_loss=0.18602]\n","Step 45849   [1.504 sec/step, loss=0.18188, avg_loss=0.18595]\n","Step 45850   [1.509 sec/step, loss=0.19266, avg_loss=0.18603]\n","Step 45851   [1.511 sec/step, loss=0.19724, avg_loss=0.18617]\n","Step 45852   [1.509 sec/step, loss=0.16924, avg_loss=0.18587]\n","Step 45853   [1.510 sec/step, loss=0.18098, avg_loss=0.18594]\n","Step 45854   [1.511 sec/step, loss=0.19223, avg_loss=0.18591]\n","Step 45855   [1.508 sec/step, loss=0.17691, avg_loss=0.18585]\n","Step 45856   [1.505 sec/step, loss=0.18042, avg_loss=0.18556]\n","Step 45857   [1.510 sec/step, loss=0.18967, avg_loss=0.18574]\n","Step 45858   [1.510 sec/step, loss=0.19626, avg_loss=0.18580]\n","Step 45859   [1.507 sec/step, loss=0.16085, avg_loss=0.18544]\n","Step 45860   [1.504 sec/step, loss=0.15895, avg_loss=0.18516]\n","Step 45861   [1.511 sec/step, loss=0.19093, avg_loss=0.18523]\n","Step 45862   [1.515 sec/step, loss=0.17500, avg_loss=0.18521]\n","Step 45863   [1.521 sec/step, loss=0.18203, avg_loss=0.18525]\n","Step 45864   [1.521 sec/step, loss=0.19062, avg_loss=0.18516]\n","Generated 32 batches of size 32 in 10.403 sec\n","Step 45865   [1.513 sec/step, loss=0.17285, avg_loss=0.18491]\n","Step 45866   [1.506 sec/step, loss=0.18893, avg_loss=0.18484]\n","Step 45867   [1.497 sec/step, loss=0.17130, avg_loss=0.18457]\n","Step 45868   [1.495 sec/step, loss=0.18674, avg_loss=0.18460]\n","Step 45869   [1.498 sec/step, loss=0.17901, avg_loss=0.18476]\n","Step 45870   [1.494 sec/step, loss=0.16308, avg_loss=0.18444]\n","Step 45871   [1.494 sec/step, loss=0.17680, avg_loss=0.18450]\n","Step 45872   [1.495 sec/step, loss=0.16546, avg_loss=0.18439]\n","Step 45873   [1.495 sec/step, loss=0.18710, avg_loss=0.18431]\n","Step 45874   [1.495 sec/step, loss=0.18563, avg_loss=0.18408]\n","Step 45875   [1.498 sec/step, loss=0.19440, avg_loss=0.18432]\n","Step 45876   [1.497 sec/step, loss=0.17275, avg_loss=0.18428]\n","Step 45877   [1.491 sec/step, loss=0.16604, avg_loss=0.18397]\n","Step 45878   [1.493 sec/step, loss=0.20783, avg_loss=0.18410]\n","Step 45879   [1.499 sec/step, loss=0.19242, avg_loss=0.18436]\n","Step 45880   [1.501 sec/step, loss=0.19767, avg_loss=0.18442]\n","Step 45881   [1.498 sec/step, loss=0.18507, avg_loss=0.18437]\n","Step 45882   [1.498 sec/step, loss=0.19788, avg_loss=0.18435]\n","Step 45883   [1.498 sec/step, loss=0.19504, avg_loss=0.18428]\n","Step 45884   [1.501 sec/step, loss=0.19375, avg_loss=0.18438]\n","Step 45885   [1.505 sec/step, loss=0.17954, avg_loss=0.18454]\n","Step 45886   [1.499 sec/step, loss=0.18889, avg_loss=0.18484]\n","Step 45887   [1.506 sec/step, loss=0.19827, avg_loss=0.18523]\n","Step 45888   [1.504 sec/step, loss=0.18045, avg_loss=0.18522]\n","Step 45889   [1.504 sec/step, loss=0.17597, avg_loss=0.18516]\n","Step 45890   [1.508 sec/step, loss=0.19301, avg_loss=0.18545]\n","Step 45891   [1.515 sec/step, loss=0.18977, avg_loss=0.18553]\n","Step 45892   [1.520 sec/step, loss=0.19867, avg_loss=0.18554]\n","Step 45893   [1.528 sec/step, loss=0.20093, avg_loss=0.18579]\n","Step 45894   [1.533 sec/step, loss=0.18770, avg_loss=0.18602]\n","Step 45895   [1.537 sec/step, loss=0.18480, avg_loss=0.18607]\n","Generated 32 batches of size 32 in 10.434 sec\n","Step 45896   [1.530 sec/step, loss=0.16957, avg_loss=0.18574]\n","Step 45897   [1.522 sec/step, loss=0.17664, avg_loss=0.18555]\n","Step 45898   [1.512 sec/step, loss=0.16245, avg_loss=0.18515]\n","Step 45899   [1.507 sec/step, loss=0.20020, avg_loss=0.18515]\n","Step 45900   [1.503 sec/step, loss=0.16160, avg_loss=0.18494]\n","Writing summary at step: 45900\n","Step 45901   [1.503 sec/step, loss=0.20300, avg_loss=0.18500]\n","Step 45902   [1.497 sec/step, loss=0.16145, avg_loss=0.18465]\n","Step 45903   [1.493 sec/step, loss=0.18024, avg_loss=0.18448]\n","Step 45904   [1.493 sec/step, loss=0.19634, avg_loss=0.18443]\n","Step 45905   [1.494 sec/step, loss=0.19231, avg_loss=0.18439]\n","Step 45906   [1.490 sec/step, loss=0.16924, avg_loss=0.18410]\n","Step 45907   [1.486 sec/step, loss=0.17138, avg_loss=0.18387]\n","Step 45908   [1.492 sec/step, loss=0.19837, avg_loss=0.18417]\n","Step 45909   [1.494 sec/step, loss=0.18985, avg_loss=0.18435]\n","Step 45910   [1.495 sec/step, loss=0.18287, avg_loss=0.18443]\n","Step 45911   [1.493 sec/step, loss=0.18017, avg_loss=0.18430]\n","Step 45912   [1.495 sec/step, loss=0.17592, avg_loss=0.18443]\n","Step 45913   [1.494 sec/step, loss=0.18876, avg_loss=0.18443]\n","Step 45914   [1.497 sec/step, loss=0.19719, avg_loss=0.18457]\n","Step 45915   [1.499 sec/step, loss=0.19784, avg_loss=0.18474]\n","Step 45916   [1.503 sec/step, loss=0.19420, avg_loss=0.18488]\n","Step 45917   [1.506 sec/step, loss=0.16410, avg_loss=0.18487]\n","Step 45918   [1.505 sec/step, loss=0.19481, avg_loss=0.18480]\n","Step 45919   [1.506 sec/step, loss=0.19409, avg_loss=0.18474]\n","Step 45920   [1.511 sec/step, loss=0.19351, avg_loss=0.18507]\n","Step 45921   [1.512 sec/step, loss=0.19366, avg_loss=0.18526]\n","Step 45922   [1.523 sec/step, loss=0.18980, avg_loss=0.18558]\n","Step 45923   [1.528 sec/step, loss=0.18389, avg_loss=0.18571]\n","Step 45924   [1.535 sec/step, loss=0.19251, avg_loss=0.18570]\n","Step 45925   [1.537 sec/step, loss=0.17251, avg_loss=0.18555]\n","Step 45926   [1.538 sec/step, loss=0.18086, avg_loss=0.18536]\n","Generated 32 batches of size 32 in 10.468 sec\n","Step 45927   [1.535 sec/step, loss=0.19676, avg_loss=0.18533]\n","Step 45928   [1.525 sec/step, loss=0.16736, avg_loss=0.18505]\n","Step 45929   [1.525 sec/step, loss=0.19035, avg_loss=0.18528]\n","Step 45930   [1.515 sec/step, loss=0.16130, avg_loss=0.18492]\n","Step 45931   [1.510 sec/step, loss=0.19512, avg_loss=0.18495]\n","Step 45932   [1.501 sec/step, loss=0.15915, avg_loss=0.18455]\n","Step 45933   [1.501 sec/step, loss=0.19177, avg_loss=0.18452]\n","Step 45934   [1.501 sec/step, loss=0.19265, avg_loss=0.18449]\n","Step 45935   [1.499 sec/step, loss=0.17375, avg_loss=0.18417]\n","Step 45936   [1.495 sec/step, loss=0.17074, avg_loss=0.18392]\n","Step 45937   [1.489 sec/step, loss=0.16398, avg_loss=0.18360]\n","Step 45938   [1.487 sec/step, loss=0.16904, avg_loss=0.18336]\n","Step 45939   [1.488 sec/step, loss=0.18608, avg_loss=0.18347]\n","Step 45940   [1.493 sec/step, loss=0.19271, avg_loss=0.18376]\n","Step 45941   [1.499 sec/step, loss=0.19670, avg_loss=0.18413]\n","Step 45942   [1.493 sec/step, loss=0.16179, avg_loss=0.18378]\n","Step 45943   [1.496 sec/step, loss=0.19820, avg_loss=0.18417]\n","Step 45944   [1.497 sec/step, loss=0.19431, avg_loss=0.18416]\n","Step 45945   [1.499 sec/step, loss=0.19047, avg_loss=0.18420]\n","Step 45946   [1.499 sec/step, loss=0.19326, avg_loss=0.18417]\n","Step 45947   [1.496 sec/step, loss=0.18447, avg_loss=0.18406]\n","Step 45948   [1.495 sec/step, loss=0.18878, avg_loss=0.18402]\n","Step 45949   [1.494 sec/step, loss=0.17998, avg_loss=0.18400]\n","Step 45950   [1.491 sec/step, loss=0.17806, avg_loss=0.18385]\n","Step 45951   [1.492 sec/step, loss=0.19664, avg_loss=0.18385]\n","Step 45952   [1.490 sec/step, loss=0.16330, avg_loss=0.18379]\n","Step 45953   [1.488 sec/step, loss=0.18502, avg_loss=0.18383]\n","Step 45954   [1.494 sec/step, loss=0.19121, avg_loss=0.18382]\n","Step 45955   [1.505 sec/step, loss=0.19466, avg_loss=0.18399]\n","Step 45956   [1.511 sec/step, loss=0.19025, avg_loss=0.18409]\n","Step 45957   [1.509 sec/step, loss=0.16088, avg_loss=0.18380]\n","Step 45958   [1.513 sec/step, loss=0.18734, avg_loss=0.18372]\n","Generated 32 batches of size 32 in 10.732 sec\n","Step 45959   [1.518 sec/step, loss=0.19430, avg_loss=0.18405]\n","Step 45960   [1.518 sec/step, loss=0.17493, avg_loss=0.18421]\n","Step 45961   [1.511 sec/step, loss=0.19541, avg_loss=0.18425]\n","Step 45962   [1.507 sec/step, loss=0.16114, avg_loss=0.18412]\n","Step 45963   [1.506 sec/step, loss=0.19138, avg_loss=0.18421]\n","Step 45964   [1.499 sec/step, loss=0.16574, avg_loss=0.18396]\n","Step 45965   [1.501 sec/step, loss=0.18966, avg_loss=0.18413]\n","Step 45966   [1.502 sec/step, loss=0.19725, avg_loss=0.18421]\n","Step 45967   [1.500 sec/step, loss=0.16025, avg_loss=0.18410]\n","Step 45968   [1.497 sec/step, loss=0.17523, avg_loss=0.18399]\n","Step 45969   [1.493 sec/step, loss=0.15651, avg_loss=0.18376]\n","Step 45970   [1.495 sec/step, loss=0.18279, avg_loss=0.18396]\n","Step 45971   [1.493 sec/step, loss=0.15719, avg_loss=0.18376]\n","Step 45972   [1.490 sec/step, loss=0.15777, avg_loss=0.18369]\n","Step 45973   [1.488 sec/step, loss=0.17772, avg_loss=0.18359]\n","Step 45974   [1.482 sec/step, loss=0.15539, avg_loss=0.18329]\n","Step 45975   [1.480 sec/step, loss=0.18960, avg_loss=0.18324]\n","Step 45976   [1.486 sec/step, loss=0.19673, avg_loss=0.18348]\n","Step 45977   [1.487 sec/step, loss=0.17184, avg_loss=0.18354]\n","Step 45978   [1.482 sec/step, loss=0.16963, avg_loss=0.18316]\n","Step 45979   [1.481 sec/step, loss=0.19325, avg_loss=0.18317]\n","Step 45980   [1.477 sec/step, loss=0.18151, avg_loss=0.18300]\n","Step 45981   [1.479 sec/step, loss=0.19729, avg_loss=0.18313]\n","Step 45982   [1.479 sec/step, loss=0.19493, avg_loss=0.18310]\n","Step 45983   [1.479 sec/step, loss=0.19340, avg_loss=0.18308]\n","Step 45984   [1.478 sec/step, loss=0.19045, avg_loss=0.18305]\n","Step 45985   [1.480 sec/step, loss=0.19894, avg_loss=0.18324]\n","Step 45986   [1.487 sec/step, loss=0.18036, avg_loss=0.18316]\n","Step 45987   [1.490 sec/step, loss=0.19811, avg_loss=0.18315]\n","Step 45988   [1.499 sec/step, loss=0.18857, avg_loss=0.18324]\n","Step 45989   [1.502 sec/step, loss=0.18394, avg_loss=0.18332]\n","Step 45990   [1.507 sec/step, loss=0.19454, avg_loss=0.18333]\n","Generated 32 batches of size 32 in 10.440 sec\n","Step 45991   [1.499 sec/step, loss=0.16702, avg_loss=0.18310]\n","Step 45992   [1.493 sec/step, loss=0.18865, avg_loss=0.18300]\n","Step 45993   [1.486 sec/step, loss=0.17987, avg_loss=0.18279]\n","Step 45994   [1.488 sec/step, loss=0.19533, avg_loss=0.18287]\n","Step 45995   [1.482 sec/step, loss=0.17574, avg_loss=0.18278]\n","Step 45996   [1.482 sec/step, loss=0.17710, avg_loss=0.18285]\n","Step 45997   [1.484 sec/step, loss=0.17637, avg_loss=0.18285]\n","Step 45998   [1.489 sec/step, loss=0.18890, avg_loss=0.18312]\n","Step 45999   [1.489 sec/step, loss=0.19525, avg_loss=0.18307]\n","Step 46000   [1.494 sec/step, loss=0.19292, avg_loss=0.18338]\n","Writing summary at step: 46000\n","Saving checkpoint to: logdir-tacotron2/son_2021-02-06_23-59-17/model.ckpt-46000\n","Saving audio and alignment...\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50504 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44148 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50732 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47540 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51080 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46020 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47197 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46304 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51424 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50504 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44148 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50732 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47540 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51080 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46020 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47197 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46304 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51424 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/train-step-000046000-align000.png\n","100% 1/1 [00:02<00:00,  2.68s/it]\n","Test finished for step 46000.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000046000-align000.png\n"," 50% 1/2 [00:12<00:12, 12.02s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000046000-align001.png\n","100% 2/2 [00:23<00:00, 11.79s/it]\n","Test finished for step 46000.\n","Step 46001   [1.491 sec/step, loss=0.17007, avg_loss=0.18305]\n","Step 46002   [1.494 sec/step, loss=0.18121, avg_loss=0.18325]\n","Step 46003   [1.492 sec/step, loss=0.16550, avg_loss=0.18310]\n","Step 46004   [1.491 sec/step, loss=0.18815, avg_loss=0.18302]\n","Step 46005   [1.491 sec/step, loss=0.19781, avg_loss=0.18307]\n","Step 46006   [1.490 sec/step, loss=0.16072, avg_loss=0.18299]\n","Step 46007   [1.488 sec/step, loss=0.16010, avg_loss=0.18287]\n","Step 46008   [1.487 sec/step, loss=0.18621, avg_loss=0.18275]\n","Step 46009   [1.487 sec/step, loss=0.20002, avg_loss=0.18286]\n","Step 46010   [1.491 sec/step, loss=0.19638, avg_loss=0.18299]\n","Step 46011   [1.489 sec/step, loss=0.17181, avg_loss=0.18291]\n","Step 46012   [1.493 sec/step, loss=0.19621, avg_loss=0.18311]\n","Step 46013   [1.493 sec/step, loss=0.18737, avg_loss=0.18310]\n","Step 46014   [1.494 sec/step, loss=0.19340, avg_loss=0.18306]\n","Step 46015   [1.490 sec/step, loss=0.17100, avg_loss=0.18279]\n","Step 46016   [1.487 sec/step, loss=0.16547, avg_loss=0.18250]\n","Step 46017   [1.495 sec/step, loss=0.19435, avg_loss=0.18280]\n","Step 46018   [1.499 sec/step, loss=0.18674, avg_loss=0.18272]\n","Step 46019   [1.503 sec/step, loss=0.19263, avg_loss=0.18271]\n","Step 46020   [1.501 sec/step, loss=0.15817, avg_loss=0.18236]\n","Step 46021   [1.503 sec/step, loss=0.17086, avg_loss=0.18213]\n","Generated 32 batches of size 32 in 10.526 sec\n","Step 46022   [1.496 sec/step, loss=0.18947, avg_loss=0.18212]\n","Step 46023   [1.494 sec/step, loss=0.16058, avg_loss=0.18189]\n","Step 46024   [1.490 sec/step, loss=0.20530, avg_loss=0.18202]\n","Step 46025   [1.491 sec/step, loss=0.20306, avg_loss=0.18232]\n","Step 46026   [1.489 sec/step, loss=0.19359, avg_loss=0.18245]\n","Step 46027   [1.485 sec/step, loss=0.19574, avg_loss=0.18244]\n","Step 46028   [1.491 sec/step, loss=0.18696, avg_loss=0.18264]\n","Step 46029   [1.488 sec/step, loss=0.19809, avg_loss=0.18272]\n","Step 46030   [1.492 sec/step, loss=0.19156, avg_loss=0.18302]\n","Step 46031   [1.492 sec/step, loss=0.20124, avg_loss=0.18308]\n","Step 46032   [1.498 sec/step, loss=0.20037, avg_loss=0.18349]\n","Step 46033   [1.495 sec/step, loss=0.17786, avg_loss=0.18335]\n","Step 46034   [1.492 sec/step, loss=0.17112, avg_loss=0.18314]\n","Step 46035   [1.496 sec/step, loss=0.19765, avg_loss=0.18338]\n","Step 46036   [1.500 sec/step, loss=0.19758, avg_loss=0.18364]\n","Step 46037   [1.500 sec/step, loss=0.16816, avg_loss=0.18369]\n","Step 46038   [1.502 sec/step, loss=0.19129, avg_loss=0.18391]\n","Step 46039   [1.500 sec/step, loss=0.19204, avg_loss=0.18397]\n","Step 46040   [1.501 sec/step, loss=0.19542, avg_loss=0.18400]\n","Step 46041   [1.501 sec/step, loss=0.18858, avg_loss=0.18391]\n","Step 46042   [1.503 sec/step, loss=0.17529, avg_loss=0.18405]\n","Step 46043   [1.499 sec/step, loss=0.16365, avg_loss=0.18370]\n","Step 46044   [1.499 sec/step, loss=0.19653, avg_loss=0.18373]\n","Step 46045   [1.497 sec/step, loss=0.20346, avg_loss=0.18386]\n","Step 46046   [1.491 sec/step, loss=0.16661, avg_loss=0.18359]\n","Step 46047   [1.494 sec/step, loss=0.19500, avg_loss=0.18369]\n","Step 46048   [1.493 sec/step, loss=0.18303, avg_loss=0.18364]\n","Step 46049   [1.499 sec/step, loss=0.21547, avg_loss=0.18399]\n","Step 46050   [1.503 sec/step, loss=0.18425, avg_loss=0.18405]\n","Step 46051   [1.504 sec/step, loss=0.18226, avg_loss=0.18391]\n","Step 46052   [1.506 sec/step, loss=0.16868, avg_loss=0.18396]\n","Step 46053   [1.506 sec/step, loss=0.16434, avg_loss=0.18376]\n","Generated 32 batches of size 32 in 10.465 sec\n","Step 46054   [1.497 sec/step, loss=0.16028, avg_loss=0.18345]\n","Step 46055   [1.491 sec/step, loss=0.19963, avg_loss=0.18350]\n","Step 46056   [1.486 sec/step, loss=0.19857, avg_loss=0.18358]\n","Step 46057   [1.483 sec/step, loss=0.17272, avg_loss=0.18370]\n","Step 46058   [1.473 sec/step, loss=0.16903, avg_loss=0.18352]\n","Step 46059   [1.468 sec/step, loss=0.19464, avg_loss=0.18352]\n","Step 46060   [1.468 sec/step, loss=0.18505, avg_loss=0.18362]\n","Step 46061   [1.471 sec/step, loss=0.20490, avg_loss=0.18372]\n","Step 46062   [1.476 sec/step, loss=0.19981, avg_loss=0.18410]\n","Step 46063   [1.475 sec/step, loss=0.19607, avg_loss=0.18415]\n","Step 46064   [1.474 sec/step, loss=0.17628, avg_loss=0.18425]\n","Step 46065   [1.474 sec/step, loss=0.19110, avg_loss=0.18427]\n","Step 46066   [1.473 sec/step, loss=0.18912, avg_loss=0.18419]\n","Step 46067   [1.475 sec/step, loss=0.16651, avg_loss=0.18425]\n","Step 46068   [1.478 sec/step, loss=0.20699, avg_loss=0.18457]\n","Step 46069   [1.482 sec/step, loss=0.18846, avg_loss=0.18489]\n","Step 46070   [1.478 sec/step, loss=0.18138, avg_loss=0.18487]\n","Step 46071   [1.482 sec/step, loss=0.19800, avg_loss=0.18528]\n","Step 46072   [1.483 sec/step, loss=0.17951, avg_loss=0.18550]\n","Step 46073   [1.482 sec/step, loss=0.17538, avg_loss=0.18548]\n","Step 46074   [1.488 sec/step, loss=0.22844, avg_loss=0.18621]\n","Step 46075   [1.486 sec/step, loss=0.16905, avg_loss=0.18600]\n","Step 46076   [1.483 sec/step, loss=0.18388, avg_loss=0.18587]\n","Step 46077   [1.488 sec/step, loss=0.20283, avg_loss=0.18618]\n","Step 46078   [1.489 sec/step, loss=0.18178, avg_loss=0.18630]\n","Step 46079   [1.490 sec/step, loss=0.19890, avg_loss=0.18636]\n","Step 46080   [1.497 sec/step, loss=0.19619, avg_loss=0.18651]\n","Step 46081   [1.493 sec/step, loss=0.17424, avg_loss=0.18628]\n","Step 46082   [1.497 sec/step, loss=0.21207, avg_loss=0.18645]\n","Step 46083   [1.501 sec/step, loss=0.19848, avg_loss=0.18650]\n","Step 46084   [1.505 sec/step, loss=0.16526, avg_loss=0.18625]\n","Generated 32 batches of size 32 in 10.430 sec\n","Step 46085   [1.508 sec/step, loss=0.19333, avg_loss=0.18619]\n","Step 46086   [1.506 sec/step, loss=0.19986, avg_loss=0.18638]\n","Step 46087   [1.502 sec/step, loss=0.19690, avg_loss=0.18637]\n","Step 46088   [1.492 sec/step, loss=0.16657, avg_loss=0.18615]\n","Step 46089   [1.493 sec/step, loss=0.19615, avg_loss=0.18627]\n","Step 46090   [1.488 sec/step, loss=0.20165, avg_loss=0.18635]\n","Step 46091   [1.487 sec/step, loss=0.19362, avg_loss=0.18661]\n","Step 46092   [1.481 sec/step, loss=0.16127, avg_loss=0.18634]\n","Step 46093   [1.484 sec/step, loss=0.20195, avg_loss=0.18656]\n","Step 46094   [1.480 sec/step, loss=0.16346, avg_loss=0.18624]\n","Step 46095   [1.480 sec/step, loss=0.17418, avg_loss=0.18622]\n","Step 46096   [1.477 sec/step, loss=0.16020, avg_loss=0.18606]\n","Step 46097   [1.479 sec/step, loss=0.19297, avg_loss=0.18622]\n","Step 46098   [1.476 sec/step, loss=0.18857, avg_loss=0.18622]\n","Step 46099   [1.472 sec/step, loss=0.17639, avg_loss=0.18603]\n","Step 46100   [1.473 sec/step, loss=0.20798, avg_loss=0.18618]\n","Writing summary at step: 46100\n","Step 46101   [1.471 sec/step, loss=0.17291, avg_loss=0.18621]\n","Step 46102   [1.471 sec/step, loss=0.18083, avg_loss=0.18620]\n","Step 46103   [1.477 sec/step, loss=0.19757, avg_loss=0.18653]\n","Step 46104   [1.472 sec/step, loss=0.16580, avg_loss=0.18630]\n","Step 46105   [1.471 sec/step, loss=0.19423, avg_loss=0.18627]\n","Step 46106   [1.475 sec/step, loss=0.19572, avg_loss=0.18662]\n","Step 46107   [1.480 sec/step, loss=0.18946, avg_loss=0.18691]\n","Step 46108   [1.478 sec/step, loss=0.17117, avg_loss=0.18676]\n","Step 46109   [1.478 sec/step, loss=0.20724, avg_loss=0.18683]\n","Step 46110   [1.472 sec/step, loss=0.16108, avg_loss=0.18648]\n","Step 46111   [1.480 sec/step, loss=0.19857, avg_loss=0.18675]\n","Step 46112   [1.481 sec/step, loss=0.18390, avg_loss=0.18662]\n","Step 46113   [1.482 sec/step, loss=0.18165, avg_loss=0.18657]\n","Step 46114   [1.486 sec/step, loss=0.19585, avg_loss=0.18659]\n","Step 46115   [1.495 sec/step, loss=0.20220, avg_loss=0.18690]\n","Generated 32 batches of size 32 in 10.452 sec\n","Step 46116   [1.494 sec/step, loss=0.16193, avg_loss=0.18687]\n","Step 46117   [1.487 sec/step, loss=0.18282, avg_loss=0.18675]\n","Step 46118   [1.483 sec/step, loss=0.19480, avg_loss=0.18683]\n","Step 46119   [1.478 sec/step, loss=0.19822, avg_loss=0.18689]\n","Step 46120   [1.481 sec/step, loss=0.19623, avg_loss=0.18727]\n","Step 46121   [1.481 sec/step, loss=0.19466, avg_loss=0.18751]\n","Step 46122   [1.482 sec/step, loss=0.19647, avg_loss=0.18758]\n","Step 46123   [1.483 sec/step, loss=0.19241, avg_loss=0.18789]\n","Step 46124   [1.481 sec/step, loss=0.18273, avg_loss=0.18767]\n","Step 46125   [1.476 sec/step, loss=0.17605, avg_loss=0.18740]\n","Step 46126   [1.471 sec/step, loss=0.16330, avg_loss=0.18710]\n","Step 46127   [1.465 sec/step, loss=0.15865, avg_loss=0.18673]\n","Step 46128   [1.465 sec/step, loss=0.19559, avg_loss=0.18681]\n","Step 46129   [1.462 sec/step, loss=0.15950, avg_loss=0.18643]\n","Step 46130   [1.465 sec/step, loss=0.19157, avg_loss=0.18643]\n","Step 46131   [1.458 sec/step, loss=0.15802, avg_loss=0.18599]\n","Step 46132   [1.456 sec/step, loss=0.18148, avg_loss=0.18580]\n","Step 46133   [1.457 sec/step, loss=0.17913, avg_loss=0.18582]\n","Step 46134   [1.460 sec/step, loss=0.19454, avg_loss=0.18605]\n","Step 46135   [1.460 sec/step, loss=0.19656, avg_loss=0.18604]\n","Step 46136   [1.460 sec/step, loss=0.20278, avg_loss=0.18609]\n","Step 46137   [1.466 sec/step, loss=0.19021, avg_loss=0.18631]\n","Step 46138   [1.462 sec/step, loss=0.16656, avg_loss=0.18607]\n","Step 46139   [1.463 sec/step, loss=0.17579, avg_loss=0.18590]\n","Step 46140   [1.458 sec/step, loss=0.17201, avg_loss=0.18567]\n","Step 46141   [1.459 sec/step, loss=0.19577, avg_loss=0.18574]\n","Step 46142   [1.460 sec/step, loss=0.19308, avg_loss=0.18592]\n","Step 46143   [1.465 sec/step, loss=0.17422, avg_loss=0.18602]\n","Step 46144   [1.469 sec/step, loss=0.19853, avg_loss=0.18604]\n","Step 46145   [1.472 sec/step, loss=0.17906, avg_loss=0.18580]\n","Step 46146   [1.475 sec/step, loss=0.16664, avg_loss=0.18580]\n","Step 46147   [1.479 sec/step, loss=0.19787, avg_loss=0.18583]\n","Generated 32 batches of size 32 in 10.545 sec\n","Step 46148   [1.484 sec/step, loss=0.19658, avg_loss=0.18597]\n","Step 46149   [1.480 sec/step, loss=0.19278, avg_loss=0.18574]\n","Step 46150   [1.474 sec/step, loss=0.18271, avg_loss=0.18572]\n","Step 46151   [1.472 sec/step, loss=0.18888, avg_loss=0.18579]\n","Step 46152   [1.471 sec/step, loss=0.17531, avg_loss=0.18586]\n","Step 46153   [1.470 sec/step, loss=0.17162, avg_loss=0.18593]\n","Step 46154   [1.474 sec/step, loss=0.18903, avg_loss=0.18622]\n","Step 46155   [1.474 sec/step, loss=0.19944, avg_loss=0.18621]\n","Step 46156   [1.473 sec/step, loss=0.17652, avg_loss=0.18599]\n","Step 46157   [1.476 sec/step, loss=0.19768, avg_loss=0.18624]\n","Step 46158   [1.480 sec/step, loss=0.18830, avg_loss=0.18644]\n","Step 46159   [1.482 sec/step, loss=0.19683, avg_loss=0.18646]\n","Step 46160   [1.480 sec/step, loss=0.17486, avg_loss=0.18636]\n","Step 46161   [1.480 sec/step, loss=0.19901, avg_loss=0.18630]\n","Step 46162   [1.474 sec/step, loss=0.16131, avg_loss=0.18591]\n","Step 46163   [1.475 sec/step, loss=0.19697, avg_loss=0.18592]\n","Step 46164   [1.472 sec/step, loss=0.16209, avg_loss=0.18578]\n","Step 46165   [1.471 sec/step, loss=0.21142, avg_loss=0.18598]\n","Step 46166   [1.467 sec/step, loss=0.18802, avg_loss=0.18597]\n","Step 46167   [1.469 sec/step, loss=0.19944, avg_loss=0.18630]\n","Step 46168   [1.469 sec/step, loss=0.20011, avg_loss=0.18623]\n","Step 46169   [1.469 sec/step, loss=0.18513, avg_loss=0.18620]\n","Step 46170   [1.475 sec/step, loss=0.21270, avg_loss=0.18651]\n","Step 46171   [1.470 sec/step, loss=0.17479, avg_loss=0.18628]\n","Step 46172   [1.474 sec/step, loss=0.19367, avg_loss=0.18642]\n","Step 46173   [1.472 sec/step, loss=0.16513, avg_loss=0.18632]\n","Step 46174   [1.467 sec/step, loss=0.17890, avg_loss=0.18582]\n","Step 46175   [1.473 sec/step, loss=0.19148, avg_loss=0.18605]\n","Step 46176   [1.479 sec/step, loss=0.20049, avg_loss=0.18621]\n","Step 46177   [1.484 sec/step, loss=0.19730, avg_loss=0.18616]\n","Step 46178   [1.491 sec/step, loss=0.20523, avg_loss=0.18639]\n","Step 46179   [1.490 sec/step, loss=0.17174, avg_loss=0.18612]\n","Generated 32 batches of size 32 in 10.492 sec\n","Step 46180   [1.488 sec/step, loss=0.19928, avg_loss=0.18615]\n","Step 46181   [1.491 sec/step, loss=0.20133, avg_loss=0.18642]\n","Step 46182   [1.488 sec/step, loss=0.20189, avg_loss=0.18632]\n","Step 46183   [1.483 sec/step, loss=0.19396, avg_loss=0.18628]\n","Step 46184   [1.477 sec/step, loss=0.22305, avg_loss=0.18685]\n","Step 46185   [1.474 sec/step, loss=0.19127, avg_loss=0.18683]\n","Step 46186   [1.472 sec/step, loss=0.20388, avg_loss=0.18687]\n","Step 46187   [1.468 sec/step, loss=0.17772, avg_loss=0.18668]\n","Step 46188   [1.473 sec/step, loss=0.18930, avg_loss=0.18691]\n","Step 46189   [1.473 sec/step, loss=0.19443, avg_loss=0.18689]\n","Step 46190   [1.471 sec/step, loss=0.18692, avg_loss=0.18674]\n","Step 46191   [1.470 sec/step, loss=0.17775, avg_loss=0.18659]\n","Step 46192   [1.471 sec/step, loss=0.17602, avg_loss=0.18673]\n","Step 46193   [1.471 sec/step, loss=0.19219, avg_loss=0.18664]\n","Step 46194   [1.475 sec/step, loss=0.19712, avg_loss=0.18697]\n","Step 46195   [1.477 sec/step, loss=0.17021, avg_loss=0.18693]\n","Step 46196   [1.483 sec/step, loss=0.20341, avg_loss=0.18736]\n","Step 46197   [1.484 sec/step, loss=0.21573, avg_loss=0.18759]\n","Step 46198   [1.480 sec/step, loss=0.16497, avg_loss=0.18736]\n","Step 46199   [1.482 sec/step, loss=0.18954, avg_loss=0.18749]\n","Step 46200   [1.482 sec/step, loss=0.19023, avg_loss=0.18731]\n","Writing summary at step: 46200\n","Step 46201   [1.480 sec/step, loss=0.16025, avg_loss=0.18718]\n","Step 46202   [1.483 sec/step, loss=0.19963, avg_loss=0.18737]\n","Step 46203   [1.483 sec/step, loss=0.19742, avg_loss=0.18737]\n","Step 46204   [1.490 sec/step, loss=0.18692, avg_loss=0.18758]\n","Step 46205   [1.486 sec/step, loss=0.17803, avg_loss=0.18742]\n","Step 46206   [1.491 sec/step, loss=0.19770, avg_loss=0.18744]\n","Step 46207   [1.491 sec/step, loss=0.17164, avg_loss=0.18726]\n","Step 46208   [1.491 sec/step, loss=0.16619, avg_loss=0.18721]\n","Step 46209   [1.490 sec/step, loss=0.16125, avg_loss=0.18675]\n","Step 46210   [1.501 sec/step, loss=0.20283, avg_loss=0.18717]\n","Generated 32 batches of size 32 in 10.516 sec\n","Step 46211   [1.501 sec/step, loss=0.16240, avg_loss=0.18681]\n","Step 46212   [1.500 sec/step, loss=0.19737, avg_loss=0.18694]\n","Step 46213   [1.499 sec/step, loss=0.19703, avg_loss=0.18710]\n","Step 46214   [1.495 sec/step, loss=0.19378, avg_loss=0.18708]\n","Step 46215   [1.491 sec/step, loss=0.19440, avg_loss=0.18700]\n","Step 46216   [1.494 sec/step, loss=0.19069, avg_loss=0.18728]\n","Step 46217   [1.494 sec/step, loss=0.18161, avg_loss=0.18727]\n","Step 46218   [1.494 sec/step, loss=0.19390, avg_loss=0.18726]\n","Step 46219   [1.494 sec/step, loss=0.18636, avg_loss=0.18714]\n","Step 46220   [1.491 sec/step, loss=0.17857, avg_loss=0.18697]\n","Step 46221   [1.490 sec/step, loss=0.18463, avg_loss=0.18687]\n","Step 46222   [1.491 sec/step, loss=0.19607, avg_loss=0.18686]\n","Step 46223   [1.486 sec/step, loss=0.16932, avg_loss=0.18663]\n","Step 46224   [1.482 sec/step, loss=0.16862, avg_loss=0.18649]\n","Step 46225   [1.484 sec/step, loss=0.17950, avg_loss=0.18653]\n","Step 46226   [1.490 sec/step, loss=0.19520, avg_loss=0.18685]\n","Step 46227   [1.492 sec/step, loss=0.17391, avg_loss=0.18700]\n","Step 46228   [1.491 sec/step, loss=0.17761, avg_loss=0.18682]\n","Step 46229   [1.497 sec/step, loss=0.18972, avg_loss=0.18712]\n","Step 46230   [1.496 sec/step, loss=0.19135, avg_loss=0.18712]\n","Step 46231   [1.498 sec/step, loss=0.17099, avg_loss=0.18725]\n","Step 46232   [1.495 sec/step, loss=0.16728, avg_loss=0.18711]\n","Step 46233   [1.493 sec/step, loss=0.16983, avg_loss=0.18701]\n","Step 46234   [1.491 sec/step, loss=0.17880, avg_loss=0.18686]\n","Step 46235   [1.484 sec/step, loss=0.15863, avg_loss=0.18648]\n","Step 46236   [1.480 sec/step, loss=0.16931, avg_loss=0.18614]\n","Step 46237   [1.480 sec/step, loss=0.19001, avg_loss=0.18614]\n","Step 46238   [1.481 sec/step, loss=0.15788, avg_loss=0.18605]\n","Step 46239   [1.490 sec/step, loss=0.19787, avg_loss=0.18627]\n","Step 46240   [1.499 sec/step, loss=0.19519, avg_loss=0.18651]\n","Step 46241   [1.500 sec/step, loss=0.16753, avg_loss=0.18622]\n","Step 46242   [1.499 sec/step, loss=0.15908, avg_loss=0.18588]\n","Generated 32 batches of size 32 in 10.564 sec\n","Step 46243   [1.502 sec/step, loss=0.19739, avg_loss=0.18611]\n","Step 46244   [1.500 sec/step, loss=0.19485, avg_loss=0.18608]\n","Step 46245   [1.498 sec/step, loss=0.19304, avg_loss=0.18622]\n","Step 46246   [1.500 sec/step, loss=0.18809, avg_loss=0.18643]\n","Step 46247   [1.495 sec/step, loss=0.18654, avg_loss=0.18632]\n","Step 46248   [1.491 sec/step, loss=0.19403, avg_loss=0.18629]\n","Step 46249   [1.489 sec/step, loss=0.16600, avg_loss=0.18603]\n","Step 46250   [1.490 sec/step, loss=0.17852, avg_loss=0.18598]\n","Step 46251   [1.491 sec/step, loss=0.18861, avg_loss=0.18598]\n","Step 46252   [1.495 sec/step, loss=0.19225, avg_loss=0.18615]\n","Step 46253   [1.493 sec/step, loss=0.16223, avg_loss=0.18606]\n","Step 46254   [1.491 sec/step, loss=0.17761, avg_loss=0.18594]\n","Step 46255   [1.491 sec/step, loss=0.19091, avg_loss=0.18586]\n","Step 46256   [1.491 sec/step, loss=0.17606, avg_loss=0.18585]\n","Step 46257   [1.489 sec/step, loss=0.17052, avg_loss=0.18558]\n","Step 46258   [1.486 sec/step, loss=0.15750, avg_loss=0.18527]\n","Step 46259   [1.483 sec/step, loss=0.17634, avg_loss=0.18507]\n","Step 46260   [1.485 sec/step, loss=0.16829, avg_loss=0.18500]\n","Step 46261   [1.484 sec/step, loss=0.19974, avg_loss=0.18501]\n","Step 46262   [1.490 sec/step, loss=0.19526, avg_loss=0.18535]\n","Step 46263   [1.484 sec/step, loss=0.15710, avg_loss=0.18495]\n","Step 46264   [1.490 sec/step, loss=0.18915, avg_loss=0.18522]\n","Step 46265   [1.487 sec/step, loss=0.15421, avg_loss=0.18465]\n","Step 46266   [1.487 sec/step, loss=0.17050, avg_loss=0.18447]\n","Step 46267   [1.489 sec/step, loss=0.19426, avg_loss=0.18442]\n","Step 46268   [1.486 sec/step, loss=0.15728, avg_loss=0.18399]\n","Step 46269   [1.488 sec/step, loss=0.18678, avg_loss=0.18401]\n","Step 46270   [1.493 sec/step, loss=0.18872, avg_loss=0.18377]\n","Step 46271   [1.504 sec/step, loss=0.19391, avg_loss=0.18396]\n","Step 46272   [1.509 sec/step, loss=0.18483, avg_loss=0.18387]\n","Step 46273   [1.520 sec/step, loss=0.19084, avg_loss=0.18413]\n","Step 46274   [1.525 sec/step, loss=0.17972, avg_loss=0.18414]\n","Generated 32 batches of size 32 in 10.736 sec\n","Step 46275   [1.523 sec/step, loss=0.19227, avg_loss=0.18415]\n","Step 46276   [1.514 sec/step, loss=0.15980, avg_loss=0.18374]\n","Step 46277   [1.505 sec/step, loss=0.17251, avg_loss=0.18349]\n","Step 46278   [1.500 sec/step, loss=0.19068, avg_loss=0.18335]\n","Step 46279   [1.495 sec/step, loss=0.15449, avg_loss=0.18317]\n","Step 46280   [1.488 sec/step, loss=0.15317, avg_loss=0.18271]\n","Step 46281   [1.484 sec/step, loss=0.16822, avg_loss=0.18238]\n","Step 46282   [1.478 sec/step, loss=0.15321, avg_loss=0.18189]\n","Step 46283   [1.479 sec/step, loss=0.18822, avg_loss=0.18184]\n","Step 46284   [1.479 sec/step, loss=0.18190, avg_loss=0.18143]\n","Step 46285   [1.478 sec/step, loss=0.18784, avg_loss=0.18139]\n","Step 46286   [1.481 sec/step, loss=0.19904, avg_loss=0.18134]\n","Step 46287   [1.481 sec/step, loss=0.17054, avg_loss=0.18127]\n","Step 46288   [1.479 sec/step, loss=0.17801, avg_loss=0.18116]\n","Step 46289   [1.476 sec/step, loss=0.17678, avg_loss=0.18098]\n","Step 46290   [1.478 sec/step, loss=0.18650, avg_loss=0.18098]\n","Step 46291   [1.479 sec/step, loss=0.16915, avg_loss=0.18089]\n","Step 46292   [1.483 sec/step, loss=0.18447, avg_loss=0.18098]\n","Step 46293   [1.481 sec/step, loss=0.16310, avg_loss=0.18069]\n","Step 46294   [1.481 sec/step, loss=0.18376, avg_loss=0.18055]\n","Step 46295   [1.481 sec/step, loss=0.17826, avg_loss=0.18063]\n","Step 46296   [1.474 sec/step, loss=0.16138, avg_loss=0.18021]\n","Step 46297   [1.475 sec/step, loss=0.19623, avg_loss=0.18002]\n","Step 46298   [1.475 sec/step, loss=0.15516, avg_loss=0.17992]\n","Step 46299   [1.478 sec/step, loss=0.19294, avg_loss=0.17995]\n","Step 46300   [1.478 sec/step, loss=0.19407, avg_loss=0.17999]\n","Writing summary at step: 46300\n","Step 46301   [1.486 sec/step, loss=0.17757, avg_loss=0.18016]\n","Step 46302   [1.491 sec/step, loss=0.19359, avg_loss=0.18010]\n","Step 46303   [1.495 sec/step, loss=0.19254, avg_loss=0.18006]\n","Step 46304   [1.493 sec/step, loss=0.17519, avg_loss=0.17994]\n","Step 46305   [1.502 sec/step, loss=0.18920, avg_loss=0.18005]\n","Generated 32 batches of size 32 in 10.697 sec\n","Step 46306   [1.499 sec/step, loss=0.19197, avg_loss=0.17999]\n","Step 46307   [1.499 sec/step, loss=0.19144, avg_loss=0.18019]\n","Step 46308   [1.498 sec/step, loss=0.16038, avg_loss=0.18013]\n","Step 46309   [1.499 sec/step, loss=0.19033, avg_loss=0.18042]\n","Step 46310   [1.489 sec/step, loss=0.16780, avg_loss=0.18007]\n","Step 46311   [1.481 sec/step, loss=0.16640, avg_loss=0.18011]\n","Step 46312   [1.482 sec/step, loss=0.19259, avg_loss=0.18006]\n","Step 46313   [1.482 sec/step, loss=0.18970, avg_loss=0.17999]\n","Step 46314   [1.478 sec/step, loss=0.16731, avg_loss=0.17973]\n","Step 46315   [1.472 sec/step, loss=0.16505, avg_loss=0.17943]\n","Step 46316   [1.466 sec/step, loss=0.15914, avg_loss=0.17912]\n","Step 46317   [1.470 sec/step, loss=0.19667, avg_loss=0.17927]\n","Step 46318   [1.469 sec/step, loss=0.19164, avg_loss=0.17925]\n","Step 46319   [1.469 sec/step, loss=0.19270, avg_loss=0.17931]\n","Step 46320   [1.468 sec/step, loss=0.16914, avg_loss=0.17922]\n","Step 46321   [1.468 sec/step, loss=0.18729, avg_loss=0.17924]\n","Step 46322   [1.462 sec/step, loss=0.17029, avg_loss=0.17898]\n","Step 46323   [1.464 sec/step, loss=0.18397, avg_loss=0.17913]\n","Step 46324   [1.466 sec/step, loss=0.16048, avg_loss=0.17905]\n","Step 46325   [1.468 sec/step, loss=0.19129, avg_loss=0.17917]\n","Step 46326   [1.463 sec/step, loss=0.15894, avg_loss=0.17880]\n","Step 46327   [1.464 sec/step, loss=0.17949, avg_loss=0.17886]\n","Step 46328   [1.466 sec/step, loss=0.18733, avg_loss=0.17896]\n","Step 46329   [1.463 sec/step, loss=0.17823, avg_loss=0.17884]\n","Step 46330   [1.463 sec/step, loss=0.18523, avg_loss=0.17878]\n","Step 46331   [1.465 sec/step, loss=0.17692, avg_loss=0.17884]\n","Step 46332   [1.471 sec/step, loss=0.19468, avg_loss=0.17911]\n","Step 46333   [1.480 sec/step, loss=0.20072, avg_loss=0.17942]\n","Step 46334   [1.487 sec/step, loss=0.19584, avg_loss=0.17959]\n","Step 46335   [1.498 sec/step, loss=0.19060, avg_loss=0.17991]\n","Step 46336   [1.507 sec/step, loss=0.19606, avg_loss=0.18018]\n","Step 46337   [1.504 sec/step, loss=0.16840, avg_loss=0.17996]\n","Generated 32 batches of size 32 in 10.559 sec\n","Step 46338   [1.508 sec/step, loss=0.19081, avg_loss=0.18029]\n","Step 46339   [1.501 sec/step, loss=0.18321, avg_loss=0.18015]\n","Step 46340   [1.494 sec/step, loss=0.16686, avg_loss=0.17986]\n","Step 46341   [1.493 sec/step, loss=0.20218, avg_loss=0.18021]\n","Step 46342   [1.494 sec/step, loss=0.17765, avg_loss=0.18040]\n","Step 46343   [1.490 sec/step, loss=0.19501, avg_loss=0.18037]\n","Step 46344   [1.485 sec/step, loss=0.15942, avg_loss=0.18002]\n","Step 46345   [1.486 sec/step, loss=0.19013, avg_loss=0.17999]\n","Step 46346   [1.484 sec/step, loss=0.18257, avg_loss=0.17993]\n","Step 46347   [1.485 sec/step, loss=0.18769, avg_loss=0.17995]\n","Step 46348   [1.485 sec/step, loss=0.19036, avg_loss=0.17991]\n","Step 46349   [1.485 sec/step, loss=0.16521, avg_loss=0.17990]\n","Step 46350   [1.481 sec/step, loss=0.16261, avg_loss=0.17974]\n","Step 46351   [1.481 sec/step, loss=0.18757, avg_loss=0.17973]\n","Step 46352   [1.481 sec/step, loss=0.18558, avg_loss=0.17966]\n","Step 46353   [1.480 sec/step, loss=0.15762, avg_loss=0.17962]\n","Step 46354   [1.481 sec/step, loss=0.17744, avg_loss=0.17962]\n","Step 46355   [1.478 sec/step, loss=0.17531, avg_loss=0.17946]\n","Step 46356   [1.476 sec/step, loss=0.17203, avg_loss=0.17942]\n","Step 46357   [1.474 sec/step, loss=0.15879, avg_loss=0.17930]\n","Step 46358   [1.474 sec/step, loss=0.15584, avg_loss=0.17929]\n","Step 46359   [1.471 sec/step, loss=0.15402, avg_loss=0.17906]\n","Step 46360   [1.475 sec/step, loss=0.19889, avg_loss=0.17937]\n","Step 46361   [1.475 sec/step, loss=0.19349, avg_loss=0.17931]\n","Step 46362   [1.475 sec/step, loss=0.20426, avg_loss=0.17940]\n","Step 46363   [1.480 sec/step, loss=0.19937, avg_loss=0.17982]\n","Step 46364   [1.481 sec/step, loss=0.19697, avg_loss=0.17990]\n","Step 46365   [1.485 sec/step, loss=0.17865, avg_loss=0.18014]\n","Step 46366   [1.488 sec/step, loss=0.17391, avg_loss=0.18018]\n","Step 46367   [1.487 sec/step, loss=0.17017, avg_loss=0.17994]\n","Step 46368   [1.496 sec/step, loss=0.20041, avg_loss=0.18037]\n","Step 46369   [1.500 sec/step, loss=0.20615, avg_loss=0.18056]\n","Generated 32 batches of size 32 in 10.303 sec\n","Step 46370   [1.499 sec/step, loss=0.19769, avg_loss=0.18065]\n","Step 46371   [1.493 sec/step, loss=0.19340, avg_loss=0.18064]\n","Step 46372   [1.488 sec/step, loss=0.19487, avg_loss=0.18075]\n","Step 46373   [1.481 sec/step, loss=0.19201, avg_loss=0.18076]\n","Step 46374   [1.479 sec/step, loss=0.18781, avg_loss=0.18084]\n","Step 46375   [1.478 sec/step, loss=0.19591, avg_loss=0.18087]\n","Step 46376   [1.484 sec/step, loss=0.19617, avg_loss=0.18124]\n","Step 46377   [1.484 sec/step, loss=0.19989, avg_loss=0.18151]\n","Step 46378   [1.481 sec/step, loss=0.17649, avg_loss=0.18137]\n","Step 46379   [1.487 sec/step, loss=0.19794, avg_loss=0.18180]\n","Step 46380   [1.492 sec/step, loss=0.19757, avg_loss=0.18225]\n","Step 46381   [1.496 sec/step, loss=0.19753, avg_loss=0.18254]\n","Step 46382   [1.497 sec/step, loss=0.17660, avg_loss=0.18278]\n","Step 46383   [1.496 sec/step, loss=0.19389, avg_loss=0.18283]\n","Step 46384   [1.499 sec/step, loss=0.19457, avg_loss=0.18296]\n","Step 46385   [1.500 sec/step, loss=0.18857, avg_loss=0.18297]\n","Step 46386   [1.500 sec/step, loss=0.19583, avg_loss=0.18293]\n","Step 46387   [1.505 sec/step, loss=0.19208, avg_loss=0.18315]\n","Step 46388   [1.505 sec/step, loss=0.18925, avg_loss=0.18326]\n","Step 46389   [1.507 sec/step, loss=0.16221, avg_loss=0.18312]\n","Step 46390   [1.505 sec/step, loss=0.17366, avg_loss=0.18299]\n","Step 46391   [1.503 sec/step, loss=0.17137, avg_loss=0.18301]\n","Step 46392   [1.500 sec/step, loss=0.17525, avg_loss=0.18292]\n","Step 46393   [1.498 sec/step, loss=0.17177, avg_loss=0.18300]\n","Step 46394   [1.499 sec/step, loss=0.20444, avg_loss=0.18321]\n","Step 46395   [1.501 sec/step, loss=0.20132, avg_loss=0.18344]\n","Step 46396   [1.504 sec/step, loss=0.20488, avg_loss=0.18388]\n","Step 46397   [1.505 sec/step, loss=0.19702, avg_loss=0.18388]\n","Step 46398   [1.507 sec/step, loss=0.16933, avg_loss=0.18403]\n","Step 46399   [1.510 sec/step, loss=0.19515, avg_loss=0.18405]\n","Step 46400   [1.507 sec/step, loss=0.16398, avg_loss=0.18375]\n","Writing summary at step: 46400\n","Step 46401   [1.503 sec/step, loss=0.16138, avg_loss=0.18359]\n","Generated 32 batches of size 32 in 10.532 sec\n","Step 46402   [1.496 sec/step, loss=0.15960, avg_loss=0.18325]\n","Step 46403   [1.492 sec/step, loss=0.19099, avg_loss=0.18323]\n","Step 46404   [1.490 sec/step, loss=0.20532, avg_loss=0.18353]\n","Step 46405   [1.481 sec/step, loss=0.17998, avg_loss=0.18344]\n","Step 46406   [1.479 sec/step, loss=0.20189, avg_loss=0.18354]\n","Step 46407   [1.479 sec/step, loss=0.19723, avg_loss=0.18360]\n","Step 46408   [1.481 sec/step, loss=0.18693, avg_loss=0.18386]\n","Step 46409   [1.477 sec/step, loss=0.18517, avg_loss=0.18381]\n","Step 46410   [1.478 sec/step, loss=0.18232, avg_loss=0.18396]\n","Step 46411   [1.482 sec/step, loss=0.20216, avg_loss=0.18431]\n","Step 46412   [1.478 sec/step, loss=0.17389, avg_loss=0.18413]\n","Step 46413   [1.478 sec/step, loss=0.19867, avg_loss=0.18422]\n","Step 46414   [1.483 sec/step, loss=0.19611, avg_loss=0.18450]\n","Step 46415   [1.483 sec/step, loss=0.18518, avg_loss=0.18471]\n","Step 46416   [1.488 sec/step, loss=0.19603, avg_loss=0.18507]\n","Step 46417   [1.485 sec/step, loss=0.17871, avg_loss=0.18489]\n","Step 46418   [1.485 sec/step, loss=0.20365, avg_loss=0.18501]\n","Step 46419   [1.485 sec/step, loss=0.19875, avg_loss=0.18507]\n","Step 46420   [1.489 sec/step, loss=0.20972, avg_loss=0.18548]\n","Step 46421   [1.487 sec/step, loss=0.18801, avg_loss=0.18549]\n","Step 46422   [1.493 sec/step, loss=0.19925, avg_loss=0.18578]\n","Step 46423   [1.495 sec/step, loss=0.19942, avg_loss=0.18593]\n","Step 46424   [1.498 sec/step, loss=0.19412, avg_loss=0.18627]\n","Step 46425   [1.492 sec/step, loss=0.16919, avg_loss=0.18605]\n","Step 46426   [1.495 sec/step, loss=0.18617, avg_loss=0.18632]\n","Step 46427   [1.498 sec/step, loss=0.20320, avg_loss=0.18656]\n","Step 46428   [1.502 sec/step, loss=0.19528, avg_loss=0.18664]\n","Step 46429   [1.502 sec/step, loss=0.16341, avg_loss=0.18649]\n","Step 46430   [1.502 sec/step, loss=0.18258, avg_loss=0.18646]\n","Step 46431   [1.509 sec/step, loss=0.18900, avg_loss=0.18658]\n","Step 46432   [1.513 sec/step, loss=0.18840, avg_loss=0.18652]\n","Step 46433   [1.505 sec/step, loss=0.16355, avg_loss=0.18615]\n","Generated 32 batches of size 32 in 10.526 sec\n","Step 46434   [1.497 sec/step, loss=0.17251, avg_loss=0.18591]\n","Step 46435   [1.486 sec/step, loss=0.15787, avg_loss=0.18559]\n","Step 46436   [1.480 sec/step, loss=0.18888, avg_loss=0.18552]\n","Step 46437   [1.477 sec/step, loss=0.15632, avg_loss=0.18539]\n","Step 46438   [1.475 sec/step, loss=0.19293, avg_loss=0.18542]\n","Step 46439   [1.475 sec/step, loss=0.17952, avg_loss=0.18538]\n","Step 46440   [1.473 sec/step, loss=0.17487, avg_loss=0.18546]\n","Step 46441   [1.468 sec/step, loss=0.17141, avg_loss=0.18515]\n","Step 46442   [1.471 sec/step, loss=0.20101, avg_loss=0.18539]\n","Step 46443   [1.467 sec/step, loss=0.17965, avg_loss=0.18523]\n","Step 46444   [1.471 sec/step, loss=0.18926, avg_loss=0.18553]\n","Step 46445   [1.467 sec/step, loss=0.16533, avg_loss=0.18528]\n","Step 46446   [1.469 sec/step, loss=0.21473, avg_loss=0.18560]\n","Step 46447   [1.469 sec/step, loss=0.19400, avg_loss=0.18567]\n","Step 46448   [1.469 sec/step, loss=0.18822, avg_loss=0.18565]\n","Step 46449   [1.468 sec/step, loss=0.17487, avg_loss=0.18574]\n","Step 46450   [1.471 sec/step, loss=0.18089, avg_loss=0.18592]\n","Step 46451   [1.471 sec/step, loss=0.18562, avg_loss=0.18591]\n","Step 46452   [1.468 sec/step, loss=0.16836, avg_loss=0.18573]\n","Step 46453   [1.470 sec/step, loss=0.17618, avg_loss=0.18592]\n","Step 46454   [1.473 sec/step, loss=0.19530, avg_loss=0.18610]\n","Step 46455   [1.470 sec/step, loss=0.16502, avg_loss=0.18599]\n","Step 46456   [1.468 sec/step, loss=0.15927, avg_loss=0.18587]\n","Step 46457   [1.474 sec/step, loss=0.19836, avg_loss=0.18626]\n","Step 46458   [1.479 sec/step, loss=0.19609, avg_loss=0.18666]\n","Step 46459   [1.479 sec/step, loss=0.16247, avg_loss=0.18675]\n","Step 46460   [1.483 sec/step, loss=0.19211, avg_loss=0.18668]\n","Step 46461   [1.480 sec/step, loss=0.16011, avg_loss=0.18635]\n","Step 46462   [1.482 sec/step, loss=0.18389, avg_loss=0.18614]\n","Step 46463   [1.483 sec/step, loss=0.17190, avg_loss=0.18587]\n","Step 46464   [1.487 sec/step, loss=0.20124, avg_loss=0.18591]\n","Generated 32 batches of size 32 in 10.522 sec\n","Step 46465   [1.493 sec/step, loss=0.19740, avg_loss=0.18610]\n","Step 46466   [1.495 sec/step, loss=0.19616, avg_loss=0.18632]\n","Step 46467   [1.496 sec/step, loss=0.19346, avg_loss=0.18656]\n","Step 46468   [1.493 sec/step, loss=0.19274, avg_loss=0.18648]\n","Step 46469   [1.485 sec/step, loss=0.17455, avg_loss=0.18616]\n","Step 46470   [1.475 sec/step, loss=0.16775, avg_loss=0.18586]\n","Step 46471   [1.475 sec/step, loss=0.18794, avg_loss=0.18581]\n","Step 46472   [1.475 sec/step, loss=0.19865, avg_loss=0.18585]\n","Step 46473   [1.472 sec/step, loss=0.15672, avg_loss=0.18549]\n","Step 46474   [1.466 sec/step, loss=0.15473, avg_loss=0.18516]\n","Step 46475   [1.467 sec/step, loss=0.20693, avg_loss=0.18527]\n","Step 46476   [1.467 sec/step, loss=0.20027, avg_loss=0.18531]\n","Step 46477   [1.465 sec/step, loss=0.15999, avg_loss=0.18491]\n","Step 46478   [1.468 sec/step, loss=0.19137, avg_loss=0.18506]\n","Step 46479   [1.464 sec/step, loss=0.16344, avg_loss=0.18472]\n","Step 46480   [1.464 sec/step, loss=0.19996, avg_loss=0.18474]\n","Step 46481   [1.461 sec/step, loss=0.18875, avg_loss=0.18465]\n","Step 46482   [1.466 sec/step, loss=0.19188, avg_loss=0.18481]\n","Step 46483   [1.466 sec/step, loss=0.19384, avg_loss=0.18481]\n","Step 46484   [1.461 sec/step, loss=0.17353, avg_loss=0.18460]\n","Step 46485   [1.461 sec/step, loss=0.18860, avg_loss=0.18460]\n","Step 46486   [1.458 sec/step, loss=0.18098, avg_loss=0.18445]\n","Step 46487   [1.453 sec/step, loss=0.17183, avg_loss=0.18425]\n","Step 46488   [1.456 sec/step, loss=0.20052, avg_loss=0.18436]\n","Step 46489   [1.457 sec/step, loss=0.19761, avg_loss=0.18471]\n","Step 46490   [1.460 sec/step, loss=0.19411, avg_loss=0.18492]\n","Step 46491   [1.463 sec/step, loss=0.16819, avg_loss=0.18489]\n","Step 46492   [1.464 sec/step, loss=0.16961, avg_loss=0.18483]\n","Step 46493   [1.473 sec/step, loss=0.18905, avg_loss=0.18500]\n","Step 46494   [1.474 sec/step, loss=0.18092, avg_loss=0.18477]\n","Step 46495   [1.474 sec/step, loss=0.17215, avg_loss=0.18447]\n","Step 46496   [1.481 sec/step, loss=0.19702, avg_loss=0.18440]\n","Generated 32 batches of size 32 in 10.523 sec\n","Step 46497   [1.482 sec/step, loss=0.17816, avg_loss=0.18421]\n","Step 46498   [1.486 sec/step, loss=0.19200, avg_loss=0.18443]\n","Step 46499   [1.482 sec/step, loss=0.18768, avg_loss=0.18436]\n","Step 46500   [1.482 sec/step, loss=0.17853, avg_loss=0.18451]\n","Writing summary at step: 46500\n","Saving audio and alignment...\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51092 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44928 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48512 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47532 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51088 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45459 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45931 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49828 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44152 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44852 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51092 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44928 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48512 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47532 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51088 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45459 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45931 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49828 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44152 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44852 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/train-step-000046500-align000.png\n","100% 1/1 [00:03<00:00,  3.85s/it]\n","Test finished for step 46500.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000046500-align000.png\n"," 50% 1/2 [00:11<00:11, 11.35s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000046500-align001.png\n","100% 2/2 [00:22<00:00, 11.41s/it]\n","Test finished for step 46500.\n","Step 46501   [1.485 sec/step, loss=0.18798, avg_loss=0.18477]\n","Step 46502   [1.485 sec/step, loss=0.17620, avg_loss=0.18494]\n","Step 46503   [1.481 sec/step, loss=0.17187, avg_loss=0.18475]\n","Step 46504   [1.484 sec/step, loss=0.20056, avg_loss=0.18470]\n","Step 46505   [1.487 sec/step, loss=0.18849, avg_loss=0.18478]\n","Step 46506   [1.483 sec/step, loss=0.16820, avg_loss=0.18445]\n","Step 46507   [1.481 sec/step, loss=0.16388, avg_loss=0.18411]\n","Step 46508   [1.479 sec/step, loss=0.16593, avg_loss=0.18390]\n","Step 46509   [1.481 sec/step, loss=0.17694, avg_loss=0.18382]\n","Step 46510   [1.478 sec/step, loss=0.16679, avg_loss=0.18367]\n","Step 46511   [1.472 sec/step, loss=0.15984, avg_loss=0.18324]\n","Step 46512   [1.476 sec/step, loss=0.19556, avg_loss=0.18346]\n","Step 46513   [1.471 sec/step, loss=0.16376, avg_loss=0.18311]\n","Step 46514   [1.468 sec/step, loss=0.17807, avg_loss=0.18293]\n","Step 46515   [1.468 sec/step, loss=0.16582, avg_loss=0.18274]\n","Step 46516   [1.468 sec/step, loss=0.19766, avg_loss=0.18275]\n","Step 46517   [1.466 sec/step, loss=0.16113, avg_loss=0.18258]\n","Step 46518   [1.467 sec/step, loss=0.19945, avg_loss=0.18253]\n","Step 46519   [1.467 sec/step, loss=0.19333, avg_loss=0.18248]\n","Step 46520   [1.467 sec/step, loss=0.19405, avg_loss=0.18232]\n","Step 46521   [1.469 sec/step, loss=0.18827, avg_loss=0.18233]\n","Step 46522   [1.474 sec/step, loss=0.19811, avg_loss=0.18231]\n","Step 46523   [1.479 sec/step, loss=0.18757, avg_loss=0.18220]\n","Step 46524   [1.485 sec/step, loss=0.19628, avg_loss=0.18222]\n","Step 46525   [1.493 sec/step, loss=0.18121, avg_loss=0.18234]\n","Step 46526   [1.497 sec/step, loss=0.17810, avg_loss=0.18226]\n","Generated 32 batches of size 32 in 10.768 sec\n","Step 46527   [1.495 sec/step, loss=0.17523, avg_loss=0.18198]\n","Step 46528   [1.485 sec/step, loss=0.17380, avg_loss=0.18176]\n","Step 46529   [1.489 sec/step, loss=0.19848, avg_loss=0.18211]\n","Step 46530   [1.490 sec/step, loss=0.19878, avg_loss=0.18228]\n","Step 46531   [1.484 sec/step, loss=0.18767, avg_loss=0.18226]\n","Step 46532   [1.480 sec/step, loss=0.19477, avg_loss=0.18233]\n","Step 46533   [1.481 sec/step, loss=0.18051, avg_loss=0.18250]\n","Step 46534   [1.481 sec/step, loss=0.17659, avg_loss=0.18254]\n","Step 46535   [1.487 sec/step, loss=0.19679, avg_loss=0.18293]\n","Step 46536   [1.489 sec/step, loss=0.19682, avg_loss=0.18300]\n","Step 46537   [1.495 sec/step, loss=0.18850, avg_loss=0.18333]\n","Step 46538   [1.494 sec/step, loss=0.18593, avg_loss=0.18326]\n","Step 46539   [1.496 sec/step, loss=0.18492, avg_loss=0.18331]\n","Step 46540   [1.497 sec/step, loss=0.18050, avg_loss=0.18337]\n","Step 46541   [1.501 sec/step, loss=0.18620, avg_loss=0.18351]\n","Step 46542   [1.498 sec/step, loss=0.20408, avg_loss=0.18355]\n","Step 46543   [1.497 sec/step, loss=0.16261, avg_loss=0.18337]\n","Step 46544   [1.492 sec/step, loss=0.16628, avg_loss=0.18315]\n","Step 46545   [1.496 sec/step, loss=0.20316, avg_loss=0.18352]\n","Step 46546   [1.496 sec/step, loss=0.19515, avg_loss=0.18333]\n","Step 46547   [1.496 sec/step, loss=0.19326, avg_loss=0.18332]\n","Step 46548   [1.496 sec/step, loss=0.19543, avg_loss=0.18339]\n","Step 46549   [1.500 sec/step, loss=0.19166, avg_loss=0.18356]\n","Step 46550   [1.496 sec/step, loss=0.16599, avg_loss=0.18341]\n","Step 46551   [1.497 sec/step, loss=0.19511, avg_loss=0.18351]\n","Step 46552   [1.497 sec/step, loss=0.18273, avg_loss=0.18365]\n","Step 46553   [1.498 sec/step, loss=0.18341, avg_loss=0.18372]\n","Step 46554   [1.502 sec/step, loss=0.19472, avg_loss=0.18372]\n","Step 46555   [1.512 sec/step, loss=0.19886, avg_loss=0.18405]\n","Step 46556   [1.519 sec/step, loss=0.16630, avg_loss=0.18412]\n","Step 46557   [1.516 sec/step, loss=0.16233, avg_loss=0.18376]\n","Step 46558   [1.515 sec/step, loss=0.17173, avg_loss=0.18352]\n","Step 46559   [1.520 sec/step, loss=0.16978, avg_loss=0.18359]\n","Generated 32 batches of size 32 in 10.270 sec\n","Step 46560   [1.511 sec/step, loss=0.15709, avg_loss=0.18324]\n","Step 46561   [1.515 sec/step, loss=0.19204, avg_loss=0.18356]\n","Step 46562   [1.507 sec/step, loss=0.15567, avg_loss=0.18328]\n","Step 46563   [1.506 sec/step, loss=0.19823, avg_loss=0.18354]\n","Step 46564   [1.497 sec/step, loss=0.17594, avg_loss=0.18329]\n","Step 46565   [1.493 sec/step, loss=0.19267, avg_loss=0.18324]\n","Step 46566   [1.486 sec/step, loss=0.15794, avg_loss=0.18286]\n","Step 46567   [1.480 sec/step, loss=0.15511, avg_loss=0.18248]\n","Step 46568   [1.480 sec/step, loss=0.19349, avg_loss=0.18249]\n","Step 46569   [1.481 sec/step, loss=0.18332, avg_loss=0.18257]\n","Step 46570   [1.487 sec/step, loss=0.19652, avg_loss=0.18286]\n","Step 46571   [1.487 sec/step, loss=0.19263, avg_loss=0.18291]\n","Step 46572   [1.487 sec/step, loss=0.19233, avg_loss=0.18284]\n","Step 46573   [1.493 sec/step, loss=0.18893, avg_loss=0.18317]\n","Step 46574   [1.499 sec/step, loss=0.19033, avg_loss=0.18352]\n","Step 46575   [1.494 sec/step, loss=0.16273, avg_loss=0.18308]\n","Step 46576   [1.491 sec/step, loss=0.18004, avg_loss=0.18288]\n","Step 46577   [1.491 sec/step, loss=0.16432, avg_loss=0.18292]\n","Step 46578   [1.487 sec/step, loss=0.17061, avg_loss=0.18271]\n","Step 46579   [1.492 sec/step, loss=0.19376, avg_loss=0.18302]\n","Step 46580   [1.487 sec/step, loss=0.16766, avg_loss=0.18269]\n","Step 46581   [1.488 sec/step, loss=0.17779, avg_loss=0.18258]\n","Step 46582   [1.487 sec/step, loss=0.19004, avg_loss=0.18257]\n","Step 46583   [1.488 sec/step, loss=0.18927, avg_loss=0.18252]\n","Step 46584   [1.486 sec/step, loss=0.15994, avg_loss=0.18238]\n","Step 46585   [1.486 sec/step, loss=0.18648, avg_loss=0.18236]\n","Step 46586   [1.494 sec/step, loss=0.20096, avg_loss=0.18256]\n","Step 46587   [1.499 sec/step, loss=0.17977, avg_loss=0.18264]\n","Step 46588   [1.503 sec/step, loss=0.19330, avg_loss=0.18257]\n","Step 46589   [1.504 sec/step, loss=0.16852, avg_loss=0.18228]\n","Step 46590   [1.505 sec/step, loss=0.17613, avg_loss=0.18210]\n","Generated 32 batches of size 32 in 10.457 sec\n","Step 46591   [1.507 sec/step, loss=0.17072, avg_loss=0.18213]\n","Step 46592   [1.504 sec/step, loss=0.16243, avg_loss=0.18205]\n","Step 46593   [1.500 sec/step, loss=0.18934, avg_loss=0.18206]\n","Step 46594   [1.498 sec/step, loss=0.19075, avg_loss=0.18215]\n","Step 46595   [1.494 sec/step, loss=0.17272, avg_loss=0.18216]\n","Step 46596   [1.483 sec/step, loss=0.15504, avg_loss=0.18174]\n","Step 46597   [1.478 sec/step, loss=0.18152, avg_loss=0.18177]\n","Step 46598   [1.472 sec/step, loss=0.15389, avg_loss=0.18139]\n","Step 46599   [1.472 sec/step, loss=0.19152, avg_loss=0.18143]\n","Step 46600   [1.474 sec/step, loss=0.18944, avg_loss=0.18154]\n","Writing summary at step: 46600\n","Step 46601   [1.470 sec/step, loss=0.15838, avg_loss=0.18124]\n","Step 46602   [1.468 sec/step, loss=0.16933, avg_loss=0.18118]\n","Step 46603   [1.466 sec/step, loss=0.15838, avg_loss=0.18104]\n","Step 46604   [1.463 sec/step, loss=0.16241, avg_loss=0.18066]\n","Step 46605   [1.461 sec/step, loss=0.17665, avg_loss=0.18054]\n","Step 46606   [1.465 sec/step, loss=0.18810, avg_loss=0.18074]\n","Step 46607   [1.465 sec/step, loss=0.17425, avg_loss=0.18084]\n","Step 46608   [1.469 sec/step, loss=0.20532, avg_loss=0.18124]\n","Step 46609   [1.472 sec/step, loss=0.19990, avg_loss=0.18147]\n","Step 46610   [1.478 sec/step, loss=0.19339, avg_loss=0.18173]\n","Step 46611   [1.479 sec/step, loss=0.17711, avg_loss=0.18191]\n","Step 46612   [1.477 sec/step, loss=0.18058, avg_loss=0.18176]\n","Step 46613   [1.475 sec/step, loss=0.16160, avg_loss=0.18173]\n","Step 46614   [1.478 sec/step, loss=0.19357, avg_loss=0.18189]\n","Step 46615   [1.484 sec/step, loss=0.18983, avg_loss=0.18213]\n","Step 46616   [1.485 sec/step, loss=0.19851, avg_loss=0.18214]\n","Step 46617   [1.490 sec/step, loss=0.17113, avg_loss=0.18224]\n","Step 46618   [1.493 sec/step, loss=0.18828, avg_loss=0.18213]\n","Step 46619   [1.498 sec/step, loss=0.21192, avg_loss=0.18231]\n","Step 46620   [1.495 sec/step, loss=0.16003, avg_loss=0.18197]\n","Step 46621   [1.499 sec/step, loss=0.19950, avg_loss=0.18208]\n","Generated 32 batches of size 32 in 10.321 sec\n","Step 46622   [1.498 sec/step, loss=0.19830, avg_loss=0.18209]\n","Step 46623   [1.491 sec/step, loss=0.18015, avg_loss=0.18201]\n","Step 46624   [1.485 sec/step, loss=0.19008, avg_loss=0.18195]\n","Step 46625   [1.484 sec/step, loss=0.19536, avg_loss=0.18209]\n","Step 46626   [1.482 sec/step, loss=0.18673, avg_loss=0.18218]\n","Step 46627   [1.478 sec/step, loss=0.16106, avg_loss=0.18204]\n","Step 46628   [1.483 sec/step, loss=0.18539, avg_loss=0.18215]\n","Step 46629   [1.480 sec/step, loss=0.17799, avg_loss=0.18195]\n","Step 46630   [1.479 sec/step, loss=0.18624, avg_loss=0.18182]\n","Step 46631   [1.479 sec/step, loss=0.17435, avg_loss=0.18169]\n","Step 46632   [1.472 sec/step, loss=0.15653, avg_loss=0.18131]\n","Step 46633   [1.469 sec/step, loss=0.15346, avg_loss=0.18104]\n","Step 46634   [1.471 sec/step, loss=0.19053, avg_loss=0.18118]\n","Step 46635   [1.472 sec/step, loss=0.20326, avg_loss=0.18124]\n","Step 46636   [1.472 sec/step, loss=0.19901, avg_loss=0.18126]\n","Step 46637   [1.472 sec/step, loss=0.19375, avg_loss=0.18131]\n","Step 46638   [1.470 sec/step, loss=0.17432, avg_loss=0.18120]\n","Step 46639   [1.467 sec/step, loss=0.18349, avg_loss=0.18118]\n","Step 46640   [1.472 sec/step, loss=0.19442, avg_loss=0.18132]\n","Step 46641   [1.472 sec/step, loss=0.20247, avg_loss=0.18149]\n","Step 46642   [1.474 sec/step, loss=0.19170, avg_loss=0.18136]\n","Step 46643   [1.478 sec/step, loss=0.19021, avg_loss=0.18164]\n","Step 46644   [1.481 sec/step, loss=0.17941, avg_loss=0.18177]\n","Step 46645   [1.476 sec/step, loss=0.17316, avg_loss=0.18147]\n","Step 46646   [1.473 sec/step, loss=0.17083, avg_loss=0.18123]\n","Step 46647   [1.467 sec/step, loss=0.17723, avg_loss=0.18107]\n","Step 46648   [1.463 sec/step, loss=0.17036, avg_loss=0.18082]\n","Step 46649   [1.467 sec/step, loss=0.19127, avg_loss=0.18081]\n","Step 46650   [1.478 sec/step, loss=0.18783, avg_loss=0.18103]\n","Step 46651   [1.482 sec/step, loss=0.19704, avg_loss=0.18105]\n","Step 46652   [1.491 sec/step, loss=0.19378, avg_loss=0.18116]\n","Step 46653   [1.495 sec/step, loss=0.18284, avg_loss=0.18115]\n","Generated 32 batches of size 32 in 10.783 sec\n","Step 46654   [1.488 sec/step, loss=0.18394, avg_loss=0.18105]\n","Step 46655   [1.485 sec/step, loss=0.20423, avg_loss=0.18110]\n","Step 46656   [1.478 sec/step, loss=0.17075, avg_loss=0.18114]\n","Step 46657   [1.477 sec/step, loss=0.16257, avg_loss=0.18115]\n","Step 46658   [1.475 sec/step, loss=0.18655, avg_loss=0.18129]\n","Step 46659   [1.473 sec/step, loss=0.17547, avg_loss=0.18135]\n","Step 46660   [1.478 sec/step, loss=0.19862, avg_loss=0.18177]\n","Step 46661   [1.475 sec/step, loss=0.17754, avg_loss=0.18162]\n","Step 46662   [1.477 sec/step, loss=0.16093, avg_loss=0.18167]\n","Step 46663   [1.476 sec/step, loss=0.20481, avg_loss=0.18174]\n","Step 46664   [1.480 sec/step, loss=0.20536, avg_loss=0.18203]\n","Step 46665   [1.474 sec/step, loss=0.16144, avg_loss=0.18172]\n","Step 46666   [1.473 sec/step, loss=0.15827, avg_loss=0.18173]\n","Step 46667   [1.479 sec/step, loss=0.19700, avg_loss=0.18214]\n","Step 46668   [1.480 sec/step, loss=0.19988, avg_loss=0.18221]\n","Step 46669   [1.478 sec/step, loss=0.17593, avg_loss=0.18213]\n","Step 46670   [1.477 sec/step, loss=0.19506, avg_loss=0.18212]\n","Step 46671   [1.472 sec/step, loss=0.17092, avg_loss=0.18190]\n","Step 46672   [1.472 sec/step, loss=0.19229, avg_loss=0.18190]\n","Step 46673   [1.472 sec/step, loss=0.18901, avg_loss=0.18190]\n","Step 46674   [1.471 sec/step, loss=0.18760, avg_loss=0.18188]\n","Step 46675   [1.476 sec/step, loss=0.19875, avg_loss=0.18224]\n","Step 46676   [1.478 sec/step, loss=0.19829, avg_loss=0.18242]\n","Step 46677   [1.484 sec/step, loss=0.19176, avg_loss=0.18269]\n","Step 46678   [1.486 sec/step, loss=0.18874, avg_loss=0.18287]\n","Step 46679   [1.483 sec/step, loss=0.18183, avg_loss=0.18276]\n","Step 46680   [1.483 sec/step, loss=0.17490, avg_loss=0.18283]\n","Step 46681   [1.490 sec/step, loss=0.19533, avg_loss=0.18300]\n","Step 46682   [1.496 sec/step, loss=0.20406, avg_loss=0.18314]\n","Step 46683   [1.494 sec/step, loss=0.16365, avg_loss=0.18289]\n","Step 46684   [1.504 sec/step, loss=0.19462, avg_loss=0.18323]\n","Step 46685   [1.508 sec/step, loss=0.19374, avg_loss=0.18331]\n","Generated 32 batches of size 32 in 10.411 sec\n","Step 46686   [1.499 sec/step, loss=0.16055, avg_loss=0.18290]\n","Step 46687   [1.496 sec/step, loss=0.19407, avg_loss=0.18305]\n","Step 46688   [1.488 sec/step, loss=0.17694, avg_loss=0.18288]\n","Step 46689   [1.484 sec/step, loss=0.16818, avg_loss=0.18288]\n","Step 46690   [1.476 sec/step, loss=0.16294, avg_loss=0.18275]\n","Step 46691   [1.473 sec/step, loss=0.16986, avg_loss=0.18274]\n","Step 46692   [1.480 sec/step, loss=0.20315, avg_loss=0.18314]\n","Step 46693   [1.480 sec/step, loss=0.19236, avg_loss=0.18318]\n","Step 46694   [1.480 sec/step, loss=0.19522, avg_loss=0.18322]\n","Step 46695   [1.484 sec/step, loss=0.19515, avg_loss=0.18344]\n","Step 46696   [1.491 sec/step, loss=0.19254, avg_loss=0.18382]\n","Step 46697   [1.494 sec/step, loss=0.19289, avg_loss=0.18393]\n","Step 46698   [1.496 sec/step, loss=0.17052, avg_loss=0.18410]\n","Step 46699   [1.496 sec/step, loss=0.18688, avg_loss=0.18405]\n","Step 46700   [1.496 sec/step, loss=0.19158, avg_loss=0.18407]\n","Writing summary at step: 46700\n","Step 46701   [1.497 sec/step, loss=0.18412, avg_loss=0.18433]\n","Step 46702   [1.498 sec/step, loss=0.17175, avg_loss=0.18436]\n","Step 46703   [1.503 sec/step, loss=0.19139, avg_loss=0.18469]\n","Step 46704   [1.502 sec/step, loss=0.16746, avg_loss=0.18474]\n","Step 46705   [1.504 sec/step, loss=0.20219, avg_loss=0.18499]\n","Step 46706   [1.499 sec/step, loss=0.15989, avg_loss=0.18471]\n","Step 46707   [1.499 sec/step, loss=0.17882, avg_loss=0.18475]\n","Step 46708   [1.496 sec/step, loss=0.16601, avg_loss=0.18436]\n","Step 46709   [1.496 sec/step, loss=0.19023, avg_loss=0.18427]\n","Step 46710   [1.490 sec/step, loss=0.15622, avg_loss=0.18389]\n","Step 46711   [1.488 sec/step, loss=0.15451, avg_loss=0.18367]\n","Step 46712   [1.494 sec/step, loss=0.18578, avg_loss=0.18372]\n","Step 46713   [1.504 sec/step, loss=0.19391, avg_loss=0.18404]\n","Step 46714   [1.508 sec/step, loss=0.18462, avg_loss=0.18395]\n","Step 46715   [1.512 sec/step, loss=0.19367, avg_loss=0.18399]\n","Step 46716   [1.513 sec/step, loss=0.17971, avg_loss=0.18380]\n","Generated 32 batches of size 32 in 10.496 sec\n","Step 46717   [1.513 sec/step, loss=0.17726, avg_loss=0.18386]\n","Step 46718   [1.503 sec/step, loss=0.16536, avg_loss=0.18364]\n","Step 46719   [1.497 sec/step, loss=0.19107, avg_loss=0.18343]\n","Step 46720   [1.497 sec/step, loss=0.17189, avg_loss=0.18355]\n","Step 46721   [1.493 sec/step, loss=0.18719, avg_loss=0.18342]\n","Step 46722   [1.485 sec/step, loss=0.16819, avg_loss=0.18312]\n","Step 46723   [1.487 sec/step, loss=0.18796, avg_loss=0.18320]\n","Step 46724   [1.482 sec/step, loss=0.15725, avg_loss=0.18287]\n","Step 46725   [1.475 sec/step, loss=0.15408, avg_loss=0.18246]\n","Step 46726   [1.472 sec/step, loss=0.17108, avg_loss=0.18230]\n","Step 46727   [1.477 sec/step, loss=0.19228, avg_loss=0.18261]\n","Step 46728   [1.477 sec/step, loss=0.18665, avg_loss=0.18263]\n","Step 46729   [1.478 sec/step, loss=0.19437, avg_loss=0.18279]\n","Step 46730   [1.478 sec/step, loss=0.19056, avg_loss=0.18283]\n","Step 46731   [1.481 sec/step, loss=0.19776, avg_loss=0.18307]\n","Step 46732   [1.482 sec/step, loss=0.16189, avg_loss=0.18312]\n","Step 46733   [1.485 sec/step, loss=0.16850, avg_loss=0.18327]\n","Step 46734   [1.485 sec/step, loss=0.19315, avg_loss=0.18330]\n","Step 46735   [1.479 sec/step, loss=0.16439, avg_loss=0.18291]\n","Step 46736   [1.480 sec/step, loss=0.19374, avg_loss=0.18286]\n","Step 46737   [1.480 sec/step, loss=0.19017, avg_loss=0.18282]\n","Step 46738   [1.482 sec/step, loss=0.19015, avg_loss=0.18298]\n","Step 46739   [1.483 sec/step, loss=0.18169, avg_loss=0.18296]\n","Step 46740   [1.476 sec/step, loss=0.16058, avg_loss=0.18262]\n","Step 46741   [1.473 sec/step, loss=0.16992, avg_loss=0.18230]\n","Step 46742   [1.474 sec/step, loss=0.19744, avg_loss=0.18235]\n","Step 46743   [1.468 sec/step, loss=0.15485, avg_loss=0.18200]\n","Step 46744   [1.472 sec/step, loss=0.17856, avg_loss=0.18199]\n","Step 46745   [1.478 sec/step, loss=0.17713, avg_loss=0.18203]\n","Step 46746   [1.486 sec/step, loss=0.19301, avg_loss=0.18225]\n","Step 46747   [1.493 sec/step, loss=0.17587, avg_loss=0.18224]\n","Step 46748   [1.496 sec/step, loss=0.17106, avg_loss=0.18225]\n","Generated 32 batches of size 32 in 10.408 sec\n","Step 46749   [1.495 sec/step, loss=0.18878, avg_loss=0.18222]\n","Step 46750   [1.491 sec/step, loss=0.19452, avg_loss=0.18229]\n","Step 46751   [1.487 sec/step, loss=0.19248, avg_loss=0.18224]\n","Step 46752   [1.482 sec/step, loss=0.18619, avg_loss=0.18217]\n","Step 46753   [1.477 sec/step, loss=0.18199, avg_loss=0.18216]\n","Step 46754   [1.481 sec/step, loss=0.19215, avg_loss=0.18224]\n","Step 46755   [1.478 sec/step, loss=0.17550, avg_loss=0.18195]\n","Step 46756   [1.484 sec/step, loss=0.19181, avg_loss=0.18216]\n","Step 46757   [1.482 sec/step, loss=0.16730, avg_loss=0.18221]\n","Step 46758   [1.485 sec/step, loss=0.19167, avg_loss=0.18226]\n","Step 46759   [1.487 sec/step, loss=0.17645, avg_loss=0.18227]\n","Step 46760   [1.486 sec/step, loss=0.18940, avg_loss=0.18218]\n","Step 46761   [1.488 sec/step, loss=0.18691, avg_loss=0.18227]\n","Step 46762   [1.487 sec/step, loss=0.16052, avg_loss=0.18227]\n","Step 46763   [1.486 sec/step, loss=0.19300, avg_loss=0.18215]\n","Step 46764   [1.487 sec/step, loss=0.18570, avg_loss=0.18196]\n","Step 46765   [1.487 sec/step, loss=0.15473, avg_loss=0.18189]\n","Step 46766   [1.493 sec/step, loss=0.18552, avg_loss=0.18216]\n","Step 46767   [1.489 sec/step, loss=0.17576, avg_loss=0.18195]\n","Step 46768   [1.486 sec/step, loss=0.17674, avg_loss=0.18172]\n","Step 46769   [1.490 sec/step, loss=0.19257, avg_loss=0.18188]\n","Step 46770   [1.486 sec/step, loss=0.15781, avg_loss=0.18151]\n","Step 46771   [1.493 sec/step, loss=0.19570, avg_loss=0.18176]\n","Step 46772   [1.490 sec/step, loss=0.16404, avg_loss=0.18148]\n","Step 46773   [1.490 sec/step, loss=0.19034, avg_loss=0.18149]\n","Step 46774   [1.484 sec/step, loss=0.15635, avg_loss=0.18118]\n","Step 46775   [1.484 sec/step, loss=0.19045, avg_loss=0.18109]\n","Step 46776   [1.491 sec/step, loss=0.18961, avg_loss=0.18101]\n","Step 46777   [1.492 sec/step, loss=0.17444, avg_loss=0.18083]\n","Step 46778   [1.494 sec/step, loss=0.17296, avg_loss=0.18068]\n","Step 46779   [1.495 sec/step, loss=0.16935, avg_loss=0.18055]\n","Step 46780   [1.503 sec/step, loss=0.19053, avg_loss=0.18071]\n","Step 46781   [1.496 sec/step, loss=0.15635, avg_loss=0.18032]\n","Generated 32 batches of size 32 in 10.465 sec\n","Step 46782   [1.491 sec/step, loss=0.19152, avg_loss=0.18019]\n","Step 46783   [1.490 sec/step, loss=0.17536, avg_loss=0.18031]\n","Step 46784   [1.485 sec/step, loss=0.18620, avg_loss=0.18023]\n","Step 46785   [1.481 sec/step, loss=0.18470, avg_loss=0.18014]\n","Step 46786   [1.479 sec/step, loss=0.15525, avg_loss=0.18008]\n","Step 46787   [1.481 sec/step, loss=0.19368, avg_loss=0.18008]\n","Step 46788   [1.483 sec/step, loss=0.17859, avg_loss=0.18009]\n","Step 46789   [1.485 sec/step, loss=0.18495, avg_loss=0.18026]\n","Step 46790   [1.488 sec/step, loss=0.16486, avg_loss=0.18028]\n","Step 46791   [1.493 sec/step, loss=0.18518, avg_loss=0.18044]\n","Step 46792   [1.487 sec/step, loss=0.15477, avg_loss=0.17995]\n","Step 46793   [1.487 sec/step, loss=0.19588, avg_loss=0.17999]\n","Step 46794   [1.483 sec/step, loss=0.16821, avg_loss=0.17972]\n","Step 46795   [1.484 sec/step, loss=0.19072, avg_loss=0.17967]\n","Step 46796   [1.483 sec/step, loss=0.18926, avg_loss=0.17964]\n","Step 46797   [1.483 sec/step, loss=0.18547, avg_loss=0.17957]\n","Step 46798   [1.481 sec/step, loss=0.15563, avg_loss=0.17942]\n","Step 46799   [1.482 sec/step, loss=0.18259, avg_loss=0.17937]\n","Step 46800   [1.479 sec/step, loss=0.18372, avg_loss=0.17929]\n","Writing summary at step: 46800\n","Step 46801   [1.476 sec/step, loss=0.15305, avg_loss=0.17898]\n","Step 46802   [1.476 sec/step, loss=0.15740, avg_loss=0.17884]\n","Step 46803   [1.477 sec/step, loss=0.19811, avg_loss=0.17891]\n","Step 46804   [1.481 sec/step, loss=0.19132, avg_loss=0.17915]\n","Step 46805   [1.476 sec/step, loss=0.16780, avg_loss=0.17880]\n","Step 46806   [1.478 sec/step, loss=0.16631, avg_loss=0.17887]\n","Step 46807   [1.485 sec/step, loss=0.18923, avg_loss=0.17897]\n","Step 46808   [1.486 sec/step, loss=0.15910, avg_loss=0.17890]\n","Step 46809   [1.487 sec/step, loss=0.18032, avg_loss=0.17880]\n","Step 46810   [1.492 sec/step, loss=0.17479, avg_loss=0.17899]\n","Step 46811   [1.499 sec/step, loss=0.17638, avg_loss=0.17921]\n","Step 46812   [1.500 sec/step, loss=0.19165, avg_loss=0.17927]\n","Generated 32 batches of size 32 in 10.412 sec\n","Step 46813   [1.494 sec/step, loss=0.17476, avg_loss=0.17907]\n","Step 46814   [1.489 sec/step, loss=0.19387, avg_loss=0.17917]\n","Step 46815   [1.484 sec/step, loss=0.19032, avg_loss=0.17913]\n","Step 46816   [1.482 sec/step, loss=0.18616, avg_loss=0.17920]\n","Step 46817   [1.479 sec/step, loss=0.17115, avg_loss=0.17914]\n","Step 46818   [1.479 sec/step, loss=0.15710, avg_loss=0.17905]\n","Step 46819   [1.473 sec/step, loss=0.15349, avg_loss=0.17868]\n","Step 46820   [1.475 sec/step, loss=0.18319, avg_loss=0.17879]\n","Step 46821   [1.471 sec/step, loss=0.16749, avg_loss=0.17859]\n","Step 46822   [1.476 sec/step, loss=0.19652, avg_loss=0.17888]\n","Step 46823   [1.477 sec/step, loss=0.19434, avg_loss=0.17894]\n","Step 46824   [1.481 sec/step, loss=0.17623, avg_loss=0.17913]\n","Step 46825   [1.481 sec/step, loss=0.16984, avg_loss=0.17929]\n","Step 46826   [1.484 sec/step, loss=0.18978, avg_loss=0.17948]\n","Step 46827   [1.484 sec/step, loss=0.19339, avg_loss=0.17949]\n","Step 46828   [1.481 sec/step, loss=0.17586, avg_loss=0.17938]\n","Step 46829   [1.483 sec/step, loss=0.19353, avg_loss=0.17937]\n","Step 46830   [1.478 sec/step, loss=0.15562, avg_loss=0.17902]\n","Step 46831   [1.477 sec/step, loss=0.18880, avg_loss=0.17893]\n","Step 46832   [1.482 sec/step, loss=0.19373, avg_loss=0.17925]\n","Step 46833   [1.482 sec/step, loss=0.15955, avg_loss=0.17916]\n","Step 46834   [1.478 sec/step, loss=0.16553, avg_loss=0.17888]\n","Step 46835   [1.482 sec/step, loss=0.17626, avg_loss=0.17900]\n","Step 46836   [1.477 sec/step, loss=0.16640, avg_loss=0.17873]\n","Step 46837   [1.474 sec/step, loss=0.16560, avg_loss=0.17848]\n","Step 46838   [1.474 sec/step, loss=0.19049, avg_loss=0.17849]\n","Step 46839   [1.474 sec/step, loss=0.15728, avg_loss=0.17824]\n","Step 46840   [1.484 sec/step, loss=0.18892, avg_loss=0.17853]\n","Step 46841   [1.488 sec/step, loss=0.17684, avg_loss=0.17860]\n","Step 46842   [1.491 sec/step, loss=0.19438, avg_loss=0.17857]\n","Step 46843   [1.502 sec/step, loss=0.19299, avg_loss=0.17895]\n","Generated 32 batches of size 32 in 10.541 sec\n","Step 46844   [1.504 sec/step, loss=0.19003, avg_loss=0.17906]\n","Step 46845   [1.502 sec/step, loss=0.18736, avg_loss=0.17916]\n","Step 46846   [1.498 sec/step, loss=0.18636, avg_loss=0.17910]\n","Step 46847   [1.497 sec/step, loss=0.19211, avg_loss=0.17926]\n","Step 46848   [1.498 sec/step, loss=0.18450, avg_loss=0.17939]\n","Step 46849   [1.495 sec/step, loss=0.19021, avg_loss=0.17941]\n","Step 46850   [1.492 sec/step, loss=0.17607, avg_loss=0.17922]\n","Step 46851   [1.489 sec/step, loss=0.18201, avg_loss=0.17912]\n","Step 46852   [1.491 sec/step, loss=0.19774, avg_loss=0.17923]\n","Step 46853   [1.488 sec/step, loss=0.16057, avg_loss=0.17902]\n","Step 46854   [1.487 sec/step, loss=0.19214, avg_loss=0.17902]\n","Step 46855   [1.483 sec/step, loss=0.15467, avg_loss=0.17881]\n","Step 46856   [1.480 sec/step, loss=0.17723, avg_loss=0.17867]\n","Step 46857   [1.487 sec/step, loss=0.18892, avg_loss=0.17888]\n","Step 46858   [1.482 sec/step, loss=0.15872, avg_loss=0.17855]\n","Step 46859   [1.485 sec/step, loss=0.19804, avg_loss=0.17877]\n","Step 46860   [1.485 sec/step, loss=0.19409, avg_loss=0.17882]\n","Step 46861   [1.485 sec/step, loss=0.19116, avg_loss=0.17886]\n","Step 46862   [1.491 sec/step, loss=0.18766, avg_loss=0.17913]\n","Step 46863   [1.489 sec/step, loss=0.16649, avg_loss=0.17886]\n","Step 46864   [1.486 sec/step, loss=0.17919, avg_loss=0.17880]\n","Step 46865   [1.492 sec/step, loss=0.19723, avg_loss=0.17922]\n","Step 46866   [1.490 sec/step, loss=0.17458, avg_loss=0.17912]\n","Step 46867   [1.488 sec/step, loss=0.16754, avg_loss=0.17903]\n","Step 46868   [1.485 sec/step, loss=0.16088, avg_loss=0.17887]\n","Step 46869   [1.485 sec/step, loss=0.20557, avg_loss=0.17900]\n","Step 46870   [1.488 sec/step, loss=0.19411, avg_loss=0.17937]\n","Step 46871   [1.487 sec/step, loss=0.17571, avg_loss=0.17917]\n","Step 46872   [1.494 sec/step, loss=0.20527, avg_loss=0.17958]\n","Step 46873   [1.496 sec/step, loss=0.18512, avg_loss=0.17953]\n","Step 46874   [1.500 sec/step, loss=0.18300, avg_loss=0.17979]\n","Step 46875   [1.505 sec/step, loss=0.19567, avg_loss=0.17985]\n","Generated 32 batches of size 32 in 10.554 sec\n","Step 46876   [1.503 sec/step, loss=0.20247, avg_loss=0.17998]\n","Step 46877   [1.502 sec/step, loss=0.19360, avg_loss=0.18017]\n","Step 46878   [1.499 sec/step, loss=0.17733, avg_loss=0.18021]\n","Step 46879   [1.496 sec/step, loss=0.17273, avg_loss=0.18024]\n","Step 46880   [1.488 sec/step, loss=0.17091, avg_loss=0.18005]\n","Step 46881   [1.490 sec/step, loss=0.19156, avg_loss=0.18040]\n","Step 46882   [1.484 sec/step, loss=0.16957, avg_loss=0.18018]\n","Step 46883   [1.484 sec/step, loss=0.17764, avg_loss=0.18020]\n","Step 46884   [1.484 sec/step, loss=0.20514, avg_loss=0.18039]\n","Step 46885   [1.484 sec/step, loss=0.19420, avg_loss=0.18049]\n","Step 46886   [1.486 sec/step, loss=0.16441, avg_loss=0.18058]\n","Step 46887   [1.485 sec/step, loss=0.19104, avg_loss=0.18055]\n","Step 46888   [1.488 sec/step, loss=0.19223, avg_loss=0.18069]\n","Step 46889   [1.488 sec/step, loss=0.19585, avg_loss=0.18080]\n","Step 46890   [1.488 sec/step, loss=0.18493, avg_loss=0.18100]\n","Step 46891   [1.488 sec/step, loss=0.18569, avg_loss=0.18100]\n","Step 46892   [1.489 sec/step, loss=0.15990, avg_loss=0.18106]\n","Step 46893   [1.489 sec/step, loss=0.19430, avg_loss=0.18104]\n","Step 46894   [1.488 sec/step, loss=0.15469, avg_loss=0.18090]\n","Step 46895   [1.488 sec/step, loss=0.19254, avg_loss=0.18092]\n","Step 46896   [1.487 sec/step, loss=0.18830, avg_loss=0.18091]\n","Step 46897   [1.485 sec/step, loss=0.17993, avg_loss=0.18086]\n","Step 46898   [1.485 sec/step, loss=0.15917, avg_loss=0.18089]\n","Step 46899   [1.485 sec/step, loss=0.19914, avg_loss=0.18106]\n","Step 46900   [1.488 sec/step, loss=0.19452, avg_loss=0.18117]\n","Writing summary at step: 46900\n","Step 46901   [1.493 sec/step, loss=0.18638, avg_loss=0.18150]\n","Step 46902   [1.497 sec/step, loss=0.17769, avg_loss=0.18170]\n","Step 46903   [1.501 sec/step, loss=0.19577, avg_loss=0.18168]\n","Step 46904   [1.498 sec/step, loss=0.15954, avg_loss=0.18136]\n","Step 46905   [1.503 sec/step, loss=0.16454, avg_loss=0.18133]\n","Step 46906   [1.509 sec/step, loss=0.17838, avg_loss=0.18145]\n","Step 46907   [1.503 sec/step, loss=0.17023, avg_loss=0.18126]\n","Generated 32 batches of size 32 in 10.586 sec\n","Step 46908   [1.508 sec/step, loss=0.19818, avg_loss=0.18165]\n","Step 46909   [1.504 sec/step, loss=0.17636, avg_loss=0.18161]\n","Step 46910   [1.503 sec/step, loss=0.17560, avg_loss=0.18162]\n","Step 46911   [1.496 sec/step, loss=0.15818, avg_loss=0.18144]\n","Step 46912   [1.492 sec/step, loss=0.19195, avg_loss=0.18144]\n","Step 46913   [1.488 sec/step, loss=0.15196, avg_loss=0.18121]\n","Step 46914   [1.485 sec/step, loss=0.17439, avg_loss=0.18102]\n","Step 46915   [1.480 sec/step, loss=0.15249, avg_loss=0.18064]\n","Step 46916   [1.480 sec/step, loss=0.19041, avg_loss=0.18068]\n","Step 46917   [1.480 sec/step, loss=0.15529, avg_loss=0.18052]\n","Step 46918   [1.483 sec/step, loss=0.18160, avg_loss=0.18077]\n","Step 46919   [1.486 sec/step, loss=0.16391, avg_loss=0.18087]\n","Step 46920   [1.485 sec/step, loss=0.16871, avg_loss=0.18073]\n","Step 46921   [1.490 sec/step, loss=0.19924, avg_loss=0.18104]\n","Step 46922   [1.487 sec/step, loss=0.17406, avg_loss=0.18082]\n","Step 46923   [1.487 sec/step, loss=0.19200, avg_loss=0.18080]\n","Step 46924   [1.484 sec/step, loss=0.15769, avg_loss=0.18061]\n","Step 46925   [1.489 sec/step, loss=0.15442, avg_loss=0.18046]\n","Step 46926   [1.489 sec/step, loss=0.19091, avg_loss=0.18047]\n","Step 46927   [1.490 sec/step, loss=0.19570, avg_loss=0.18049]\n","Step 46928   [1.491 sec/step, loss=0.17475, avg_loss=0.18048]\n","Step 46929   [1.486 sec/step, loss=0.15370, avg_loss=0.18008]\n","Step 46930   [1.488 sec/step, loss=0.17587, avg_loss=0.18028]\n","Step 46931   [1.489 sec/step, loss=0.18827, avg_loss=0.18028]\n","Step 46932   [1.489 sec/step, loss=0.19323, avg_loss=0.18027]\n","Step 46933   [1.493 sec/step, loss=0.19133, avg_loss=0.18059]\n","Step 46934   [1.500 sec/step, loss=0.18615, avg_loss=0.18080]\n","Step 46935   [1.506 sec/step, loss=0.18486, avg_loss=0.18088]\n","Step 46936   [1.515 sec/step, loss=0.19035, avg_loss=0.18112]\n","Step 46937   [1.521 sec/step, loss=0.17694, avg_loss=0.18124]\n","Step 46938   [1.524 sec/step, loss=0.18952, avg_loss=0.18123]\n","Generated 32 batches of size 32 in 10.619 sec\n","Step 46939   [1.530 sec/step, loss=0.19128, avg_loss=0.18157]\n","Step 46940   [1.527 sec/step, loss=0.19306, avg_loss=0.18161]\n","Step 46941   [1.522 sec/step, loss=0.17206, avg_loss=0.18156]\n","Step 46942   [1.518 sec/step, loss=0.18971, avg_loss=0.18151]\n","Step 46943   [1.508 sec/step, loss=0.15983, avg_loss=0.18118]\n","Step 46944   [1.506 sec/step, loss=0.19141, avg_loss=0.18120]\n","Step 46945   [1.506 sec/step, loss=0.19338, avg_loss=0.18126]\n","Step 46946   [1.502 sec/step, loss=0.16877, avg_loss=0.18108]\n","Step 46947   [1.495 sec/step, loss=0.15513, avg_loss=0.18071]\n","Step 46948   [1.493 sec/step, loss=0.17654, avg_loss=0.18063]\n","Step 46949   [1.490 sec/step, loss=0.17392, avg_loss=0.18047]\n","Step 46950   [1.493 sec/step, loss=0.19215, avg_loss=0.18063]\n","Step 46951   [1.494 sec/step, loss=0.17307, avg_loss=0.18054]\n","Step 46952   [1.491 sec/step, loss=0.16618, avg_loss=0.18022]\n","Step 46953   [1.497 sec/step, loss=0.18857, avg_loss=0.18050]\n","Step 46954   [1.493 sec/step, loss=0.16662, avg_loss=0.18025]\n","Step 46955   [1.495 sec/step, loss=0.16592, avg_loss=0.18036]\n","Step 46956   [1.498 sec/step, loss=0.19186, avg_loss=0.18051]\n","Step 46957   [1.498 sec/step, loss=0.19520, avg_loss=0.18057]\n","Step 46958   [1.503 sec/step, loss=0.18976, avg_loss=0.18088]\n","Step 46959   [1.497 sec/step, loss=0.15956, avg_loss=0.18050]\n","Step 46960   [1.492 sec/step, loss=0.17577, avg_loss=0.18031]\n","Step 46961   [1.490 sec/step, loss=0.18044, avg_loss=0.18021]\n","Step 46962   [1.491 sec/step, loss=0.18763, avg_loss=0.18021]\n","Step 46963   [1.493 sec/step, loss=0.19209, avg_loss=0.18046]\n","Step 46964   [1.496 sec/step, loss=0.18659, avg_loss=0.18054]\n","Step 46965   [1.495 sec/step, loss=0.18613, avg_loss=0.18042]\n","Step 46966   [1.502 sec/step, loss=0.18903, avg_loss=0.18057]\n","Step 46967   [1.505 sec/step, loss=0.16115, avg_loss=0.18051]\n","Step 46968   [1.515 sec/step, loss=0.18744, avg_loss=0.18077]\n","Step 46969   [1.516 sec/step, loss=0.17738, avg_loss=0.18049]\n","Step 46970   [1.514 sec/step, loss=0.15404, avg_loss=0.18009]\n","Step 46971   [1.520 sec/step, loss=0.18664, avg_loss=0.18020]\n","Generated 32 batches of size 32 in 10.619 sec\n","Step 46972   [1.516 sec/step, loss=0.21754, avg_loss=0.18032]\n","Step 46973   [1.515 sec/step, loss=0.19328, avg_loss=0.18040]\n","Step 46974   [1.514 sec/step, loss=0.16578, avg_loss=0.18023]\n","Step 46975   [1.504 sec/step, loss=0.17585, avg_loss=0.18003]\n","Step 46976   [1.495 sec/step, loss=0.15878, avg_loss=0.17959]\n","Step 46977   [1.495 sec/step, loss=0.18797, avg_loss=0.17954]\n","Step 46978   [1.495 sec/step, loss=0.17528, avg_loss=0.17952]\n","Step 46979   [1.498 sec/step, loss=0.19837, avg_loss=0.17977]\n","Step 46980   [1.499 sec/step, loss=0.18295, avg_loss=0.17989]\n","Step 46981   [1.500 sec/step, loss=0.19093, avg_loss=0.17989]\n","Step 46982   [1.505 sec/step, loss=0.18809, avg_loss=0.18007]\n","Step 46983   [1.507 sec/step, loss=0.17586, avg_loss=0.18006]\n","Step 46984   [1.507 sec/step, loss=0.19341, avg_loss=0.17994]\n","Step 46985   [1.505 sec/step, loss=0.16735, avg_loss=0.17967]\n","Step 46986   [1.508 sec/step, loss=0.18474, avg_loss=0.17987]\n","Step 46987   [1.504 sec/step, loss=0.16068, avg_loss=0.17957]\n","Step 46988   [1.499 sec/step, loss=0.16209, avg_loss=0.17927]\n","Step 46989   [1.499 sec/step, loss=0.19934, avg_loss=0.17930]\n","Step 46990   [1.497 sec/step, loss=0.15468, avg_loss=0.17900]\n","Step 46991   [1.492 sec/step, loss=0.17269, avg_loss=0.17887]\n","Step 46992   [1.499 sec/step, loss=0.19240, avg_loss=0.17920]\n","Step 46993   [1.496 sec/step, loss=0.18540, avg_loss=0.17911]\n","Step 46994   [1.495 sec/step, loss=0.15968, avg_loss=0.17916]\n","Step 46995   [1.495 sec/step, loss=0.19483, avg_loss=0.17918]\n","Step 46996   [1.495 sec/step, loss=0.18814, avg_loss=0.17918]\n","Step 46997   [1.497 sec/step, loss=0.18669, avg_loss=0.17924]\n","Step 46998   [1.505 sec/step, loss=0.17966, avg_loss=0.17945]\n","Step 46999   [1.502 sec/step, loss=0.15795, avg_loss=0.17904]\n","Step 47000   [1.501 sec/step, loss=0.17374, avg_loss=0.17883]\n","Writing summary at step: 47000\n","Saving audio and alignment...\n","Generated 32 batches of size 32 in 9.652 sec\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/train-step-000047000-align000.png\n","100% 1/1 [00:03<00:00,  3.97s/it]\n","Test finished for step 47000.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000047000-align000.png\n"," 50% 1/2 [00:11<00:11, 11.54s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000047000-align001.png\n","100% 2/2 [00:23<00:00, 11.55s/it]\n","Test finished for step 47000.\n","Step 47001   [1.502 sec/step, loss=0.19069, avg_loss=0.17887]\n","Step 47002   [1.503 sec/step, loss=0.20068, avg_loss=0.17910]\n","Step 47003   [1.496 sec/step, loss=0.18427, avg_loss=0.17899]\n","Step 47004   [1.498 sec/step, loss=0.19127, avg_loss=0.17931]\n","Step 47005   [1.492 sec/step, loss=0.16125, avg_loss=0.17927]\n","Step 47006   [1.491 sec/step, loss=0.19352, avg_loss=0.17942]\n","Step 47007   [1.490 sec/step, loss=0.17667, avg_loss=0.17949]\n","Step 47008   [1.485 sec/step, loss=0.17546, avg_loss=0.17926]\n","Step 47009   [1.488 sec/step, loss=0.18881, avg_loss=0.17939]\n","Step 47010   [1.486 sec/step, loss=0.17280, avg_loss=0.17936]\n","Step 47011   [1.488 sec/step, loss=0.16880, avg_loss=0.17946]\n","Step 47012   [1.489 sec/step, loss=0.19715, avg_loss=0.17952]\n","Step 47013   [1.492 sec/step, loss=0.16591, avg_loss=0.17966]\n","Step 47014   [1.496 sec/step, loss=0.19350, avg_loss=0.17985]\n","Step 47015   [1.496 sec/step, loss=0.16237, avg_loss=0.17995]\n","Step 47016   [1.496 sec/step, loss=0.19216, avg_loss=0.17996]\n","Step 47017   [1.500 sec/step, loss=0.18902, avg_loss=0.18030]\n","Step 47018   [1.502 sec/step, loss=0.19244, avg_loss=0.18041]\n","Step 47019   [1.500 sec/step, loss=0.16162, avg_loss=0.18039]\n","Step 47020   [1.504 sec/step, loss=0.18871, avg_loss=0.18059]\n","Step 47021   [1.501 sec/step, loss=0.18391, avg_loss=0.18043]\n","Step 47022   [1.498 sec/step, loss=0.15567, avg_loss=0.18025]\n","Step 47023   [1.497 sec/step, loss=0.18773, avg_loss=0.18021]\n","Step 47024   [1.497 sec/step, loss=0.15234, avg_loss=0.18015]\n","Step 47025   [1.497 sec/step, loss=0.18380, avg_loss=0.18045]\n","Step 47026   [1.497 sec/step, loss=0.18264, avg_loss=0.18036]\n","Step 47027   [1.496 sec/step, loss=0.19205, avg_loss=0.18033]\n","Step 47028   [1.500 sec/step, loss=0.17682, avg_loss=0.18035]\n","Step 47029   [1.502 sec/step, loss=0.15907, avg_loss=0.18040]\n","Step 47030   [1.511 sec/step, loss=0.19837, avg_loss=0.18063]\n","Step 47031   [1.513 sec/step, loss=0.17315, avg_loss=0.18048]\n","Step 47032   [1.517 sec/step, loss=0.19005, avg_loss=0.18044]\n","Step 47033   [1.517 sec/step, loss=0.17132, avg_loss=0.18024]\n","Generated 32 batches of size 32 in 10.906 sec\n","Step 47034   [1.514 sec/step, loss=0.18953, avg_loss=0.18028]\n","Step 47035   [1.506 sec/step, loss=0.16752, avg_loss=0.18010]\n","Step 47036   [1.502 sec/step, loss=0.19192, avg_loss=0.18012]\n","Step 47037   [1.501 sec/step, loss=0.19065, avg_loss=0.18026]\n","Step 47038   [1.492 sec/step, loss=0.16813, avg_loss=0.18004]\n","Step 47039   [1.489 sec/step, loss=0.18942, avg_loss=0.18002]\n","Step 47040   [1.488 sec/step, loss=0.15768, avg_loss=0.17967]\n","Step 47041   [1.486 sec/step, loss=0.15865, avg_loss=0.17954]\n","Step 47042   [1.486 sec/step, loss=0.18989, avg_loss=0.17954]\n","Step 47043   [1.486 sec/step, loss=0.15185, avg_loss=0.17946]\n","Step 47044   [1.486 sec/step, loss=0.19270, avg_loss=0.17947]\n","Step 47045   [1.486 sec/step, loss=0.19163, avg_loss=0.17945]\n","Step 47046   [1.490 sec/step, loss=0.18913, avg_loss=0.17966]\n","Step 47047   [1.496 sec/step, loss=0.18942, avg_loss=0.18000]\n","Step 47048   [1.496 sec/step, loss=0.17822, avg_loss=0.18002]\n","Step 47049   [1.496 sec/step, loss=0.17554, avg_loss=0.18003]\n","Step 47050   [1.496 sec/step, loss=0.18899, avg_loss=0.18000]\n","Step 47051   [1.498 sec/step, loss=0.19429, avg_loss=0.18021]\n","Step 47052   [1.498 sec/step, loss=0.17483, avg_loss=0.18030]\n","Step 47053   [1.493 sec/step, loss=0.17451, avg_loss=0.18016]\n","Step 47054   [1.498 sec/step, loss=0.18939, avg_loss=0.18039]\n","Step 47055   [1.497 sec/step, loss=0.16789, avg_loss=0.18041]\n","Step 47056   [1.496 sec/step, loss=0.18804, avg_loss=0.18037]\n","Step 47057   [1.492 sec/step, loss=0.16655, avg_loss=0.18008]\n","Step 47058   [1.492 sec/step, loss=0.18566, avg_loss=0.18004]\n","Step 47059   [1.498 sec/step, loss=0.18813, avg_loss=0.18033]\n","Step 47060   [1.502 sec/step, loss=0.16108, avg_loss=0.18018]\n","Step 47061   [1.501 sec/step, loss=0.16888, avg_loss=0.18006]\n","Step 47062   [1.498 sec/step, loss=0.15908, avg_loss=0.17978]\n","Step 47063   [1.502 sec/step, loss=0.18750, avg_loss=0.17973]\n","Step 47064   [1.506 sec/step, loss=0.18710, avg_loss=0.17974]\n","Step 47065   [1.503 sec/step, loss=0.16601, avg_loss=0.17954]\n","Generated 32 batches of size 32 in 10.537 sec\n","Step 47066   [1.501 sec/step, loss=0.18464, avg_loss=0.17949]\n","Step 47067   [1.502 sec/step, loss=0.16800, avg_loss=0.17956]\n","Step 47068   [1.495 sec/step, loss=0.18190, avg_loss=0.17951]\n","Step 47069   [1.494 sec/step, loss=0.18822, avg_loss=0.17961]\n","Step 47070   [1.496 sec/step, loss=0.19397, avg_loss=0.18001]\n","Step 47071   [1.487 sec/step, loss=0.16849, avg_loss=0.17983]\n","Step 47072   [1.480 sec/step, loss=0.16642, avg_loss=0.17932]\n","Step 47073   [1.479 sec/step, loss=0.18852, avg_loss=0.17927]\n","Step 47074   [1.481 sec/step, loss=0.17750, avg_loss=0.17939]\n","Step 47075   [1.486 sec/step, loss=0.19509, avg_loss=0.17958]\n","Step 47076   [1.491 sec/step, loss=0.19225, avg_loss=0.17992]\n","Step 47077   [1.488 sec/step, loss=0.16431, avg_loss=0.17968]\n","Step 47078   [1.489 sec/step, loss=0.16819, avg_loss=0.17961]\n","Step 47079   [1.489 sec/step, loss=0.19117, avg_loss=0.17954]\n","Step 47080   [1.492 sec/step, loss=0.19054, avg_loss=0.17961]\n","Step 47081   [1.493 sec/step, loss=0.18683, avg_loss=0.17957]\n","Step 47082   [1.490 sec/step, loss=0.15855, avg_loss=0.17928]\n","Step 47083   [1.493 sec/step, loss=0.19669, avg_loss=0.17949]\n","Step 47084   [1.486 sec/step, loss=0.15879, avg_loss=0.17914]\n","Step 47085   [1.486 sec/step, loss=0.17832, avg_loss=0.17925]\n","Step 47086   [1.485 sec/step, loss=0.17579, avg_loss=0.17916]\n","Step 47087   [1.483 sec/step, loss=0.15566, avg_loss=0.17911]\n","Step 47088   [1.488 sec/step, loss=0.19275, avg_loss=0.17942]\n","Step 47089   [1.487 sec/step, loss=0.18743, avg_loss=0.17930]\n","Step 47090   [1.487 sec/step, loss=0.15644, avg_loss=0.17931]\n","Step 47091   [1.491 sec/step, loss=0.19354, avg_loss=0.17952]\n","Step 47092   [1.496 sec/step, loss=0.19144, avg_loss=0.17951]\n","Step 47093   [1.498 sec/step, loss=0.17292, avg_loss=0.17939]\n","Step 47094   [1.500 sec/step, loss=0.15709, avg_loss=0.17936]\n","Step 47095   [1.500 sec/step, loss=0.16609, avg_loss=0.17908]\n","Step 47096   [1.502 sec/step, loss=0.18887, avg_loss=0.17908]\n","Step 47097   [1.504 sec/step, loss=0.18240, avg_loss=0.17904]\n","Generated 32 batches of size 32 in 10.677 sec\n","Step 47098   [1.503 sec/step, loss=0.18919, avg_loss=0.17914]\n","Step 47099   [1.506 sec/step, loss=0.19307, avg_loss=0.17949]\n","Step 47100   [1.507 sec/step, loss=0.18590, avg_loss=0.17961]\n","Writing summary at step: 47100\n","Step 47101   [1.507 sec/step, loss=0.18462, avg_loss=0.17955]\n","Step 47102   [1.507 sec/step, loss=0.20731, avg_loss=0.17961]\n","Step 47103   [1.503 sec/step, loss=0.16507, avg_loss=0.17942]\n","Step 47104   [1.501 sec/step, loss=0.19600, avg_loss=0.17947]\n","Step 47105   [1.504 sec/step, loss=0.19162, avg_loss=0.17977]\n","Step 47106   [1.503 sec/step, loss=0.20355, avg_loss=0.17987]\n","Step 47107   [1.505 sec/step, loss=0.19978, avg_loss=0.18010]\n","Step 47108   [1.502 sec/step, loss=0.16030, avg_loss=0.17995]\n","Step 47109   [1.501 sec/step, loss=0.19119, avg_loss=0.17998]\n","Step 47110   [1.503 sec/step, loss=0.18829, avg_loss=0.18013]\n","Step 47111   [1.502 sec/step, loss=0.15720, avg_loss=0.18002]\n","Step 47112   [1.497 sec/step, loss=0.18556, avg_loss=0.17990]\n","Step 47113   [1.497 sec/step, loss=0.18068, avg_loss=0.18005]\n","Step 47114   [1.497 sec/step, loss=0.19052, avg_loss=0.18002]\n","Step 47115   [1.497 sec/step, loss=0.16091, avg_loss=0.18000]\n","Step 47116   [1.493 sec/step, loss=0.17364, avg_loss=0.17982]\n","Step 47117   [1.493 sec/step, loss=0.19119, avg_loss=0.17984]\n","Step 47118   [1.493 sec/step, loss=0.18520, avg_loss=0.17977]\n","Step 47119   [1.498 sec/step, loss=0.20170, avg_loss=0.18017]\n","Step 47120   [1.500 sec/step, loss=0.19434, avg_loss=0.18022]\n","Step 47121   [1.500 sec/step, loss=0.18082, avg_loss=0.18019]\n","Step 47122   [1.502 sec/step, loss=0.17044, avg_loss=0.18034]\n","Step 47123   [1.508 sec/step, loss=0.19400, avg_loss=0.18040]\n","Step 47124   [1.519 sec/step, loss=0.18858, avg_loss=0.18077]\n","Step 47125   [1.518 sec/step, loss=0.17108, avg_loss=0.18064]\n","Step 47126   [1.518 sec/step, loss=0.16889, avg_loss=0.18050]\n","Step 47127   [1.523 sec/step, loss=0.19196, avg_loss=0.18050]\n","Generated 32 batches of size 32 in 11.033 sec\n","Step 47128   [1.526 sec/step, loss=0.19708, avg_loss=0.18070]\n","Step 47129   [1.523 sec/step, loss=0.16118, avg_loss=0.18072]\n","Step 47130   [1.514 sec/step, loss=0.16133, avg_loss=0.18035]\n","Step 47131   [1.513 sec/step, loss=0.19202, avg_loss=0.18054]\n","Step 47132   [1.505 sec/step, loss=0.18247, avg_loss=0.18047]\n","Step 47133   [1.503 sec/step, loss=0.16563, avg_loss=0.18041]\n","Step 47134   [1.497 sec/step, loss=0.17035, avg_loss=0.18022]\n","Step 47135   [1.501 sec/step, loss=0.19376, avg_loss=0.18048]\n","Step 47136   [1.498 sec/step, loss=0.17480, avg_loss=0.18031]\n","Step 47137   [1.494 sec/step, loss=0.16754, avg_loss=0.18008]\n","Step 47138   [1.499 sec/step, loss=0.18715, avg_loss=0.18027]\n","Step 47139   [1.494 sec/step, loss=0.16358, avg_loss=0.18001]\n","Step 47140   [1.490 sec/step, loss=0.15577, avg_loss=0.17999]\n","Step 47141   [1.490 sec/step, loss=0.15353, avg_loss=0.17994]\n","Step 47142   [1.488 sec/step, loss=0.17931, avg_loss=0.17983]\n","Step 47143   [1.494 sec/step, loss=0.19196, avg_loss=0.18023]\n","Step 47144   [1.494 sec/step, loss=0.19941, avg_loss=0.18030]\n","Step 47145   [1.493 sec/step, loss=0.18760, avg_loss=0.18026]\n","Step 47146   [1.493 sec/step, loss=0.19052, avg_loss=0.18028]\n","Step 47147   [1.487 sec/step, loss=0.16726, avg_loss=0.18005]\n","Step 47148   [1.484 sec/step, loss=0.16144, avg_loss=0.17989]\n","Step 47149   [1.487 sec/step, loss=0.19029, avg_loss=0.18003]\n","Step 47150   [1.484 sec/step, loss=0.17948, avg_loss=0.17994]\n","Step 47151   [1.484 sec/step, loss=0.18775, avg_loss=0.17987]\n","Step 47152   [1.485 sec/step, loss=0.17346, avg_loss=0.17986]\n","Step 47153   [1.485 sec/step, loss=0.17094, avg_loss=0.17982]\n","Step 47154   [1.484 sec/step, loss=0.19223, avg_loss=0.17985]\n","Step 47155   [1.493 sec/step, loss=0.19804, avg_loss=0.18015]\n","Step 47156   [1.497 sec/step, loss=0.18780, avg_loss=0.18015]\n","Step 47157   [1.498 sec/step, loss=0.17757, avg_loss=0.18026]\n","Step 47158   [1.501 sec/step, loss=0.18563, avg_loss=0.18026]\n","Step 47159   [1.506 sec/step, loss=0.18453, avg_loss=0.18023]\n","Generated 32 batches of size 32 in 10.509 sec\n","Step 47160   [1.505 sec/step, loss=0.17192, avg_loss=0.18033]\n","Step 47161   [1.508 sec/step, loss=0.19345, avg_loss=0.18058]\n","Step 47162   [1.511 sec/step, loss=0.19182, avg_loss=0.18091]\n","Step 47163   [1.508 sec/step, loss=0.19051, avg_loss=0.18094]\n","Step 47164   [1.504 sec/step, loss=0.19200, avg_loss=0.18099]\n","Step 47165   [1.504 sec/step, loss=0.18904, avg_loss=0.18122]\n","Step 47166   [1.496 sec/step, loss=0.15972, avg_loss=0.18097]\n","Step 47167   [1.499 sec/step, loss=0.19156, avg_loss=0.18120]\n","Step 47168   [1.501 sec/step, loss=0.19567, avg_loss=0.18134]\n","Step 47169   [1.499 sec/step, loss=0.17724, avg_loss=0.18123]\n","Step 47170   [1.499 sec/step, loss=0.18711, avg_loss=0.18116]\n","Step 47171   [1.502 sec/step, loss=0.18381, avg_loss=0.18131]\n","Step 47172   [1.508 sec/step, loss=0.15546, avg_loss=0.18121]\n","Step 47173   [1.503 sec/step, loss=0.16076, avg_loss=0.18093]\n","Step 47174   [1.500 sec/step, loss=0.15581, avg_loss=0.18071]\n","Step 47175   [1.495 sec/step, loss=0.17585, avg_loss=0.18052]\n","Step 47176   [1.494 sec/step, loss=0.19374, avg_loss=0.18053]\n","Step 47177   [1.495 sec/step, loss=0.18100, avg_loss=0.18070]\n","Step 47178   [1.499 sec/step, loss=0.20287, avg_loss=0.18105]\n","Step 47179   [1.495 sec/step, loss=0.16184, avg_loss=0.18075]\n","Step 47180   [1.494 sec/step, loss=0.18831, avg_loss=0.18073]\n","Step 47181   [1.493 sec/step, loss=0.19539, avg_loss=0.18082]\n","Step 47182   [1.497 sec/step, loss=0.19210, avg_loss=0.18115]\n","Step 47183   [1.494 sec/step, loss=0.17755, avg_loss=0.18096]\n","Step 47184   [1.494 sec/step, loss=0.16914, avg_loss=0.18106]\n","Step 47185   [1.497 sec/step, loss=0.19431, avg_loss=0.18122]\n","Step 47186   [1.495 sec/step, loss=0.17502, avg_loss=0.18122]\n","Step 47187   [1.500 sec/step, loss=0.17090, avg_loss=0.18137]\n","Step 47188   [1.497 sec/step, loss=0.15580, avg_loss=0.18100]\n","Step 47189   [1.503 sec/step, loss=0.19422, avg_loss=0.18107]\n","Step 47190   [1.507 sec/step, loss=0.17159, avg_loss=0.18122]\n","Step 47191   [1.512 sec/step, loss=0.19509, avg_loss=0.18123]\n","Step 47192   [1.507 sec/step, loss=0.16662, avg_loss=0.18099]\n","Generated 32 batches of size 32 in 10.556 sec\n","Step 47193   [1.510 sec/step, loss=0.19437, avg_loss=0.18120]\n","Step 47194   [1.514 sec/step, loss=0.19183, avg_loss=0.18155]\n","Step 47195   [1.515 sec/step, loss=0.18972, avg_loss=0.18178]\n","Step 47196   [1.513 sec/step, loss=0.18940, avg_loss=0.18179]\n","Step 47197   [1.512 sec/step, loss=0.19088, avg_loss=0.18187]\n","Step 47198   [1.508 sec/step, loss=0.17658, avg_loss=0.18175]\n","Step 47199   [1.504 sec/step, loss=0.17433, avg_loss=0.18156]\n","Step 47200   [1.504 sec/step, loss=0.19549, avg_loss=0.18166]\n","Writing summary at step: 47200\n","Step 47201   [1.500 sec/step, loss=0.16830, avg_loss=0.18149]\n","Step 47202   [1.494 sec/step, loss=0.16685, avg_loss=0.18109]\n","Step 47203   [1.494 sec/step, loss=0.15957, avg_loss=0.18103]\n","Step 47204   [1.497 sec/step, loss=0.20081, avg_loss=0.18108]\n","Step 47205   [1.497 sec/step, loss=0.18041, avg_loss=0.18097]\n","Step 47206   [1.498 sec/step, loss=0.19246, avg_loss=0.18086]\n","Step 47207   [1.492 sec/step, loss=0.16788, avg_loss=0.18054]\n","Step 47208   [1.496 sec/step, loss=0.17566, avg_loss=0.18069]\n","Step 47209   [1.496 sec/step, loss=0.19421, avg_loss=0.18072]\n","Step 47210   [1.496 sec/step, loss=0.17642, avg_loss=0.18061]\n","Step 47211   [1.497 sec/step, loss=0.17748, avg_loss=0.18081]\n","Step 47212   [1.499 sec/step, loss=0.16498, avg_loss=0.18060]\n","Step 47213   [1.501 sec/step, loss=0.19140, avg_loss=0.18071]\n","Step 47214   [1.497 sec/step, loss=0.15689, avg_loss=0.18037]\n","Step 47215   [1.499 sec/step, loss=0.16798, avg_loss=0.18044]\n","Step 47216   [1.502 sec/step, loss=0.18486, avg_loss=0.18056]\n","Step 47217   [1.502 sec/step, loss=0.19514, avg_loss=0.18060]\n","Step 47218   [1.508 sec/step, loss=0.19289, avg_loss=0.18067]\n","Step 47219   [1.511 sec/step, loss=0.18425, avg_loss=0.18050]\n","Step 47220   [1.515 sec/step, loss=0.18588, avg_loss=0.18041]\n","Step 47221   [1.523 sec/step, loss=0.18886, avg_loss=0.18049]\n","Step 47222   [1.530 sec/step, loss=0.18787, avg_loss=0.18067]\n","Generated 32 batches of size 32 in 10.644 sec\n","Step 47223   [1.526 sec/step, loss=0.19038, avg_loss=0.18063]\n","Step 47224   [1.516 sec/step, loss=0.15864, avg_loss=0.18033]\n","Step 47225   [1.517 sec/step, loss=0.18823, avg_loss=0.18050]\n","Step 47226   [1.514 sec/step, loss=0.18756, avg_loss=0.18069]\n","Step 47227   [1.510 sec/step, loss=0.18755, avg_loss=0.18065]\n","Step 47228   [1.503 sec/step, loss=0.17711, avg_loss=0.18045]\n","Step 47229   [1.509 sec/step, loss=0.18821, avg_loss=0.18072]\n","Step 47230   [1.509 sec/step, loss=0.18509, avg_loss=0.18095]\n","Step 47231   [1.508 sec/step, loss=0.19525, avg_loss=0.18099]\n","Step 47232   [1.511 sec/step, loss=0.18478, avg_loss=0.18101]\n","Step 47233   [1.508 sec/step, loss=0.15851, avg_loss=0.18094]\n","Step 47234   [1.513 sec/step, loss=0.19524, avg_loss=0.18119]\n","Step 47235   [1.513 sec/step, loss=0.19047, avg_loss=0.18115]\n","Step 47236   [1.513 sec/step, loss=0.18674, avg_loss=0.18127]\n","Step 47237   [1.511 sec/step, loss=0.15774, avg_loss=0.18118]\n","Step 47238   [1.510 sec/step, loss=0.18810, avg_loss=0.18119]\n","Step 47239   [1.511 sec/step, loss=0.17238, avg_loss=0.18127]\n","Step 47240   [1.515 sec/step, loss=0.18271, avg_loss=0.18154]\n","Step 47241   [1.518 sec/step, loss=0.17869, avg_loss=0.18179]\n","Step 47242   [1.520 sec/step, loss=0.19192, avg_loss=0.18192]\n","Step 47243   [1.520 sec/step, loss=0.18943, avg_loss=0.18190]\n","Step 47244   [1.513 sec/step, loss=0.16203, avg_loss=0.18152]\n","Step 47245   [1.514 sec/step, loss=0.18600, avg_loss=0.18151]\n","Step 47246   [1.514 sec/step, loss=0.19252, avg_loss=0.18153]\n","Step 47247   [1.516 sec/step, loss=0.16007, avg_loss=0.18145]\n","Step 47248   [1.517 sec/step, loss=0.17040, avg_loss=0.18154]\n","Step 47249   [1.515 sec/step, loss=0.18568, avg_loss=0.18150]\n","Step 47250   [1.521 sec/step, loss=0.18994, avg_loss=0.18160]\n","Step 47251   [1.518 sec/step, loss=0.15797, avg_loss=0.18130]\n","Step 47252   [1.522 sec/step, loss=0.17496, avg_loss=0.18132]\n","Step 47253   [1.531 sec/step, loss=0.20073, avg_loss=0.18162]\n","Step 47254   [1.528 sec/step, loss=0.15605, avg_loss=0.18126]\n","Step 47255   [1.523 sec/step, loss=0.18476, avg_loss=0.18112]\n","Generated 32 batches of size 32 in 10.560 sec\n","Step 47256   [1.521 sec/step, loss=0.19204, avg_loss=0.18116]\n","Step 47257   [1.521 sec/step, loss=0.16505, avg_loss=0.18104]\n","Step 47258   [1.519 sec/step, loss=0.18773, avg_loss=0.18106]\n","Step 47259   [1.514 sec/step, loss=0.19009, avg_loss=0.18112]\n","Step 47260   [1.516 sec/step, loss=0.19833, avg_loss=0.18138]\n","Step 47261   [1.510 sec/step, loss=0.16627, avg_loss=0.18111]\n","Step 47262   [1.510 sec/step, loss=0.18942, avg_loss=0.18108]\n","Step 47263   [1.506 sec/step, loss=0.16147, avg_loss=0.18079]\n","Step 47264   [1.506 sec/step, loss=0.19141, avg_loss=0.18079]\n","Step 47265   [1.505 sec/step, loss=0.17095, avg_loss=0.18061]\n","Step 47266   [1.507 sec/step, loss=0.16745, avg_loss=0.18068]\n","Step 47267   [1.507 sec/step, loss=0.19237, avg_loss=0.18069]\n","Step 47268   [1.507 sec/step, loss=0.19399, avg_loss=0.18068]\n","Step 47269   [1.509 sec/step, loss=0.19026, avg_loss=0.18081]\n","Step 47270   [1.507 sec/step, loss=0.18303, avg_loss=0.18077]\n","Step 47271   [1.505 sec/step, loss=0.17915, avg_loss=0.18072]\n","Step 47272   [1.503 sec/step, loss=0.17555, avg_loss=0.18092]\n","Step 47273   [1.502 sec/step, loss=0.16154, avg_loss=0.18093]\n","Step 47274   [1.507 sec/step, loss=0.20183, avg_loss=0.18139]\n","Step 47275   [1.507 sec/step, loss=0.19165, avg_loss=0.18155]\n","Step 47276   [1.508 sec/step, loss=0.19680, avg_loss=0.18158]\n","Step 47277   [1.508 sec/step, loss=0.19633, avg_loss=0.18173]\n","Step 47278   [1.501 sec/step, loss=0.16564, avg_loss=0.18136]\n","Step 47279   [1.502 sec/step, loss=0.18719, avg_loss=0.18161]\n","Step 47280   [1.500 sec/step, loss=0.16931, avg_loss=0.18142]\n","Step 47281   [1.500 sec/step, loss=0.19078, avg_loss=0.18137]\n","Step 47282   [1.505 sec/step, loss=0.19994, avg_loss=0.18145]\n","Step 47283   [1.512 sec/step, loss=0.18749, avg_loss=0.18155]\n","Step 47284   [1.515 sec/step, loss=0.16450, avg_loss=0.18151]\n","Step 47285   [1.519 sec/step, loss=0.19185, avg_loss=0.18148]\n","Step 47286   [1.528 sec/step, loss=0.19596, avg_loss=0.18169]\n","Generated 32 batches of size 32 in 10.488 sec\n","Step 47287   [1.525 sec/step, loss=0.15570, avg_loss=0.18154]\n","Step 47288   [1.529 sec/step, loss=0.19298, avg_loss=0.18191]\n","Step 47289   [1.523 sec/step, loss=0.19247, avg_loss=0.18189]\n","Step 47290   [1.520 sec/step, loss=0.17285, avg_loss=0.18191]\n","Step 47291   [1.515 sec/step, loss=0.19083, avg_loss=0.18186]\n","Step 47292   [1.510 sec/step, loss=0.16795, avg_loss=0.18188]\n","Step 47293   [1.509 sec/step, loss=0.19372, avg_loss=0.18187]\n","Step 47294   [1.502 sec/step, loss=0.16081, avg_loss=0.18156]\n","Step 47295   [1.500 sec/step, loss=0.19752, avg_loss=0.18164]\n","Step 47296   [1.499 sec/step, loss=0.18822, avg_loss=0.18163]\n","Step 47297   [1.498 sec/step, loss=0.18648, avg_loss=0.18158]\n","Step 47298   [1.496 sec/step, loss=0.16070, avg_loss=0.18142]\n","Step 47299   [1.498 sec/step, loss=0.16732, avg_loss=0.18135]\n","Step 47300   [1.495 sec/step, loss=0.17821, avg_loss=0.18118]\n","Writing summary at step: 47300\n","Step 47301   [1.499 sec/step, loss=0.19288, avg_loss=0.18143]\n","Step 47302   [1.505 sec/step, loss=0.19448, avg_loss=0.18170]\n","Step 47303   [1.505 sec/step, loss=0.15889, avg_loss=0.18170]\n","Step 47304   [1.500 sec/step, loss=0.17208, avg_loss=0.18141]\n","Step 47305   [1.498 sec/step, loss=0.16855, avg_loss=0.18129]\n","Step 47306   [1.497 sec/step, loss=0.19007, avg_loss=0.18127]\n","Step 47307   [1.497 sec/step, loss=0.15552, avg_loss=0.18114]\n","Step 47308   [1.499 sec/step, loss=0.18864, avg_loss=0.18127]\n","Step 47309   [1.500 sec/step, loss=0.19232, avg_loss=0.18125]\n","Step 47310   [1.500 sec/step, loss=0.18408, avg_loss=0.18133]\n","Step 47311   [1.501 sec/step, loss=0.17984, avg_loss=0.18135]\n","Step 47312   [1.504 sec/step, loss=0.19704, avg_loss=0.18167]\n","Step 47313   [1.502 sec/step, loss=0.15857, avg_loss=0.18135]\n","Step 47314   [1.509 sec/step, loss=0.18626, avg_loss=0.18164]\n","Step 47315   [1.519 sec/step, loss=0.19263, avg_loss=0.18189]\n","Step 47316   [1.523 sec/step, loss=0.19488, avg_loss=0.18199]\n","Step 47317   [1.527 sec/step, loss=0.18895, avg_loss=0.18192]\n","Generated 32 batches of size 32 in 10.505 sec\n","Step 47318   [1.522 sec/step, loss=0.18860, avg_loss=0.18188]\n","Step 47319   [1.518 sec/step, loss=0.19034, avg_loss=0.18194]\n","Step 47320   [1.509 sec/step, loss=0.19007, avg_loss=0.18198]\n","Step 47321   [1.505 sec/step, loss=0.19182, avg_loss=0.18201]\n","Step 47322   [1.496 sec/step, loss=0.17329, avg_loss=0.18187]\n","Step 47323   [1.488 sec/step, loss=0.16002, avg_loss=0.18156]\n","Step 47324   [1.491 sec/step, loss=0.16622, avg_loss=0.18164]\n","Step 47325   [1.487 sec/step, loss=0.17000, avg_loss=0.18146]\n","Step 47326   [1.490 sec/step, loss=0.19133, avg_loss=0.18150]\n","Step 47327   [1.490 sec/step, loss=0.19065, avg_loss=0.18153]\n","Step 47328   [1.490 sec/step, loss=0.17702, avg_loss=0.18153]\n","Step 47329   [1.488 sec/step, loss=0.17521, avg_loss=0.18140]\n","Step 47330   [1.486 sec/step, loss=0.16089, avg_loss=0.18115]\n","Step 47331   [1.487 sec/step, loss=0.19191, avg_loss=0.18112]\n","Step 47332   [1.486 sec/step, loss=0.19005, avg_loss=0.18117]\n","Step 47333   [1.492 sec/step, loss=0.18650, avg_loss=0.18145]\n","Step 47334   [1.491 sec/step, loss=0.19006, avg_loss=0.18140]\n","Step 47335   [1.489 sec/step, loss=0.17995, avg_loss=0.18130]\n","Step 47336   [1.487 sec/step, loss=0.16547, avg_loss=0.18108]\n","Step 47337   [1.493 sec/step, loss=0.18907, avg_loss=0.18140]\n","Step 47338   [1.493 sec/step, loss=0.19245, avg_loss=0.18144]\n","Step 47339   [1.492 sec/step, loss=0.15737, avg_loss=0.18129]\n","Step 47340   [1.492 sec/step, loss=0.19029, avg_loss=0.18137]\n","Step 47341   [1.489 sec/step, loss=0.15249, avg_loss=0.18110]\n","Step 47342   [1.489 sec/step, loss=0.18849, avg_loss=0.18107]\n","Step 47343   [1.489 sec/step, loss=0.18968, avg_loss=0.18107]\n","Step 47344   [1.491 sec/step, loss=0.17260, avg_loss=0.18118]\n","Step 47345   [1.496 sec/step, loss=0.18872, avg_loss=0.18120]\n","Step 47346   [1.497 sec/step, loss=0.17642, avg_loss=0.18104]\n","Step 47347   [1.502 sec/step, loss=0.17438, avg_loss=0.18119]\n","Step 47348   [1.511 sec/step, loss=0.18922, avg_loss=0.18138]\n","Step 47349   [1.512 sec/step, loss=0.16974, avg_loss=0.18122]\n","Generated 32 batches of size 32 in 10.612 sec\n","Step 47350   [1.511 sec/step, loss=0.19159, avg_loss=0.18123]\n","Step 47351   [1.508 sec/step, loss=0.16104, avg_loss=0.18126]\n","Step 47352   [1.507 sec/step, loss=0.19050, avg_loss=0.18142]\n","Step 47353   [1.501 sec/step, loss=0.18864, avg_loss=0.18130]\n","Step 47354   [1.503 sec/step, loss=0.16870, avg_loss=0.18142]\n","Step 47355   [1.503 sec/step, loss=0.18454, avg_loss=0.18142]\n","Step 47356   [1.501 sec/step, loss=0.18502, avg_loss=0.18135]\n","Step 47357   [1.504 sec/step, loss=0.18938, avg_loss=0.18159]\n","Step 47358   [1.499 sec/step, loss=0.16710, avg_loss=0.18139]\n","Step 47359   [1.497 sec/step, loss=0.18037, avg_loss=0.18129]\n","Step 47360   [1.492 sec/step, loss=0.16543, avg_loss=0.18096]\n","Step 47361   [1.497 sec/step, loss=0.18798, avg_loss=0.18118]\n","Step 47362   [1.491 sec/step, loss=0.15790, avg_loss=0.18086]\n","Step 47363   [1.494 sec/step, loss=0.15309, avg_loss=0.18078]\n","Step 47364   [1.495 sec/step, loss=0.19024, avg_loss=0.18077]\n","Step 47365   [1.496 sec/step, loss=0.17391, avg_loss=0.18080]\n","Step 47366   [1.499 sec/step, loss=0.18991, avg_loss=0.18102]\n","Step 47367   [1.495 sec/step, loss=0.17348, avg_loss=0.18083]\n","Step 47368   [1.491 sec/step, loss=0.17320, avg_loss=0.18063]\n","Step 47369   [1.492 sec/step, loss=0.18936, avg_loss=0.18062]\n","Step 47370   [1.495 sec/step, loss=0.18937, avg_loss=0.18068]\n","Step 47371   [1.497 sec/step, loss=0.18808, avg_loss=0.18077]\n","Step 47372   [1.499 sec/step, loss=0.18817, avg_loss=0.18090]\n","Step 47373   [1.499 sec/step, loss=0.15954, avg_loss=0.18088]\n","Step 47374   [1.500 sec/step, loss=0.18927, avg_loss=0.18075]\n","Step 47375   [1.498 sec/step, loss=0.15390, avg_loss=0.18037]\n","Step 47376   [1.492 sec/step, loss=0.15329, avg_loss=0.17994]\n","Step 47377   [1.491 sec/step, loss=0.15267, avg_loss=0.17950]\n","Step 47378   [1.497 sec/step, loss=0.17061, avg_loss=0.17955]\n","Step 47379   [1.504 sec/step, loss=0.18317, avg_loss=0.17951]\n","Step 47380   [1.510 sec/step, loss=0.19326, avg_loss=0.17975]\n","Step 47381   [1.516 sec/step, loss=0.19589, avg_loss=0.17980]\n","Generated 32 batches of size 32 in 10.616 sec\n","Step 47382   [1.512 sec/step, loss=0.17909, avg_loss=0.17959]\n","Step 47383   [1.504 sec/step, loss=0.16338, avg_loss=0.17935]\n","Step 47384   [1.507 sec/step, loss=0.18776, avg_loss=0.17958]\n","Step 47385   [1.502 sec/step, loss=0.18852, avg_loss=0.17955]\n","Step 47386   [1.497 sec/step, loss=0.18278, avg_loss=0.17942]\n","Step 47387   [1.501 sec/step, loss=0.19253, avg_loss=0.17979]\n","Step 47388   [1.501 sec/step, loss=0.19003, avg_loss=0.17976]\n","Step 47389   [1.497 sec/step, loss=0.17045, avg_loss=0.17954]\n","Step 47390   [1.499 sec/step, loss=0.17702, avg_loss=0.17958]\n","Step 47391   [1.497 sec/step, loss=0.17439, avg_loss=0.17941]\n","Step 47392   [1.501 sec/step, loss=0.19133, avg_loss=0.17965]\n","Step 47393   [1.500 sec/step, loss=0.19592, avg_loss=0.17967]\n","Step 47394   [1.506 sec/step, loss=0.18725, avg_loss=0.17993]\n","Step 47395   [1.505 sec/step, loss=0.16032, avg_loss=0.17956]\n","Step 47396   [1.505 sec/step, loss=0.18952, avg_loss=0.17958]\n","Step 47397   [1.505 sec/step, loss=0.18272, avg_loss=0.17954]\n","Step 47398   [1.505 sec/step, loss=0.16782, avg_loss=0.17961]\n","Step 47399   [1.505 sec/step, loss=0.18524, avg_loss=0.17979]\n","Step 47400   [1.508 sec/step, loss=0.18325, avg_loss=0.17984]\n","Writing summary at step: 47400\n","Step 47401   [1.502 sec/step, loss=0.16081, avg_loss=0.17952]\n","Step 47402   [1.503 sec/step, loss=0.18932, avg_loss=0.17947]\n","Step 47403   [1.509 sec/step, loss=0.19103, avg_loss=0.17979]\n","Step 47404   [1.507 sec/step, loss=0.15435, avg_loss=0.17961]\n","Step 47405   [1.506 sec/step, loss=0.15310, avg_loss=0.17946]\n","Step 47406   [1.507 sec/step, loss=0.18662, avg_loss=0.17942]\n","Step 47407   [1.507 sec/step, loss=0.15188, avg_loss=0.17939]\n","Step 47408   [1.508 sec/step, loss=0.16764, avg_loss=0.17918]\n","Step 47409   [1.511 sec/step, loss=0.18804, avg_loss=0.17913]\n","Step 47410   [1.515 sec/step, loss=0.17457, avg_loss=0.17904]\n","Step 47411   [1.522 sec/step, loss=0.18390, avg_loss=0.17908]\n","Step 47412   [1.524 sec/step, loss=0.17219, avg_loss=0.17883]\n","Generated 32 batches of size 32 in 10.533 sec\n","Step 47413   [1.525 sec/step, loss=0.16783, avg_loss=0.17892]\n","Step 47414   [1.521 sec/step, loss=0.18323, avg_loss=0.17889]\n","Step 47415   [1.512 sec/step, loss=0.16472, avg_loss=0.17861]\n","Step 47416   [1.501 sec/step, loss=0.16119, avg_loss=0.17828]\n","Step 47417   [1.491 sec/step, loss=0.15575, avg_loss=0.17794]\n","Step 47418   [1.491 sec/step, loss=0.19007, avg_loss=0.17796]\n","Step 47419   [1.488 sec/step, loss=0.18044, avg_loss=0.17786]\n","Step 47420   [1.492 sec/step, loss=0.18769, avg_loss=0.17784]\n","Step 47421   [1.487 sec/step, loss=0.17249, avg_loss=0.17764]\n","Step 47422   [1.487 sec/step, loss=0.16801, avg_loss=0.17759]\n","Step 47423   [1.492 sec/step, loss=0.18773, avg_loss=0.17787]\n","Step 47424   [1.491 sec/step, loss=0.16746, avg_loss=0.17788]\n","Step 47425   [1.495 sec/step, loss=0.19383, avg_loss=0.17812]\n","Step 47426   [1.491 sec/step, loss=0.17859, avg_loss=0.17799]\n","Step 47427   [1.485 sec/step, loss=0.17119, avg_loss=0.17780]\n","Step 47428   [1.487 sec/step, loss=0.18927, avg_loss=0.17792]\n","Step 47429   [1.489 sec/step, loss=0.18878, avg_loss=0.17805]\n","Step 47430   [1.495 sec/step, loss=0.19949, avg_loss=0.17844]\n","Step 47431   [1.493 sec/step, loss=0.18364, avg_loss=0.17836]\n","Step 47432   [1.494 sec/step, loss=0.18978, avg_loss=0.17835]\n","Step 47433   [1.494 sec/step, loss=0.18399, avg_loss=0.17833]\n","Step 47434   [1.490 sec/step, loss=0.17596, avg_loss=0.17819]\n","Step 47435   [1.489 sec/step, loss=0.17673, avg_loss=0.17816]\n","Step 47436   [1.494 sec/step, loss=0.19399, avg_loss=0.17844]\n","Step 47437   [1.495 sec/step, loss=0.19155, avg_loss=0.17847]\n","Step 47438   [1.495 sec/step, loss=0.18922, avg_loss=0.17843]\n","Step 47439   [1.495 sec/step, loss=0.15946, avg_loss=0.17845]\n","Step 47440   [1.492 sec/step, loss=0.15530, avg_loss=0.17810]\n","Step 47441   [1.503 sec/step, loss=0.18981, avg_loss=0.17848]\n","Step 47442   [1.509 sec/step, loss=0.19846, avg_loss=0.17858]\n","Step 47443   [1.508 sec/step, loss=0.16061, avg_loss=0.17829]\n","Step 47444   [1.513 sec/step, loss=0.17882, avg_loss=0.17835]\n","Generated 32 batches of size 32 in 10.577 sec\n","Step 47445   [1.509 sec/step, loss=0.17521, avg_loss=0.17821]\n","Step 47446   [1.503 sec/step, loss=0.17001, avg_loss=0.17815]\n","Step 47447   [1.499 sec/step, loss=0.16272, avg_loss=0.17803]\n","Step 47448   [1.494 sec/step, loss=0.18930, avg_loss=0.17803]\n","Step 47449   [1.495 sec/step, loss=0.18556, avg_loss=0.17819]\n","Step 47450   [1.492 sec/step, loss=0.19028, avg_loss=0.17818]\n","Step 47451   [1.495 sec/step, loss=0.18538, avg_loss=0.17842]\n","Step 47452   [1.492 sec/step, loss=0.18080, avg_loss=0.17833]\n","Step 47453   [1.492 sec/step, loss=0.19107, avg_loss=0.17835]\n","Step 47454   [1.492 sec/step, loss=0.17365, avg_loss=0.17840]\n","Step 47455   [1.492 sec/step, loss=0.18776, avg_loss=0.17843]\n","Step 47456   [1.489 sec/step, loss=0.16546, avg_loss=0.17824]\n","Step 47457   [1.483 sec/step, loss=0.16072, avg_loss=0.17795]\n","Step 47458   [1.481 sec/step, loss=0.15570, avg_loss=0.17784]\n","Step 47459   [1.477 sec/step, loss=0.15257, avg_loss=0.17756]\n","Step 47460   [1.477 sec/step, loss=0.17065, avg_loss=0.17761]\n","Step 47461   [1.478 sec/step, loss=0.20206, avg_loss=0.17775]\n","Step 47462   [1.484 sec/step, loss=0.19240, avg_loss=0.17810]\n","Step 47463   [1.484 sec/step, loss=0.19151, avg_loss=0.17848]\n","Step 47464   [1.485 sec/step, loss=0.19136, avg_loss=0.17849]\n","Step 47465   [1.484 sec/step, loss=0.17789, avg_loss=0.17853]\n","Step 47466   [1.480 sec/step, loss=0.16560, avg_loss=0.17829]\n","Step 47467   [1.485 sec/step, loss=0.19248, avg_loss=0.17848]\n","Step 47468   [1.489 sec/step, loss=0.19471, avg_loss=0.17869]\n","Step 47469   [1.489 sec/step, loss=0.18594, avg_loss=0.17866]\n","Step 47470   [1.482 sec/step, loss=0.17340, avg_loss=0.17850]\n","Step 47471   [1.482 sec/step, loss=0.19247, avg_loss=0.17854]\n","Step 47472   [1.481 sec/step, loss=0.18019, avg_loss=0.17846]\n","Step 47473   [1.484 sec/step, loss=0.15763, avg_loss=0.17844]\n","Step 47474   [1.489 sec/step, loss=0.19219, avg_loss=0.17847]\n","Step 47475   [1.500 sec/step, loss=0.19524, avg_loss=0.17889]\n","Step 47476   [1.508 sec/step, loss=0.19058, avg_loss=0.17926]\n","Generated 32 batches of size 32 in 10.622 sec\n","Step 47477   [1.515 sec/step, loss=0.19240, avg_loss=0.17966]\n","Step 47478   [1.516 sec/step, loss=0.18850, avg_loss=0.17983]\n","Step 47479   [1.508 sec/step, loss=0.17170, avg_loss=0.17972]\n","Step 47480   [1.501 sec/step, loss=0.17678, avg_loss=0.17956]\n","Step 47481   [1.496 sec/step, loss=0.18625, avg_loss=0.17946]\n","Step 47482   [1.492 sec/step, loss=0.16220, avg_loss=0.17929]\n","Step 47483   [1.490 sec/step, loss=0.16853, avg_loss=0.17934]\n","Step 47484   [1.485 sec/step, loss=0.16077, avg_loss=0.17907]\n","Step 47485   [1.486 sec/step, loss=0.18506, avg_loss=0.17904]\n","Step 47486   [1.486 sec/step, loss=0.19542, avg_loss=0.17916]\n","Step 47487   [1.479 sec/step, loss=0.15693, avg_loss=0.17881]\n","Step 47488   [1.480 sec/step, loss=0.19373, avg_loss=0.17884]\n","Step 47489   [1.482 sec/step, loss=0.18326, avg_loss=0.17897]\n","Step 47490   [1.483 sec/step, loss=0.15771, avg_loss=0.17878]\n","Step 47491   [1.483 sec/step, loss=0.17535, avg_loss=0.17879]\n","Step 47492   [1.484 sec/step, loss=0.19442, avg_loss=0.17882]\n","Step 47493   [1.484 sec/step, loss=0.19129, avg_loss=0.17877]\n","Step 47494   [1.478 sec/step, loss=0.16570, avg_loss=0.17856]\n","Step 47495   [1.482 sec/step, loss=0.19306, avg_loss=0.17889]\n","Step 47496   [1.482 sec/step, loss=0.19305, avg_loss=0.17892]\n","Step 47497   [1.482 sec/step, loss=0.18856, avg_loss=0.17898]\n","Step 47498   [1.484 sec/step, loss=0.19669, avg_loss=0.17927]\n","Step 47499   [1.487 sec/step, loss=0.19283, avg_loss=0.17934]\n","Step 47500   [1.483 sec/step, loss=0.17702, avg_loss=0.17928]\n","Writing summary at step: 47500\n","Saving audio and alignment...\n","Generated 8 batches of size 2 in 0.000 sec\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/train-step-000047500-align000.png\n","100% 1/1 [00:03<00:00,  3.80s/it]\n","Test finished for step 47500.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000047500-align000.png\n"," 50% 1/2 [00:11<00:11, 11.90s/it]Training korean : Use jamo\n","/content/drive/My Drive/AI대학원/tacotron2/utils/plot.py:28: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n","  fig, ax = plt.subplots(figsize=(char_len/5, 5))\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000047500-align001.png\n","100% 2/2 [00:23<00:00, 11.93s/it]\n","Test finished for step 47500.\n","Step 47501   [1.489 sec/step, loss=0.19253, avg_loss=0.17960]\n","Step 47502   [1.488 sec/step, loss=0.16869, avg_loss=0.17939]\n","Step 47503   [1.487 sec/step, loss=0.15814, avg_loss=0.17906]\n","Step 47504   [1.492 sec/step, loss=0.16787, avg_loss=0.17920]\n","Step 47505   [1.495 sec/step, loss=0.15699, avg_loss=0.17924]\n","Step 47506   [1.499 sec/step, loss=0.18791, avg_loss=0.17925]\n","Step 47507   [1.507 sec/step, loss=0.18259, avg_loss=0.17956]\n","Generated 32 batches of size 32 in 10.683 sec\n","Step 47508   [1.509 sec/step, loss=0.19411, avg_loss=0.17982]\n","Step 47509   [1.505 sec/step, loss=0.19226, avg_loss=0.17986]\n","Step 47510   [1.500 sec/step, loss=0.17931, avg_loss=0.17991]\n","Step 47511   [1.496 sec/step, loss=0.19296, avg_loss=0.18000]\n","Step 47512   [1.489 sec/step, loss=0.16065, avg_loss=0.17989]\n","Step 47513   [1.492 sec/step, loss=0.18871, avg_loss=0.18010]\n","Step 47514   [1.489 sec/step, loss=0.17043, avg_loss=0.17997]\n","Step 47515   [1.493 sec/step, loss=0.18821, avg_loss=0.18020]\n","Step 47516   [1.500 sec/step, loss=0.19030, avg_loss=0.18049]\n","Step 47517   [1.505 sec/step, loss=0.18399, avg_loss=0.18078]\n","Step 47518   [1.504 sec/step, loss=0.16352, avg_loss=0.18051]\n","Step 47519   [1.501 sec/step, loss=0.15940, avg_loss=0.18030]\n","Step 47520   [1.501 sec/step, loss=0.18433, avg_loss=0.18027]\n","Step 47521   [1.501 sec/step, loss=0.16587, avg_loss=0.18020]\n","Step 47522   [1.501 sec/step, loss=0.16388, avg_loss=0.18016]\n","Step 47523   [1.499 sec/step, loss=0.17857, avg_loss=0.18007]\n","Step 47524   [1.503 sec/step, loss=0.19985, avg_loss=0.18039]\n","Step 47525   [1.503 sec/step, loss=0.19371, avg_loss=0.18039]\n","Step 47526   [1.500 sec/step, loss=0.15632, avg_loss=0.18017]\n","Step 47527   [1.506 sec/step, loss=0.19360, avg_loss=0.18039]\n","Step 47528   [1.506 sec/step, loss=0.19366, avg_loss=0.18044]\n","Step 47529   [1.507 sec/step, loss=0.18847, avg_loss=0.18043]\n","Step 47530   [1.507 sec/step, loss=0.18818, avg_loss=0.18032]\n","Step 47531   [1.502 sec/step, loss=0.15702, avg_loss=0.18005]\n","Step 47532   [1.498 sec/step, loss=0.17691, avg_loss=0.17992]\n","Step 47533   [1.498 sec/step, loss=0.19043, avg_loss=0.17999]\n","Step 47534   [1.503 sec/step, loss=0.17978, avg_loss=0.18003]\n","Step 47535   [1.505 sec/step, loss=0.15493, avg_loss=0.17981]\n","Step 47536   [1.506 sec/step, loss=0.17489, avg_loss=0.17962]\n","Step 47537   [1.509 sec/step, loss=0.18673, avg_loss=0.17957]\n","Step 47538   [1.513 sec/step, loss=0.19094, avg_loss=0.17959]\n","Generated 32 batches of size 32 in 10.640 sec\n","Step 47539   [1.520 sec/step, loss=0.17508, avg_loss=0.17974]\n","Step 47540   [1.517 sec/step, loss=0.15492, avg_loss=0.17974]\n","Step 47541   [1.512 sec/step, loss=0.18335, avg_loss=0.17968]\n","Step 47542   [1.504 sec/step, loss=0.17382, avg_loss=0.17943]\n","Step 47543   [1.501 sec/step, loss=0.17038, avg_loss=0.17953]\n","Step 47544   [1.500 sec/step, loss=0.18194, avg_loss=0.17956]\n","Step 47545   [1.498 sec/step, loss=0.18836, avg_loss=0.17969]\n","Step 47546   [1.498 sec/step, loss=0.15269, avg_loss=0.17952]\n","Step 47547   [1.495 sec/step, loss=0.15066, avg_loss=0.17940]\n","Step 47548   [1.495 sec/step, loss=0.20460, avg_loss=0.17955]\n","Step 47549   [1.490 sec/step, loss=0.15111, avg_loss=0.17920]\n","Step 47550   [1.490 sec/step, loss=0.19316, avg_loss=0.17923]\n","Step 47551   [1.488 sec/step, loss=0.17026, avg_loss=0.17908]\n","Step 47552   [1.491 sec/step, loss=0.18959, avg_loss=0.17917]\n","Step 47553   [1.492 sec/step, loss=0.18820, avg_loss=0.17914]\n","Step 47554   [1.494 sec/step, loss=0.18542, avg_loss=0.17926]\n","Step 47555   [1.490 sec/step, loss=0.16693, avg_loss=0.17905]\n","Step 47556   [1.487 sec/step, loss=0.15905, avg_loss=0.17899]\n","Step 47557   [1.492 sec/step, loss=0.16190, avg_loss=0.17900]\n","Step 47558   [1.498 sec/step, loss=0.18640, avg_loss=0.17931]\n","Step 47559   [1.502 sec/step, loss=0.17835, avg_loss=0.17956]\n","Step 47560   [1.506 sec/step, loss=0.20022, avg_loss=0.17986]\n","Step 47561   [1.503 sec/step, loss=0.17377, avg_loss=0.17958]\n","Step 47562   [1.501 sec/step, loss=0.17243, avg_loss=0.17938]\n","Step 47563   [1.501 sec/step, loss=0.18972, avg_loss=0.17936]\n","Step 47564   [1.501 sec/step, loss=0.18541, avg_loss=0.17930]\n","Step 47565   [1.505 sec/step, loss=0.19052, avg_loss=0.17943]\n","Step 47566   [1.509 sec/step, loss=0.15736, avg_loss=0.17934]\n","Step 47567   [1.506 sec/step, loss=0.15422, avg_loss=0.17896]\n","Step 47568   [1.510 sec/step, loss=0.18981, avg_loss=0.17891]\n","Step 47569   [1.512 sec/step, loss=0.18441, avg_loss=0.17890]\n","Step 47570   [1.519 sec/step, loss=0.17943, avg_loss=0.17896]\n","Step 47571   [1.524 sec/step, loss=0.19098, avg_loss=0.17894]\n","Generated 32 batches of size 32 in 10.591 sec\n","Step 47572   [1.526 sec/step, loss=0.18640, avg_loss=0.17900]\n","Step 47573   [1.524 sec/step, loss=0.16956, avg_loss=0.17912]\n","Step 47574   [1.520 sec/step, loss=0.18418, avg_loss=0.17904]\n","Step 47575   [1.512 sec/step, loss=0.18196, avg_loss=0.17891]\n","Step 47576   [1.506 sec/step, loss=0.16567, avg_loss=0.17866]\n","Step 47577   [1.500 sec/step, loss=0.17562, avg_loss=0.17849]\n","Step 47578   [1.497 sec/step, loss=0.17199, avg_loss=0.17833]\n","Step 47579   [1.500 sec/step, loss=0.19086, avg_loss=0.17852]\n","Step 47580   [1.503 sec/step, loss=0.19335, avg_loss=0.17868]\n","Step 47581   [1.502 sec/step, loss=0.19102, avg_loss=0.17873]\n","Step 47582   [1.498 sec/step, loss=0.16519, avg_loss=0.17876]\n","Step 47583   [1.504 sec/step, loss=0.18979, avg_loss=0.17898]\n","Step 47584   [1.504 sec/step, loss=0.15453, avg_loss=0.17891]\n","Step 47585   [1.504 sec/step, loss=0.18914, avg_loss=0.17895]\n","Step 47586   [1.504 sec/step, loss=0.18577, avg_loss=0.17886]\n","Step 47587   [1.505 sec/step, loss=0.17478, avg_loss=0.17904]\n","Step 47588   [1.499 sec/step, loss=0.16217, avg_loss=0.17872]\n","Step 47589   [1.501 sec/step, loss=0.19292, avg_loss=0.17882]\n","Step 47590   [1.501 sec/step, loss=0.18633, avg_loss=0.17910]\n","Step 47591   [1.501 sec/step, loss=0.17010, avg_loss=0.17905]\n","Step 47592   [1.497 sec/step, loss=0.15517, avg_loss=0.17866]\n","Step 47593   [1.496 sec/step, loss=0.18229, avg_loss=0.17857]\n","Step 47594   [1.502 sec/step, loss=0.18150, avg_loss=0.17873]\n","Step 47595   [1.502 sec/step, loss=0.18059, avg_loss=0.17860]\n","Step 47596   [1.498 sec/step, loss=0.16686, avg_loss=0.17834]\n","Step 47597   [1.499 sec/step, loss=0.19020, avg_loss=0.17836]\n","Step 47598   [1.499 sec/step, loss=0.15500, avg_loss=0.17794]\n","Step 47599   [1.503 sec/step, loss=0.18722, avg_loss=0.17788]\n","Step 47600   [1.511 sec/step, loss=0.19739, avg_loss=0.17809]\n","Writing summary at step: 47600\n","Step 47601   [1.512 sec/step, loss=0.18754, avg_loss=0.17804]\n","Generated 32 batches of size 32 in 10.459 sec\n","Step 47602   [1.511 sec/step, loss=0.16695, avg_loss=0.17802]\n","Step 47603   [1.506 sec/step, loss=0.15351, avg_loss=0.17797]\n","Step 47604   [1.507 sec/step, loss=0.18824, avg_loss=0.17818]\n","Step 47605   [1.507 sec/step, loss=0.17446, avg_loss=0.17835]\n","Step 47606   [1.503 sec/step, loss=0.19080, avg_loss=0.17838]\n","Step 47607   [1.500 sec/step, loss=0.15295, avg_loss=0.17808]\n","Step 47608   [1.495 sec/step, loss=0.17218, avg_loss=0.17786]\n","Step 47609   [1.494 sec/step, loss=0.18723, avg_loss=0.17781]\n","Step 47610   [1.499 sec/step, loss=0.18899, avg_loss=0.17791]\n","Step 47611   [1.494 sec/step, loss=0.16750, avg_loss=0.17766]\n","Step 47612   [1.497 sec/step, loss=0.16183, avg_loss=0.17767]\n","Step 47613   [1.496 sec/step, loss=0.18790, avg_loss=0.17766]\n","Step 47614   [1.500 sec/step, loss=0.18915, avg_loss=0.17785]\n","Step 47615   [1.494 sec/step, loss=0.15624, avg_loss=0.17753]\n","Step 47616   [1.493 sec/step, loss=0.18613, avg_loss=0.17749]\n","Step 47617   [1.491 sec/step, loss=0.17800, avg_loss=0.17743]\n","Step 47618   [1.493 sec/step, loss=0.18968, avg_loss=0.17769]\n","Step 47619   [1.499 sec/step, loss=0.18397, avg_loss=0.17793]\n","Step 47620   [1.499 sec/step, loss=0.18700, avg_loss=0.17796]\n","Step 47621   [1.499 sec/step, loss=0.17143, avg_loss=0.17802]\n","Step 47622   [1.503 sec/step, loss=0.18208, avg_loss=0.17820]\n","Step 47623   [1.501 sec/step, loss=0.16502, avg_loss=0.17806]\n","Step 47624   [1.496 sec/step, loss=0.15677, avg_loss=0.17763]\n","Step 47625   [1.495 sec/step, loss=0.19004, avg_loss=0.17759]\n","Step 47626   [1.501 sec/step, loss=0.18989, avg_loss=0.17793]\n","Step 47627   [1.500 sec/step, loss=0.18635, avg_loss=0.17786]\n","Step 47628   [1.494 sec/step, loss=0.15378, avg_loss=0.17746]\n","Step 47629   [1.496 sec/step, loss=0.17652, avg_loss=0.17734]\n","Step 47630   [1.493 sec/step, loss=0.15150, avg_loss=0.17697]\n","Step 47631   [1.498 sec/step, loss=0.16271, avg_loss=0.17703]\n","Step 47632   [1.498 sec/step, loss=0.15051, avg_loss=0.17677]\n","Step 47633   [1.503 sec/step, loss=0.18790, avg_loss=0.17674]\n","Step 47634   [1.506 sec/step, loss=0.18420, avg_loss=0.17678]\n","Generated 32 batches of size 32 in 10.591 sec\n","Step 47635   [1.508 sec/step, loss=0.19148, avg_loss=0.17715]\n","Step 47636   [1.503 sec/step, loss=0.16790, avg_loss=0.17708]\n","Step 47637   [1.497 sec/step, loss=0.17541, avg_loss=0.17697]\n","Step 47638   [1.492 sec/step, loss=0.18556, avg_loss=0.17691]\n","Step 47639   [1.488 sec/step, loss=0.17230, avg_loss=0.17689]\n","Step 47640   [1.494 sec/step, loss=0.18883, avg_loss=0.17722]\n","Step 47641   [1.494 sec/step, loss=0.19845, avg_loss=0.17738]\n","Step 47642   [1.496 sec/step, loss=0.18324, avg_loss=0.17747]\n","Step 47643   [1.498 sec/step, loss=0.17367, avg_loss=0.17750]\n","Step 47644   [1.493 sec/step, loss=0.17158, avg_loss=0.17740]\n","Step 47645   [1.493 sec/step, loss=0.18802, avg_loss=0.17740]\n","Step 47646   [1.497 sec/step, loss=0.18165, avg_loss=0.17769]\n","Step 47647   [1.500 sec/step, loss=0.16019, avg_loss=0.17778]\n","Step 47648   [1.500 sec/step, loss=0.15138, avg_loss=0.17725]\n","Step 47649   [1.500 sec/step, loss=0.16309, avg_loss=0.17737]\n","Step 47650   [1.497 sec/step, loss=0.17065, avg_loss=0.17714]\n","Step 47651   [1.501 sec/step, loss=0.18946, avg_loss=0.17734]\n","Step 47652   [1.497 sec/step, loss=0.16782, avg_loss=0.17712]\n","Step 47653   [1.495 sec/step, loss=0.17098, avg_loss=0.17695]\n","Step 47654   [1.495 sec/step, loss=0.19244, avg_loss=0.17702]\n","Step 47655   [1.500 sec/step, loss=0.18866, avg_loss=0.17723]\n","Step 47656   [1.500 sec/step, loss=0.15458, avg_loss=0.17719]\n","Step 47657   [1.501 sec/step, loss=0.18453, avg_loss=0.17741]\n","Step 47658   [1.501 sec/step, loss=0.18189, avg_loss=0.17737]\n","Step 47659   [1.505 sec/step, loss=0.19041, avg_loss=0.17749]\n","Step 47660   [1.505 sec/step, loss=0.18898, avg_loss=0.17738]\n","Step 47661   [1.504 sec/step, loss=0.15446, avg_loss=0.17718]\n","Step 47662   [1.506 sec/step, loss=0.16935, avg_loss=0.17715]\n","Step 47663   [1.505 sec/step, loss=0.15526, avg_loss=0.17681]\n","Step 47664   [1.509 sec/step, loss=0.18698, avg_loss=0.17682]\n","Step 47665   [1.512 sec/step, loss=0.18758, avg_loss=0.17680]\n","Step 47666   [1.511 sec/step, loss=0.16851, avg_loss=0.17691]\n","Generated 32 batches of size 32 in 10.364 sec\n","Step 47667   [1.510 sec/step, loss=0.15766, avg_loss=0.17694]\n","Step 47668   [1.505 sec/step, loss=0.19744, avg_loss=0.17702]\n","Step 47669   [1.498 sec/step, loss=0.15148, avg_loss=0.17669]\n","Step 47670   [1.496 sec/step, loss=0.19263, avg_loss=0.17682]\n","Step 47671   [1.486 sec/step, loss=0.15248, avg_loss=0.17643]\n","Step 47672   [1.481 sec/step, loss=0.17083, avg_loss=0.17628]\n","Step 47673   [1.482 sec/step, loss=0.17898, avg_loss=0.17637]\n","Step 47674   [1.477 sec/step, loss=0.15505, avg_loss=0.17608]\n","Step 47675   [1.477 sec/step, loss=0.17232, avg_loss=0.17599]\n","Step 47676   [1.481 sec/step, loss=0.18993, avg_loss=0.17623]\n","Step 47677   [1.483 sec/step, loss=0.18898, avg_loss=0.17636]\n","Step 47678   [1.485 sec/step, loss=0.19233, avg_loss=0.17657]\n","Step 47679   [1.480 sec/step, loss=0.15633, avg_loss=0.17622]\n","Step 47680   [1.479 sec/step, loss=0.18724, avg_loss=0.17616]\n","Step 47681   [1.480 sec/step, loss=0.18707, avg_loss=0.17612]\n","Step 47682   [1.486 sec/step, loss=0.18735, avg_loss=0.17634]\n","Step 47683   [1.485 sec/step, loss=0.18599, avg_loss=0.17630]\n","Step 47684   [1.490 sec/step, loss=0.18357, avg_loss=0.17659]\n","Step 47685   [1.491 sec/step, loss=0.18982, avg_loss=0.17660]\n","Step 47686   [1.488 sec/step, loss=0.18585, avg_loss=0.17660]\n","Step 47687   [1.493 sec/step, loss=0.19262, avg_loss=0.17678]\n","Step 47688   [1.494 sec/step, loss=0.17241, avg_loss=0.17688]\n","Step 47689   [1.494 sec/step, loss=0.18949, avg_loss=0.17685]\n","Step 47690   [1.494 sec/step, loss=0.18987, avg_loss=0.17688]\n","Step 47691   [1.496 sec/step, loss=0.18644, avg_loss=0.17705]\n","Step 47692   [1.498 sec/step, loss=0.17512, avg_loss=0.17725]\n","Step 47693   [1.499 sec/step, loss=0.17336, avg_loss=0.17716]\n","Step 47694   [1.504 sec/step, loss=0.19102, avg_loss=0.17725]\n","Step 47695   [1.509 sec/step, loss=0.18919, avg_loss=0.17734]\n","Step 47696   [1.509 sec/step, loss=0.16060, avg_loss=0.17728]\n","Step 47697   [1.508 sec/step, loss=0.17546, avg_loss=0.17713]\n","Generated 32 batches of size 32 in 10.242 sec\n","Step 47698   [1.512 sec/step, loss=0.16531, avg_loss=0.17723]\n","Step 47699   [1.503 sec/step, loss=0.16677, avg_loss=0.17703]\n","Step 47700   [1.495 sec/step, loss=0.15407, avg_loss=0.17659]\n","Writing summary at step: 47700\n","Step 47701   [1.492 sec/step, loss=0.18254, avg_loss=0.17654]\n","Step 47702   [1.490 sec/step, loss=0.17709, avg_loss=0.17664]\n","Step 47703   [1.496 sec/step, loss=0.19288, avg_loss=0.17704]\n","Step 47704   [1.497 sec/step, loss=0.19934, avg_loss=0.17715]\n","Step 47705   [1.500 sec/step, loss=0.19145, avg_loss=0.17732]\n","Step 47706   [1.497 sec/step, loss=0.18251, avg_loss=0.17724]\n","Step 47707   [1.497 sec/step, loss=0.18859, avg_loss=0.17759]\n","Step 47708   [1.499 sec/step, loss=0.18738, avg_loss=0.17774]\n","Step 47709   [1.500 sec/step, loss=0.20476, avg_loss=0.17792]\n","Step 47710   [1.493 sec/step, loss=0.16096, avg_loss=0.17764]\n","Step 47711   [1.498 sec/step, loss=0.19032, avg_loss=0.17787]\n","Step 47712   [1.495 sec/step, loss=0.15530, avg_loss=0.17780]\n","Step 47713   [1.494 sec/step, loss=0.18689, avg_loss=0.17779]\n","Step 47714   [1.494 sec/step, loss=0.18428, avg_loss=0.17774]\n","Step 47715   [1.495 sec/step, loss=0.19367, avg_loss=0.17812]\n","Step 47716   [1.491 sec/step, loss=0.18332, avg_loss=0.17809]\n","Step 47717   [1.490 sec/step, loss=0.17281, avg_loss=0.17804]\n","Step 47718   [1.490 sec/step, loss=0.19626, avg_loss=0.17810]\n","Step 47719   [1.491 sec/step, loss=0.16426, avg_loss=0.17791]\n","Step 47720   [1.489 sec/step, loss=0.18260, avg_loss=0.17786]\n","Step 47721   [1.493 sec/step, loss=0.18983, avg_loss=0.17805]\n","Step 47722   [1.487 sec/step, loss=0.17413, avg_loss=0.17797]\n","Step 47723   [1.491 sec/step, loss=0.20588, avg_loss=0.17838]\n","Step 47724   [1.495 sec/step, loss=0.16373, avg_loss=0.17845]\n","Step 47725   [1.500 sec/step, loss=0.19354, avg_loss=0.17848]\n","Step 47726   [1.505 sec/step, loss=0.20329, avg_loss=0.17861]\n","Step 47727   [1.509 sec/step, loss=0.19298, avg_loss=0.17868]\n","Step 47728   [1.514 sec/step, loss=0.19319, avg_loss=0.17907]\n","Generated 32 batches of size 32 in 10.424 sec\n","Step 47729   [1.515 sec/step, loss=0.19113, avg_loss=0.17922]\n","Step 47730   [1.512 sec/step, loss=0.16671, avg_loss=0.17937]\n","Step 47731   [1.511 sec/step, loss=0.19159, avg_loss=0.17966]\n","Step 47732   [1.510 sec/step, loss=0.19465, avg_loss=0.18010]\n","Step 47733   [1.502 sec/step, loss=0.18353, avg_loss=0.18006]\n","Step 47734   [1.498 sec/step, loss=0.19723, avg_loss=0.18019]\n","Step 47735   [1.490 sec/step, loss=0.16201, avg_loss=0.17989]\n","Step 47736   [1.489 sec/step, loss=0.15905, avg_loss=0.17981]\n","Step 47737   [1.492 sec/step, loss=0.19655, avg_loss=0.18002]\n","Step 47738   [1.490 sec/step, loss=0.18069, avg_loss=0.17997]\n","Step 47739   [1.492 sec/step, loss=0.19601, avg_loss=0.18021]\n","Step 47740   [1.492 sec/step, loss=0.19813, avg_loss=0.18030]\n","Step 47741   [1.493 sec/step, loss=0.19073, avg_loss=0.18022]\n","Step 47742   [1.489 sec/step, loss=0.18745, avg_loss=0.18026]\n","Step 47743   [1.486 sec/step, loss=0.16990, avg_loss=0.18023]\n","Step 47744   [1.490 sec/step, loss=0.18952, avg_loss=0.18041]\n","Step 47745   [1.485 sec/step, loss=0.16057, avg_loss=0.18013]\n","Step 47746   [1.486 sec/step, loss=0.18773, avg_loss=0.18019]\n","Step 47747   [1.486 sec/step, loss=0.18221, avg_loss=0.18041]\n","Step 47748   [1.485 sec/step, loss=0.17876, avg_loss=0.18069]\n","Step 47749   [1.491 sec/step, loss=0.18605, avg_loss=0.18092]\n","Step 47750   [1.489 sec/step, loss=0.17821, avg_loss=0.18099]\n","Step 47751   [1.485 sec/step, loss=0.16158, avg_loss=0.18071]\n","Step 47752   [1.484 sec/step, loss=0.16122, avg_loss=0.18065]\n","Step 47753   [1.486 sec/step, loss=0.19305, avg_loss=0.18087]\n","Step 47754   [1.483 sec/step, loss=0.18525, avg_loss=0.18080]\n","Step 47755   [1.480 sec/step, loss=0.18157, avg_loss=0.18072]\n","Step 47756   [1.487 sec/step, loss=0.17624, avg_loss=0.18094]\n","Step 47757   [1.493 sec/step, loss=0.19651, avg_loss=0.18106]\n","Step 47758   [1.498 sec/step, loss=0.19080, avg_loss=0.18115]\n","Step 47759   [1.501 sec/step, loss=0.19480, avg_loss=0.18119]\n","Step 47760   [1.506 sec/step, loss=0.19157, avg_loss=0.18122]\n","Generated 32 batches of size 32 in 10.732 sec\n","Step 47761   [1.511 sec/step, loss=0.19204, avg_loss=0.18160]\n","Step 47762   [1.511 sec/step, loss=0.18688, avg_loss=0.18177]\n","Step 47763   [1.511 sec/step, loss=0.19065, avg_loss=0.18212]\n","Step 47764   [1.503 sec/step, loss=0.17988, avg_loss=0.18205]\n","Step 47765   [1.499 sec/step, loss=0.18819, avg_loss=0.18206]\n","Step 47766   [1.496 sec/step, loss=0.17138, avg_loss=0.18209]\n","Step 47767   [1.500 sec/step, loss=0.18575, avg_loss=0.18237]\n","Step 47768   [1.494 sec/step, loss=0.16327, avg_loss=0.18203]\n","Step 47769   [1.500 sec/step, loss=0.18315, avg_loss=0.18234]\n","Step 47770   [1.500 sec/step, loss=0.18220, avg_loss=0.18224]\n","Step 47771   [1.504 sec/step, loss=0.20883, avg_loss=0.18280]\n","Step 47772   [1.505 sec/step, loss=0.17530, avg_loss=0.18285]\n","Step 47773   [1.505 sec/step, loss=0.19105, avg_loss=0.18297]\n","Step 47774   [1.506 sec/step, loss=0.17072, avg_loss=0.18313]\n","Step 47775   [1.509 sec/step, loss=0.19557, avg_loss=0.18336]\n","Step 47776   [1.509 sec/step, loss=0.19307, avg_loss=0.18339]\n","Step 47777   [1.504 sec/step, loss=0.15878, avg_loss=0.18309]\n","Step 47778   [1.502 sec/step, loss=0.16461, avg_loss=0.18281]\n","Step 47779   [1.502 sec/step, loss=0.15835, avg_loss=0.18283]\n","Step 47780   [1.499 sec/step, loss=0.16827, avg_loss=0.18264]\n","Step 47781   [1.498 sec/step, loss=0.19200, avg_loss=0.18269]\n","Step 47782   [1.498 sec/step, loss=0.20427, avg_loss=0.18286]\n","Step 47783   [1.499 sec/step, loss=0.19882, avg_loss=0.18299]\n","Step 47784   [1.500 sec/step, loss=0.19231, avg_loss=0.18308]\n","Step 47785   [1.494 sec/step, loss=0.16175, avg_loss=0.18279]\n","Step 47786   [1.495 sec/step, loss=0.18959, avg_loss=0.18283]\n","Step 47787   [1.496 sec/step, loss=0.19447, avg_loss=0.18285]\n","Step 47788   [1.505 sec/step, loss=0.20089, avg_loss=0.18314]\n","Step 47789   [1.502 sec/step, loss=0.15598, avg_loss=0.18280]\n","Step 47790   [1.506 sec/step, loss=0.18954, avg_loss=0.18280]\n","Step 47791   [1.512 sec/step, loss=0.18648, avg_loss=0.18280]\n","Step 47792   [1.516 sec/step, loss=0.17902, avg_loss=0.18284]\n","Generated 32 batches of size 32 in 10.605 sec\n","Step 47793   [1.516 sec/step, loss=0.18815, avg_loss=0.18298]\n","Step 47794   [1.509 sec/step, loss=0.17510, avg_loss=0.18282]\n","Step 47795   [1.499 sec/step, loss=0.15738, avg_loss=0.18251]\n","Step 47796   [1.502 sec/step, loss=0.19745, avg_loss=0.18288]\n","Step 47797   [1.503 sec/step, loss=0.19423, avg_loss=0.18306]\n","Step 47798   [1.500 sec/step, loss=0.17725, avg_loss=0.18318]\n","Step 47799   [1.505 sec/step, loss=0.18825, avg_loss=0.18340]\n","Step 47800   [1.503 sec/step, loss=0.15428, avg_loss=0.18340]\n","Writing summary at step: 47800\n","Step 47801   [1.505 sec/step, loss=0.19181, avg_loss=0.18349]\n","Step 47802   [1.504 sec/step, loss=0.16239, avg_loss=0.18334]\n","Step 47803   [1.498 sec/step, loss=0.15189, avg_loss=0.18294]\n","Step 47804   [1.497 sec/step, loss=0.18742, avg_loss=0.18282]\n","Step 47805   [1.497 sec/step, loss=0.19786, avg_loss=0.18288]\n","Step 47806   [1.494 sec/step, loss=0.15069, avg_loss=0.18256]\n","Step 47807   [1.492 sec/step, loss=0.17651, avg_loss=0.18244]\n","Step 47808   [1.492 sec/step, loss=0.19038, avg_loss=0.18247]\n","Step 47809   [1.488 sec/step, loss=0.16970, avg_loss=0.18212]\n","Step 47810   [1.489 sec/step, loss=0.16773, avg_loss=0.18219]\n","Step 47811   [1.487 sec/step, loss=0.17242, avg_loss=0.18201]\n","Step 47812   [1.486 sec/step, loss=0.15355, avg_loss=0.18199]\n","Step 47813   [1.486 sec/step, loss=0.18861, avg_loss=0.18201]\n","Step 47814   [1.481 sec/step, loss=0.14894, avg_loss=0.18166]\n","Step 47815   [1.485 sec/step, loss=0.19216, avg_loss=0.18164]\n","Step 47816   [1.484 sec/step, loss=0.17365, avg_loss=0.18154]\n","Step 47817   [1.489 sec/step, loss=0.18514, avg_loss=0.18167]\n","Step 47818   [1.486 sec/step, loss=0.16228, avg_loss=0.18133]\n","Step 47819   [1.487 sec/step, loss=0.17373, avg_loss=0.18142]\n","Step 47820   [1.493 sec/step, loss=0.18769, avg_loss=0.18147]\n","Step 47821   [1.494 sec/step, loss=0.17074, avg_loss=0.18128]\n","Step 47822   [1.504 sec/step, loss=0.18540, avg_loss=0.18139]\n","Step 47823   [1.504 sec/step, loss=0.16778, avg_loss=0.18101]\n","Generated 32 batches of size 32 in 10.402 sec\n","Step 47824   [1.509 sec/step, loss=0.18720, avg_loss=0.18125]\n","Step 47825   [1.504 sec/step, loss=0.18448, avg_loss=0.18116]\n","Step 47826   [1.499 sec/step, loss=0.18390, avg_loss=0.18096]\n","Step 47827   [1.496 sec/step, loss=0.19172, avg_loss=0.18095]\n","Step 47828   [1.493 sec/step, loss=0.15635, avg_loss=0.18058]\n","Step 47829   [1.487 sec/step, loss=0.17670, avg_loss=0.18044]\n","Step 47830   [1.493 sec/step, loss=0.18763, avg_loss=0.18065]\n","Step 47831   [1.491 sec/step, loss=0.16404, avg_loss=0.18037]\n","Step 47832   [1.491 sec/step, loss=0.16276, avg_loss=0.18005]\n","Step 47833   [1.495 sec/step, loss=0.18551, avg_loss=0.18007]\n","Step 47834   [1.492 sec/step, loss=0.17130, avg_loss=0.17981]\n","Step 47835   [1.492 sec/step, loss=0.15666, avg_loss=0.17976]\n","Step 47836   [1.492 sec/step, loss=0.15234, avg_loss=0.17969]\n","Step 47837   [1.485 sec/step, loss=0.14949, avg_loss=0.17922]\n","Step 47838   [1.488 sec/step, loss=0.18862, avg_loss=0.17930]\n","Step 47839   [1.489 sec/step, loss=0.19057, avg_loss=0.17925]\n","Step 47840   [1.489 sec/step, loss=0.18371, avg_loss=0.17910]\n","Step 47841   [1.484 sec/step, loss=0.17430, avg_loss=0.17894]\n","Step 47842   [1.488 sec/step, loss=0.18704, avg_loss=0.17893]\n","Step 47843   [1.491 sec/step, loss=0.17488, avg_loss=0.17898]\n","Step 47844   [1.491 sec/step, loss=0.19231, avg_loss=0.17901]\n","Step 47845   [1.497 sec/step, loss=0.18719, avg_loss=0.17928]\n","Step 47846   [1.497 sec/step, loss=0.18491, avg_loss=0.17925]\n","Step 47847   [1.494 sec/step, loss=0.17406, avg_loss=0.17917]\n","Step 47848   [1.494 sec/step, loss=0.17251, avg_loss=0.17911]\n","Step 47849   [1.491 sec/step, loss=0.16195, avg_loss=0.17887]\n","Step 47850   [1.489 sec/step, loss=0.15698, avg_loss=0.17865]\n","Step 47851   [1.498 sec/step, loss=0.19195, avg_loss=0.17896]\n","Step 47852   [1.509 sec/step, loss=0.18993, avg_loss=0.17924]\n","Step 47853   [1.514 sec/step, loss=0.18772, avg_loss=0.17919]\n","Step 47854   [1.521 sec/step, loss=0.19133, avg_loss=0.17925]\n","Step 47855   [1.523 sec/step, loss=0.16744, avg_loss=0.17911]\n","Generated 32 batches of size 32 in 10.668 sec\n","Step 47856   [1.523 sec/step, loss=0.15489, avg_loss=0.17890]\n","Step 47857   [1.518 sec/step, loss=0.18368, avg_loss=0.17877]\n","Step 47858   [1.513 sec/step, loss=0.18224, avg_loss=0.17868]\n","Step 47859   [1.509 sec/step, loss=0.19684, avg_loss=0.17870]\n","Step 47860   [1.505 sec/step, loss=0.19276, avg_loss=0.17871]\n","Step 47861   [1.502 sec/step, loss=0.19713, avg_loss=0.17877]\n","Step 47862   [1.504 sec/step, loss=0.18772, avg_loss=0.17877]\n","Step 47863   [1.505 sec/step, loss=0.19001, avg_loss=0.17877]\n","Step 47864   [1.503 sec/step, loss=0.17176, avg_loss=0.17869]\n","Step 47865   [1.503 sec/step, loss=0.18860, avg_loss=0.17869]\n","Step 47866   [1.507 sec/step, loss=0.19180, avg_loss=0.17889]\n","Step 47867   [1.505 sec/step, loss=0.17682, avg_loss=0.17881]\n","Step 47868   [1.508 sec/step, loss=0.17441, avg_loss=0.17892]\n","Step 47869   [1.505 sec/step, loss=0.17209, avg_loss=0.17881]\n","Step 47870   [1.499 sec/step, loss=0.15951, avg_loss=0.17858]\n","Step 47871   [1.496 sec/step, loss=0.15481, avg_loss=0.17804]\n","Step 47872   [1.500 sec/step, loss=0.18967, avg_loss=0.17818]\n","Step 47873   [1.502 sec/step, loss=0.19282, avg_loss=0.17820]\n","Step 47874   [1.504 sec/step, loss=0.18539, avg_loss=0.17835]\n","Step 47875   [1.505 sec/step, loss=0.18828, avg_loss=0.17827]\n","Step 47876   [1.505 sec/step, loss=0.19119, avg_loss=0.17826]\n","Step 47877   [1.504 sec/step, loss=0.17199, avg_loss=0.17839]\n","Step 47878   [1.503 sec/step, loss=0.17507, avg_loss=0.17849]\n","Step 47879   [1.505 sec/step, loss=0.17075, avg_loss=0.17862]\n","Step 47880   [1.505 sec/step, loss=0.16832, avg_loss=0.17862]\n","Step 47881   [1.505 sec/step, loss=0.18834, avg_loss=0.17858]\n","Step 47882   [1.504 sec/step, loss=0.19518, avg_loss=0.17849]\n","Step 47883   [1.508 sec/step, loss=0.19122, avg_loss=0.17841]\n","Step 47884   [1.508 sec/step, loss=0.18152, avg_loss=0.17831]\n","Step 47885   [1.517 sec/step, loss=0.18864, avg_loss=0.17857]\n","Step 47886   [1.519 sec/step, loss=0.18870, avg_loss=0.17857]\n","Step 47887   [1.522 sec/step, loss=0.18618, avg_loss=0.17848]\n","Generated 32 batches of size 32 in 10.428 sec\n","Step 47888   [1.516 sec/step, loss=0.16205, avg_loss=0.17809]\n","Step 47889   [1.513 sec/step, loss=0.15916, avg_loss=0.17813]\n","Step 47890   [1.509 sec/step, loss=0.18802, avg_loss=0.17811]\n","Step 47891   [1.501 sec/step, loss=0.16462, avg_loss=0.17789]\n","Step 47892   [1.500 sec/step, loss=0.18421, avg_loss=0.17794]\n","Step 47893   [1.493 sec/step, loss=0.16786, avg_loss=0.17774]\n","Step 47894   [1.495 sec/step, loss=0.18246, avg_loss=0.17781]\n","Step 47895   [1.496 sec/step, loss=0.15829, avg_loss=0.17782]\n","Step 47896   [1.491 sec/step, loss=0.16623, avg_loss=0.17751]\n","Step 47897   [1.490 sec/step, loss=0.20268, avg_loss=0.17760]\n","Step 47898   [1.493 sec/step, loss=0.19053, avg_loss=0.17773]\n","Step 47899   [1.490 sec/step, loss=0.17631, avg_loss=0.17761]\n","Step 47900   [1.496 sec/step, loss=0.20698, avg_loss=0.17814]\n","Writing summary at step: 47900\n","Step 47901   [1.493 sec/step, loss=0.17307, avg_loss=0.17795]\n","Step 47902   [1.495 sec/step, loss=0.17246, avg_loss=0.17805]\n","Step 47903   [1.501 sec/step, loss=0.19377, avg_loss=0.17847]\n","Step 47904   [1.502 sec/step, loss=0.19610, avg_loss=0.17856]\n","Step 47905   [1.495 sec/step, loss=0.15904, avg_loss=0.17817]\n","Step 47906   [1.501 sec/step, loss=0.19058, avg_loss=0.17857]\n","Step 47907   [1.501 sec/step, loss=0.18003, avg_loss=0.17860]\n","Step 47908   [1.494 sec/step, loss=0.15283, avg_loss=0.17823]\n","Step 47909   [1.498 sec/step, loss=0.18811, avg_loss=0.17841]\n","Step 47910   [1.502 sec/step, loss=0.19281, avg_loss=0.17866]\n","Step 47911   [1.502 sec/step, loss=0.17378, avg_loss=0.17867]\n","Step 47912   [1.504 sec/step, loss=0.17269, avg_loss=0.17887]\n","Step 47913   [1.503 sec/step, loss=0.18791, avg_loss=0.17886]\n","Step 47914   [1.509 sec/step, loss=0.15536, avg_loss=0.17892]\n","Step 47915   [1.508 sec/step, loss=0.16445, avg_loss=0.17865]\n","Step 47916   [1.509 sec/step, loss=0.15695, avg_loss=0.17848]\n","Step 47917   [1.512 sec/step, loss=0.18318, avg_loss=0.17846]\n","Step 47918   [1.511 sec/step, loss=0.15146, avg_loss=0.17835]\n","Step 47919   [1.516 sec/step, loss=0.20133, avg_loss=0.17863]\n","Generated 32 batches of size 32 in 10.511 sec\n","Step 47920   [1.515 sec/step, loss=0.19286, avg_loss=0.17868]\n","Step 47921   [1.514 sec/step, loss=0.19262, avg_loss=0.17890]\n","Step 47922   [1.511 sec/step, loss=0.18309, avg_loss=0.17887]\n","Step 47923   [1.511 sec/step, loss=0.18772, avg_loss=0.17907]\n","Step 47924   [1.508 sec/step, loss=0.18845, avg_loss=0.17909]\n","Step 47925   [1.506 sec/step, loss=0.17780, avg_loss=0.17902]\n","Step 47926   [1.507 sec/step, loss=0.18819, avg_loss=0.17906]\n","Step 47927   [1.504 sec/step, loss=0.17241, avg_loss=0.17887]\n","Step 47928   [1.508 sec/step, loss=0.18699, avg_loss=0.17918]\n","Step 47929   [1.511 sec/step, loss=0.18370, avg_loss=0.17925]\n","Step 47930   [1.510 sec/step, loss=0.18188, avg_loss=0.17919]\n","Step 47931   [1.509 sec/step, loss=0.16956, avg_loss=0.17924]\n","Step 47932   [1.512 sec/step, loss=0.18992, avg_loss=0.17952]\n","Step 47933   [1.512 sec/step, loss=0.18195, avg_loss=0.17948]\n","Step 47934   [1.512 sec/step, loss=0.17235, avg_loss=0.17949]\n","Step 47935   [1.517 sec/step, loss=0.18086, avg_loss=0.17973]\n","Step 47936   [1.523 sec/step, loss=0.19072, avg_loss=0.18012]\n","Step 47937   [1.525 sec/step, loss=0.17205, avg_loss=0.18034]\n","Step 47938   [1.521 sec/step, loss=0.15540, avg_loss=0.18001]\n","Step 47939   [1.514 sec/step, loss=0.15456, avg_loss=0.17965]\n","Step 47940   [1.510 sec/step, loss=0.16545, avg_loss=0.17947]\n","Step 47941   [1.508 sec/step, loss=0.15088, avg_loss=0.17923]\n","Step 47942   [1.504 sec/step, loss=0.16618, avg_loss=0.17902]\n","Step 47943   [1.506 sec/step, loss=0.18737, avg_loss=0.17915]\n","Step 47944   [1.506 sec/step, loss=0.18520, avg_loss=0.17908]\n","Step 47945   [1.505 sec/step, loss=0.18976, avg_loss=0.17910]\n","Step 47946   [1.509 sec/step, loss=0.18777, avg_loss=0.17913]\n","Step 47947   [1.520 sec/step, loss=0.18676, avg_loss=0.17926]\n","Step 47948   [1.524 sec/step, loss=0.18205, avg_loss=0.17935]\n","Step 47949   [1.527 sec/step, loss=0.17784, avg_loss=0.17951]\n","Step 47950   [1.531 sec/step, loss=0.16312, avg_loss=0.17957]\n","Generated 32 batches of size 32 in 10.533 sec\n","Step 47951   [1.526 sec/step, loss=0.17252, avg_loss=0.17938]\n","Step 47952   [1.515 sec/step, loss=0.15423, avg_loss=0.17902]\n","Step 47953   [1.511 sec/step, loss=0.19151, avg_loss=0.17906]\n","Step 47954   [1.504 sec/step, loss=0.18427, avg_loss=0.17899]\n","Step 47955   [1.505 sec/step, loss=0.19046, avg_loss=0.17922]\n","Step 47956   [1.501 sec/step, loss=0.17837, avg_loss=0.17946]\n","Step 47957   [1.495 sec/step, loss=0.16159, avg_loss=0.17923]\n","Step 47958   [1.495 sec/step, loss=0.18837, avg_loss=0.17930]\n","Step 47959   [1.491 sec/step, loss=0.15838, avg_loss=0.17891]\n","Step 47960   [1.490 sec/step, loss=0.18634, avg_loss=0.17885]\n","Step 47961   [1.490 sec/step, loss=0.18594, avg_loss=0.17874]\n","Step 47962   [1.490 sec/step, loss=0.18497, avg_loss=0.17871]\n","Step 47963   [1.489 sec/step, loss=0.18716, avg_loss=0.17868]\n","Step 47964   [1.495 sec/step, loss=0.18939, avg_loss=0.17886]\n","Step 47965   [1.490 sec/step, loss=0.16596, avg_loss=0.17863]\n","Step 47966   [1.490 sec/step, loss=0.18350, avg_loss=0.17855]\n","Step 47967   [1.493 sec/step, loss=0.18641, avg_loss=0.17864]\n","Step 47968   [1.496 sec/step, loss=0.18812, avg_loss=0.17878]\n","Step 47969   [1.496 sec/step, loss=0.18232, avg_loss=0.17888]\n","Step 47970   [1.502 sec/step, loss=0.15671, avg_loss=0.17885]\n","Step 47971   [1.502 sec/step, loss=0.16137, avg_loss=0.17892]\n","Step 47972   [1.501 sec/step, loss=0.18797, avg_loss=0.17890]\n","Step 47973   [1.495 sec/step, loss=0.15152, avg_loss=0.17849]\n","Step 47974   [1.491 sec/step, loss=0.15057, avg_loss=0.17814]\n","Step 47975   [1.487 sec/step, loss=0.17065, avg_loss=0.17796]\n","Step 47976   [1.486 sec/step, loss=0.18698, avg_loss=0.17792]\n","Step 47977   [1.489 sec/step, loss=0.18447, avg_loss=0.17805]\n","Step 47978   [1.493 sec/step, loss=0.16525, avg_loss=0.17795]\n","Step 47979   [1.498 sec/step, loss=0.17781, avg_loss=0.17802]\n","Step 47980   [1.499 sec/step, loss=0.16120, avg_loss=0.17795]\n","Step 47981   [1.503 sec/step, loss=0.18834, avg_loss=0.17795]\n","Step 47982   [1.508 sec/step, loss=0.18506, avg_loss=0.17785]\n","Generated 32 batches of size 32 in 10.508 sec\n","Step 47983   [1.508 sec/step, loss=0.18327, avg_loss=0.17777]\n","Step 47984   [1.507 sec/step, loss=0.18148, avg_loss=0.17777]\n","Step 47985   [1.501 sec/step, loss=0.16644, avg_loss=0.17755]\n","Step 47986   [1.497 sec/step, loss=0.16880, avg_loss=0.17735]\n","Step 47987   [1.494 sec/step, loss=0.19102, avg_loss=0.17739]\n","Step 47988   [1.495 sec/step, loss=0.18714, avg_loss=0.17765]\n","Step 47989   [1.495 sec/step, loss=0.15445, avg_loss=0.17760]\n","Step 47990   [1.494 sec/step, loss=0.18064, avg_loss=0.17753]\n","Step 47991   [1.496 sec/step, loss=0.18794, avg_loss=0.17776]\n","Step 47992   [1.496 sec/step, loss=0.18463, avg_loss=0.17776]\n","Step 47993   [1.496 sec/step, loss=0.15857, avg_loss=0.17767]\n","Step 47994   [1.491 sec/step, loss=0.15050, avg_loss=0.17735]\n","Step 47995   [1.491 sec/step, loss=0.14902, avg_loss=0.17726]\n","Step 47996   [1.491 sec/step, loss=0.16805, avg_loss=0.17728]\n","Step 47997   [1.489 sec/step, loss=0.17406, avg_loss=0.17699]\n","Step 47998   [1.489 sec/step, loss=0.19246, avg_loss=0.17701]\n","Step 47999   [1.492 sec/step, loss=0.18975, avg_loss=0.17714]\n","Step 48000   [1.486 sec/step, loss=0.15791, avg_loss=0.17665]\n","Writing summary at step: 48000\n","Saving checkpoint to: logdir-tacotron2/son_2021-02-06_23-59-17/model.ckpt-48000\n","Saving audio and alignment...\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/train-step-000048000-align000.png\n","100% 1/1 [00:02<00:00,  2.87s/it]\n","Test finished for step 48000.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000048000-align000.png\n"," 50% 1/2 [00:12<00:12, 12.90s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000048000-align001.png\n","100% 2/2 [00:24<00:00, 12.33s/it]\n","Test finished for step 48000.\n","Step 48001   [1.489 sec/step, loss=0.18584, avg_loss=0.17678]\n","Step 48002   [1.489 sec/step, loss=0.16355, avg_loss=0.17669]\n","Step 48003   [1.486 sec/step, loss=0.17392, avg_loss=0.17649]\n","Step 48004   [1.481 sec/step, loss=0.16672, avg_loss=0.17620]\n","Step 48005   [1.487 sec/step, loss=0.18389, avg_loss=0.17645]\n","Step 48006   [1.488 sec/step, loss=0.19358, avg_loss=0.17648]\n","Step 48007   [1.491 sec/step, loss=0.19039, avg_loss=0.17658]\n","Step 48008   [1.501 sec/step, loss=0.18980, avg_loss=0.17695]\n","Step 48009   [1.505 sec/step, loss=0.18311, avg_loss=0.17690]\n","Step 48010   [1.506 sec/step, loss=0.17765, avg_loss=0.17675]\n","Step 48011   [1.506 sec/step, loss=0.15434, avg_loss=0.17655]\n","Step 48012   [1.515 sec/step, loss=0.18875, avg_loss=0.17671]\n","Generated 32 batches of size 32 in 10.711 sec\n","Step 48013   [1.517 sec/step, loss=0.17172, avg_loss=0.17655]\n","Step 48014   [1.513 sec/step, loss=0.17299, avg_loss=0.17673]\n","Step 48015   [1.514 sec/step, loss=0.18575, avg_loss=0.17694]\n","Step 48016   [1.517 sec/step, loss=0.18749, avg_loss=0.17725]\n","Step 48017   [1.510 sec/step, loss=0.17307, avg_loss=0.17715]\n","Step 48018   [1.509 sec/step, loss=0.16655, avg_loss=0.17730]\n","Step 48019   [1.504 sec/step, loss=0.18570, avg_loss=0.17714]\n","Step 48020   [1.501 sec/step, loss=0.18083, avg_loss=0.17702]\n","Step 48021   [1.500 sec/step, loss=0.17966, avg_loss=0.17689]\n","Step 48022   [1.496 sec/step, loss=0.16573, avg_loss=0.17672]\n","Step 48023   [1.493 sec/step, loss=0.17208, avg_loss=0.17656]\n","Step 48024   [1.488 sec/step, loss=0.15438, avg_loss=0.17622]\n","Step 48025   [1.488 sec/step, loss=0.15813, avg_loss=0.17602]\n","Step 48026   [1.485 sec/step, loss=0.16886, avg_loss=0.17583]\n","Step 48027   [1.484 sec/step, loss=0.16802, avg_loss=0.17579]\n","Step 48028   [1.479 sec/step, loss=0.15388, avg_loss=0.17546]\n","Step 48029   [1.479 sec/step, loss=0.20496, avg_loss=0.17567]\n","Step 48030   [1.474 sec/step, loss=0.14971, avg_loss=0.17535]\n","Step 48031   [1.474 sec/step, loss=0.14935, avg_loss=0.17514]\n","Step 48032   [1.474 sec/step, loss=0.18724, avg_loss=0.17512]\n","Step 48033   [1.475 sec/step, loss=0.19070, avg_loss=0.17520]\n","Step 48034   [1.478 sec/step, loss=0.18403, avg_loss=0.17532]\n","Step 48035   [1.476 sec/step, loss=0.17494, avg_loss=0.17526]\n","Step 48036   [1.476 sec/step, loss=0.18181, avg_loss=0.17517]\n","Step 48037   [1.481 sec/step, loss=0.19226, avg_loss=0.17538]\n","Step 48038   [1.485 sec/step, loss=0.19016, avg_loss=0.17572]\n","Step 48039   [1.490 sec/step, loss=0.18796, avg_loss=0.17606]\n","Step 48040   [1.499 sec/step, loss=0.19077, avg_loss=0.17631]\n","Step 48041   [1.509 sec/step, loss=0.18883, avg_loss=0.17669]\n","Step 48042   [1.511 sec/step, loss=0.15938, avg_loss=0.17662]\n","Step 48043   [1.516 sec/step, loss=0.19318, avg_loss=0.17668]\n","Step 48044   [1.513 sec/step, loss=0.15222, avg_loss=0.17635]\n","Generated 32 batches of size 32 in 10.473 sec\n","Step 48045   [1.517 sec/step, loss=0.18890, avg_loss=0.17634]\n","Step 48046   [1.509 sec/step, loss=0.17547, avg_loss=0.17622]\n","Step 48047   [1.500 sec/step, loss=0.17006, avg_loss=0.17605]\n","Step 48048   [1.498 sec/step, loss=0.18839, avg_loss=0.17611]\n","Step 48049   [1.498 sec/step, loss=0.19297, avg_loss=0.17627]\n","Step 48050   [1.500 sec/step, loss=0.19056, avg_loss=0.17654]\n","Step 48051   [1.495 sec/step, loss=0.16291, avg_loss=0.17644]\n","Step 48052   [1.501 sec/step, loss=0.18686, avg_loss=0.17677]\n","Step 48053   [1.500 sec/step, loss=0.18458, avg_loss=0.17670]\n","Step 48054   [1.502 sec/step, loss=0.18748, avg_loss=0.17673]\n","Step 48055   [1.498 sec/step, loss=0.18642, avg_loss=0.17669]\n","Step 48056   [1.496 sec/step, loss=0.16318, avg_loss=0.17654]\n","Step 48057   [1.498 sec/step, loss=0.17121, avg_loss=0.17664]\n","Step 48058   [1.492 sec/step, loss=0.15760, avg_loss=0.17633]\n","Step 48059   [1.492 sec/step, loss=0.16825, avg_loss=0.17643]\n","Step 48060   [1.492 sec/step, loss=0.19114, avg_loss=0.17648]\n","Step 48061   [1.491 sec/step, loss=0.19384, avg_loss=0.17656]\n","Step 48062   [1.488 sec/step, loss=0.17936, avg_loss=0.17650]\n","Step 48063   [1.488 sec/step, loss=0.18913, avg_loss=0.17652]\n","Step 48064   [1.487 sec/step, loss=0.18808, avg_loss=0.17651]\n","Step 48065   [1.486 sec/step, loss=0.16057, avg_loss=0.17645]\n","Step 48066   [1.482 sec/step, loss=0.18752, avg_loss=0.17649]\n","Step 48067   [1.479 sec/step, loss=0.17562, avg_loss=0.17638]\n","Step 48068   [1.479 sec/step, loss=0.18660, avg_loss=0.17637]\n","Step 48069   [1.481 sec/step, loss=0.18429, avg_loss=0.17639]\n","Step 48070   [1.481 sec/step, loss=0.19191, avg_loss=0.17674]\n","Step 48071   [1.482 sec/step, loss=0.15347, avg_loss=0.17666]\n","Step 48072   [1.486 sec/step, loss=0.18821, avg_loss=0.17666]\n","Step 48073   [1.494 sec/step, loss=0.17898, avg_loss=0.17694]\n","Step 48074   [1.504 sec/step, loss=0.18901, avg_loss=0.17732]\n","Step 48075   [1.509 sec/step, loss=0.17337, avg_loss=0.17735]\n","Step 48076   [1.506 sec/step, loss=0.15183, avg_loss=0.17700]\n","Generated 32 batches of size 32 in 10.518 sec\n","Step 48077   [1.513 sec/step, loss=0.18660, avg_loss=0.17702]\n","Step 48078   [1.511 sec/step, loss=0.17581, avg_loss=0.17713]\n","Step 48079   [1.509 sec/step, loss=0.18916, avg_loss=0.17724]\n","Step 48080   [1.509 sec/step, loss=0.16474, avg_loss=0.17727]\n","Step 48081   [1.506 sec/step, loss=0.19091, avg_loss=0.17730]\n","Step 48082   [1.495 sec/step, loss=0.15243, avg_loss=0.17697]\n","Step 48083   [1.492 sec/step, loss=0.18611, avg_loss=0.17700]\n","Step 48084   [1.487 sec/step, loss=0.16736, avg_loss=0.17686]\n","Step 48085   [1.487 sec/step, loss=0.17424, avg_loss=0.17694]\n","Step 48086   [1.489 sec/step, loss=0.17151, avg_loss=0.17697]\n","Step 48087   [1.489 sec/step, loss=0.18899, avg_loss=0.17695]\n","Step 48088   [1.483 sec/step, loss=0.15396, avg_loss=0.17661]\n","Step 48089   [1.488 sec/step, loss=0.18576, avg_loss=0.17693]\n","Step 48090   [1.488 sec/step, loss=0.17432, avg_loss=0.17686]\n","Step 48091   [1.488 sec/step, loss=0.19191, avg_loss=0.17690]\n","Step 48092   [1.482 sec/step, loss=0.15249, avg_loss=0.17658]\n","Step 48093   [1.486 sec/step, loss=0.18683, avg_loss=0.17687]\n","Step 48094   [1.487 sec/step, loss=0.16878, avg_loss=0.17705]\n","Step 48095   [1.493 sec/step, loss=0.18467, avg_loss=0.17740]\n","Step 48096   [1.494 sec/step, loss=0.15318, avg_loss=0.17726]\n","Step 48097   [1.496 sec/step, loss=0.18785, avg_loss=0.17739]\n","Step 48098   [1.496 sec/step, loss=0.18668, avg_loss=0.17734]\n","Step 48099   [1.493 sec/step, loss=0.17295, avg_loss=0.17717]\n","Step 48100   [1.496 sec/step, loss=0.17035, avg_loss=0.17729]\n","Writing summary at step: 48100\n","Step 48101   [1.492 sec/step, loss=0.16883, avg_loss=0.17712]\n","Step 48102   [1.494 sec/step, loss=0.18492, avg_loss=0.17734]\n","Step 48103   [1.501 sec/step, loss=0.18603, avg_loss=0.17746]\n","Step 48104   [1.503 sec/step, loss=0.15294, avg_loss=0.17732]\n","Step 48105   [1.505 sec/step, loss=0.16136, avg_loss=0.17709]\n","Step 48106   [1.509 sec/step, loss=0.18762, avg_loss=0.17703]\n","Step 48107   [1.508 sec/step, loss=0.16496, avg_loss=0.17678]\n","Step 48108   [1.509 sec/step, loss=0.18867, avg_loss=0.17677]\n","Generated 32 batches of size 32 in 11.151 sec\n","Step 48109   [1.501 sec/step, loss=0.15093, avg_loss=0.17645]\n","Step 48110   [1.500 sec/step, loss=0.18352, avg_loss=0.17651]\n","Step 48111   [1.503 sec/step, loss=0.18232, avg_loss=0.17679]\n","Step 48112   [1.493 sec/step, loss=0.14944, avg_loss=0.17639]\n","Step 48113   [1.492 sec/step, loss=0.18868, avg_loss=0.17656]\n","Step 48114   [1.495 sec/step, loss=0.18219, avg_loss=0.17665]\n","Step 48115   [1.495 sec/step, loss=0.18162, avg_loss=0.17661]\n","Step 48116   [1.493 sec/step, loss=0.17950, avg_loss=0.17653]\n","Step 48117   [1.490 sec/step, loss=0.15088, avg_loss=0.17631]\n","Step 48118   [1.494 sec/step, loss=0.18841, avg_loss=0.17653]\n","Step 48119   [1.494 sec/step, loss=0.18622, avg_loss=0.17653]\n","Step 48120   [1.491 sec/step, loss=0.17077, avg_loss=0.17643]\n","Step 48121   [1.486 sec/step, loss=0.14867, avg_loss=0.17612]\n","Step 48122   [1.485 sec/step, loss=0.14712, avg_loss=0.17594]\n","Step 48123   [1.484 sec/step, loss=0.17020, avg_loss=0.17592]\n","Step 48124   [1.489 sec/step, loss=0.19033, avg_loss=0.17628]\n","Step 48125   [1.489 sec/step, loss=0.16914, avg_loss=0.17639]\n","Step 48126   [1.491 sec/step, loss=0.18732, avg_loss=0.17657]\n","Step 48127   [1.493 sec/step, loss=0.19369, avg_loss=0.17683]\n","Step 48128   [1.498 sec/step, loss=0.18176, avg_loss=0.17711]\n","Step 48129   [1.495 sec/step, loss=0.17128, avg_loss=0.17677]\n","Step 48130   [1.501 sec/step, loss=0.18625, avg_loss=0.17714]\n","Step 48131   [1.504 sec/step, loss=0.16628, avg_loss=0.17731]\n","Step 48132   [1.500 sec/step, loss=0.16052, avg_loss=0.17704]\n","Step 48133   [1.496 sec/step, loss=0.16769, avg_loss=0.17681]\n","Step 48134   [1.495 sec/step, loss=0.19265, avg_loss=0.17690]\n","Step 48135   [1.495 sec/step, loss=0.15800, avg_loss=0.17673]\n","Step 48136   [1.500 sec/step, loss=0.19177, avg_loss=0.17683]\n","Step 48137   [1.505 sec/step, loss=0.19036, avg_loss=0.17681]\n","Step 48138   [1.510 sec/step, loss=0.18799, avg_loss=0.17679]\n","Step 48139   [1.509 sec/step, loss=0.17971, avg_loss=0.17670]\n","Generated 32 batches of size 32 in 10.574 sec\n","Step 48140   [1.504 sec/step, loss=0.17470, avg_loss=0.17654]\n","Step 48141   [1.501 sec/step, loss=0.19101, avg_loss=0.17656]\n","Step 48142   [1.499 sec/step, loss=0.16775, avg_loss=0.17665]\n","Step 48143   [1.495 sec/step, loss=0.18878, avg_loss=0.17660]\n","Step 48144   [1.495 sec/step, loss=0.17704, avg_loss=0.17685]\n","Step 48145   [1.492 sec/step, loss=0.18673, avg_loss=0.17683]\n","Step 48146   [1.496 sec/step, loss=0.18520, avg_loss=0.17693]\n","Step 48147   [1.498 sec/step, loss=0.16743, avg_loss=0.17690]\n","Step 48148   [1.498 sec/step, loss=0.18423, avg_loss=0.17686]\n","Step 48149   [1.492 sec/step, loss=0.16261, avg_loss=0.17656]\n","Step 48150   [1.492 sec/step, loss=0.19609, avg_loss=0.17661]\n","Step 48151   [1.499 sec/step, loss=0.19229, avg_loss=0.17690]\n","Step 48152   [1.498 sec/step, loss=0.19507, avg_loss=0.17699]\n","Step 48153   [1.499 sec/step, loss=0.18499, avg_loss=0.17699]\n","Step 48154   [1.495 sec/step, loss=0.17381, avg_loss=0.17685]\n","Step 48155   [1.493 sec/step, loss=0.15573, avg_loss=0.17655]\n","Step 48156   [1.495 sec/step, loss=0.17935, avg_loss=0.17671]\n","Step 48157   [1.495 sec/step, loss=0.15512, avg_loss=0.17655]\n","Step 48158   [1.501 sec/step, loss=0.19438, avg_loss=0.17692]\n","Step 48159   [1.506 sec/step, loss=0.19819, avg_loss=0.17722]\n","Step 48160   [1.506 sec/step, loss=0.18759, avg_loss=0.17718]\n","Step 48161   [1.506 sec/step, loss=0.18446, avg_loss=0.17709]\n","Step 48162   [1.505 sec/step, loss=0.17095, avg_loss=0.17700]\n","Step 48163   [1.499 sec/step, loss=0.15920, avg_loss=0.17670]\n","Step 48164   [1.500 sec/step, loss=0.18698, avg_loss=0.17669]\n","Step 48165   [1.507 sec/step, loss=0.19357, avg_loss=0.17702]\n","Step 48166   [1.505 sec/step, loss=0.15361, avg_loss=0.17668]\n","Step 48167   [1.511 sec/step, loss=0.18796, avg_loss=0.17681]\n","Step 48168   [1.511 sec/step, loss=0.16996, avg_loss=0.17664]\n","Step 48169   [1.515 sec/step, loss=0.18977, avg_loss=0.17669]\n","Step 48170   [1.517 sec/step, loss=0.18375, avg_loss=0.17661]\n","Step 48171   [1.529 sec/step, loss=0.19514, avg_loss=0.17703]\n","Generated 32 batches of size 32 in 10.657 sec\n","Step 48172   [1.523 sec/step, loss=0.16998, avg_loss=0.17685]\n","Step 48173   [1.515 sec/step, loss=0.15993, avg_loss=0.17666]\n","Step 48174   [1.509 sec/step, loss=0.17536, avg_loss=0.17652]\n","Step 48175   [1.505 sec/step, loss=0.17563, avg_loss=0.17654]\n","Step 48176   [1.509 sec/step, loss=0.18941, avg_loss=0.17692]\n","Step 48177   [1.505 sec/step, loss=0.18658, avg_loss=0.17692]\n","Step 48178   [1.505 sec/step, loss=0.17563, avg_loss=0.17692]\n","Step 48179   [1.505 sec/step, loss=0.18774, avg_loss=0.17690]\n","Step 48180   [1.505 sec/step, loss=0.17348, avg_loss=0.17699]\n","Step 48181   [1.505 sec/step, loss=0.18944, avg_loss=0.17698]\n","Step 48182   [1.508 sec/step, loss=0.16304, avg_loss=0.17708]\n","Step 48183   [1.509 sec/step, loss=0.18613, avg_loss=0.17708]\n","Step 48184   [1.513 sec/step, loss=0.18453, avg_loss=0.17725]\n","Step 48185   [1.513 sec/step, loss=0.17350, avg_loss=0.17725]\n","Step 48186   [1.509 sec/step, loss=0.15582, avg_loss=0.17709]\n","Step 48187   [1.509 sec/step, loss=0.18880, avg_loss=0.17709]\n","Step 48188   [1.515 sec/step, loss=0.18142, avg_loss=0.17736]\n","Step 48189   [1.516 sec/step, loss=0.19337, avg_loss=0.17744]\n","Step 48190   [1.512 sec/step, loss=0.15192, avg_loss=0.17721]\n","Step 48191   [1.512 sec/step, loss=0.18151, avg_loss=0.17711]\n","Step 48192   [1.518 sec/step, loss=0.18242, avg_loss=0.17741]\n","Step 48193   [1.515 sec/step, loss=0.16620, avg_loss=0.17720]\n","Step 48194   [1.515 sec/step, loss=0.16429, avg_loss=0.17716]\n","Step 48195   [1.515 sec/step, loss=0.18663, avg_loss=0.17718]\n","Step 48196   [1.519 sec/step, loss=0.18793, avg_loss=0.17753]\n","Step 48197   [1.515 sec/step, loss=0.15595, avg_loss=0.17721]\n","Step 48198   [1.510 sec/step, loss=0.15472, avg_loss=0.17689]\n","Step 48199   [1.518 sec/step, loss=0.18468, avg_loss=0.17700]\n","Step 48200   [1.517 sec/step, loss=0.14915, avg_loss=0.17679]\n","Writing summary at step: 48200\n","Step 48201   [1.520 sec/step, loss=0.16985, avg_loss=0.17680]\n","Step 48202   [1.522 sec/step, loss=0.17630, avg_loss=0.17672]\n","Step 48203   [1.522 sec/step, loss=0.18723, avg_loss=0.17673]\n","Generated 32 batches of size 32 in 10.673 sec\n","Step 48204   [1.520 sec/step, loss=0.15299, avg_loss=0.17673]\n","Step 48205   [1.516 sec/step, loss=0.17078, avg_loss=0.17682]\n","Step 48206   [1.510 sec/step, loss=0.18890, avg_loss=0.17684]\n","Step 48207   [1.510 sec/step, loss=0.18438, avg_loss=0.17703]\n","Step 48208   [1.505 sec/step, loss=0.18615, avg_loss=0.17700]\n","Step 48209   [1.509 sec/step, loss=0.18085, avg_loss=0.17730]\n","Step 48210   [1.508 sec/step, loss=0.18204, avg_loss=0.17729]\n","Step 48211   [1.509 sec/step, loss=0.18489, avg_loss=0.17731]\n","Step 48212   [1.515 sec/step, loss=0.19103, avg_loss=0.17773]\n","Step 48213   [1.511 sec/step, loss=0.17017, avg_loss=0.17755]\n","Step 48214   [1.511 sec/step, loss=0.18404, avg_loss=0.17756]\n","Step 48215   [1.510 sec/step, loss=0.18309, avg_loss=0.17758]\n","Step 48216   [1.508 sec/step, loss=0.16345, avg_loss=0.17742]\n","Step 48217   [1.508 sec/step, loss=0.15561, avg_loss=0.17747]\n","Step 48218   [1.504 sec/step, loss=0.16288, avg_loss=0.17721]\n","Step 48219   [1.502 sec/step, loss=0.16077, avg_loss=0.17696]\n","Step 48220   [1.505 sec/step, loss=0.18852, avg_loss=0.17713]\n","Step 48221   [1.508 sec/step, loss=0.17536, avg_loss=0.17740]\n","Step 48222   [1.509 sec/step, loss=0.16392, avg_loss=0.17757]\n","Step 48223   [1.512 sec/step, loss=0.18931, avg_loss=0.17776]\n","Step 48224   [1.509 sec/step, loss=0.17082, avg_loss=0.17756]\n","Step 48225   [1.506 sec/step, loss=0.15402, avg_loss=0.17741]\n","Step 48226   [1.506 sec/step, loss=0.18130, avg_loss=0.17735]\n","Step 48227   [1.501 sec/step, loss=0.14952, avg_loss=0.17691]\n","Step 48228   [1.502 sec/step, loss=0.18870, avg_loss=0.17698]\n","Step 48229   [1.504 sec/step, loss=0.18668, avg_loss=0.17713]\n","Step 48230   [1.506 sec/step, loss=0.17434, avg_loss=0.17702]\n","Step 48231   [1.509 sec/step, loss=0.15552, avg_loss=0.17691]\n","Step 48232   [1.514 sec/step, loss=0.17151, avg_loss=0.17702]\n","Step 48233   [1.515 sec/step, loss=0.15612, avg_loss=0.17690]\n","Step 48234   [1.517 sec/step, loss=0.17006, avg_loss=0.17668]\n","Step 48235   [1.516 sec/step, loss=0.14951, avg_loss=0.17659]\n","Generated 32 batches of size 32 in 10.633 sec\n","Step 48236   [1.514 sec/step, loss=0.19305, avg_loss=0.17660]\n","Step 48237   [1.509 sec/step, loss=0.19310, avg_loss=0.17663]\n","Step 48238   [1.504 sec/step, loss=0.18820, avg_loss=0.17663]\n","Step 48239   [1.500 sec/step, loss=0.16704, avg_loss=0.17651]\n","Step 48240   [1.501 sec/step, loss=0.18658, avg_loss=0.17663]\n","Step 48241   [1.500 sec/step, loss=0.18624, avg_loss=0.17658]\n","Step 48242   [1.504 sec/step, loss=0.18655, avg_loss=0.17677]\n","Step 48243   [1.503 sec/step, loss=0.18536, avg_loss=0.17673]\n","Step 48244   [1.506 sec/step, loss=0.18121, avg_loss=0.17677]\n","Step 48245   [1.499 sec/step, loss=0.16516, avg_loss=0.17656]\n","Step 48246   [1.497 sec/step, loss=0.18162, avg_loss=0.17652]\n","Step 48247   [1.499 sec/step, loss=0.19033, avg_loss=0.17675]\n","Step 48248   [1.494 sec/step, loss=0.15161, avg_loss=0.17642]\n","Step 48249   [1.495 sec/step, loss=0.17213, avg_loss=0.17652]\n","Step 48250   [1.490 sec/step, loss=0.16727, avg_loss=0.17623]\n","Step 48251   [1.489 sec/step, loss=0.18722, avg_loss=0.17618]\n","Step 48252   [1.489 sec/step, loss=0.18643, avg_loss=0.17609]\n","Step 48253   [1.489 sec/step, loss=0.18678, avg_loss=0.17611]\n","Step 48254   [1.494 sec/step, loss=0.18343, avg_loss=0.17621]\n","Step 48255   [1.497 sec/step, loss=0.17589, avg_loss=0.17641]\n","Step 48256   [1.493 sec/step, loss=0.16796, avg_loss=0.17630]\n","Step 48257   [1.497 sec/step, loss=0.18801, avg_loss=0.17662]\n","Step 48258   [1.493 sec/step, loss=0.16499, avg_loss=0.17633]\n","Step 48259   [1.492 sec/step, loss=0.18817, avg_loss=0.17623]\n","Step 48260   [1.490 sec/step, loss=0.17537, avg_loss=0.17611]\n","Step 48261   [1.490 sec/step, loss=0.19683, avg_loss=0.17623]\n","Step 48262   [1.498 sec/step, loss=0.18876, avg_loss=0.17641]\n","Step 48263   [1.505 sec/step, loss=0.17160, avg_loss=0.17653]\n","Step 48264   [1.509 sec/step, loss=0.18789, avg_loss=0.17654]\n","Step 48265   [1.510 sec/step, loss=0.17046, avg_loss=0.17631]\n","Step 48266   [1.515 sec/step, loss=0.17283, avg_loss=0.17650]\n","Step 48267   [1.508 sec/step, loss=0.16038, avg_loss=0.17623]\n","Generated 32 batches of size 32 in 10.483 sec\n","Step 48268   [1.507 sec/step, loss=0.16001, avg_loss=0.17613]\n","Step 48269   [1.497 sec/step, loss=0.15250, avg_loss=0.17576]\n","Step 48270   [1.495 sec/step, loss=0.18692, avg_loss=0.17579]\n","Step 48271   [1.490 sec/step, loss=0.19044, avg_loss=0.17574]\n","Step 48272   [1.489 sec/step, loss=0.17454, avg_loss=0.17579]\n","Step 48273   [1.494 sec/step, loss=0.19291, avg_loss=0.17612]\n","Step 48274   [1.497 sec/step, loss=0.18543, avg_loss=0.17622]\n","Step 48275   [1.501 sec/step, loss=0.18469, avg_loss=0.17631]\n","Step 48276   [1.501 sec/step, loss=0.18349, avg_loss=0.17625]\n","Step 48277   [1.499 sec/step, loss=0.17126, avg_loss=0.17610]\n","Step 48278   [1.501 sec/step, loss=0.18494, avg_loss=0.17619]\n","Step 48279   [1.502 sec/step, loss=0.18660, avg_loss=0.17618]\n","Step 48280   [1.499 sec/step, loss=0.17254, avg_loss=0.17617]\n","Step 48281   [1.496 sec/step, loss=0.17065, avg_loss=0.17598]\n","Step 48282   [1.494 sec/step, loss=0.17629, avg_loss=0.17611]\n","Step 48283   [1.494 sec/step, loss=0.18315, avg_loss=0.17608]\n","Step 48284   [1.490 sec/step, loss=0.15597, avg_loss=0.17580]\n","Step 48285   [1.493 sec/step, loss=0.18343, avg_loss=0.17590]\n","Step 48286   [1.495 sec/step, loss=0.16588, avg_loss=0.17600]\n","Step 48287   [1.496 sec/step, loss=0.18573, avg_loss=0.17597]\n","Step 48288   [1.496 sec/step, loss=0.18426, avg_loss=0.17599]\n","Step 48289   [1.490 sec/step, loss=0.15429, avg_loss=0.17560]\n","Step 48290   [1.496 sec/step, loss=0.18377, avg_loss=0.17592]\n","Step 48291   [1.493 sec/step, loss=0.16353, avg_loss=0.17574]\n","Step 48292   [1.493 sec/step, loss=0.18383, avg_loss=0.17576]\n","Step 48293   [1.497 sec/step, loss=0.18130, avg_loss=0.17591]\n","Step 48294   [1.498 sec/step, loss=0.15218, avg_loss=0.17579]\n","Step 48295   [1.498 sec/step, loss=0.16696, avg_loss=0.17559]\n","Step 48296   [1.503 sec/step, loss=0.19622, avg_loss=0.17567]\n","Step 48297   [1.504 sec/step, loss=0.14996, avg_loss=0.17561]\n","Step 48298   [1.507 sec/step, loss=0.14943, avg_loss=0.17556]\n","Step 48299   [1.506 sec/step, loss=0.18548, avg_loss=0.17557]\n","Generated 32 batches of size 32 in 10.827 sec\n","Step 48300   [1.509 sec/step, loss=0.17210, avg_loss=0.17580]\n","Writing summary at step: 48300\n","Step 48301   [1.506 sec/step, loss=0.17003, avg_loss=0.17580]\n","Step 48302   [1.502 sec/step, loss=0.16883, avg_loss=0.17572]\n","Step 48303   [1.496 sec/step, loss=0.16117, avg_loss=0.17546]\n","Step 48304   [1.499 sec/step, loss=0.16901, avg_loss=0.17562]\n","Step 48305   [1.495 sec/step, loss=0.15895, avg_loss=0.17551]\n","Step 48306   [1.497 sec/step, loss=0.19087, avg_loss=0.17553]\n","Step 48307   [1.497 sec/step, loss=0.18191, avg_loss=0.17550]\n","Step 48308   [1.494 sec/step, loss=0.16836, avg_loss=0.17532]\n","Step 48309   [1.489 sec/step, loss=0.15019, avg_loss=0.17502]\n","Step 48310   [1.490 sec/step, loss=0.18738, avg_loss=0.17507]\n","Step 48311   [1.490 sec/step, loss=0.18008, avg_loss=0.17502]\n","Step 48312   [1.487 sec/step, loss=0.17479, avg_loss=0.17486]\n","Step 48313   [1.487 sec/step, loss=0.16630, avg_loss=0.17482]\n","Step 48314   [1.488 sec/step, loss=0.18589, avg_loss=0.17484]\n","Step 48315   [1.489 sec/step, loss=0.18906, avg_loss=0.17490]\n","Step 48316   [1.487 sec/step, loss=0.15147, avg_loss=0.17478]\n","Step 48317   [1.489 sec/step, loss=0.16417, avg_loss=0.17486]\n","Step 48318   [1.487 sec/step, loss=0.14696, avg_loss=0.17471]\n","Step 48319   [1.491 sec/step, loss=0.18768, avg_loss=0.17497]\n","Step 48320   [1.485 sec/step, loss=0.14643, avg_loss=0.17455]\n","Step 48321   [1.488 sec/step, loss=0.18543, avg_loss=0.17465]\n","Step 48322   [1.490 sec/step, loss=0.17089, avg_loss=0.17472]\n","Step 48323   [1.489 sec/step, loss=0.18268, avg_loss=0.17466]\n","Step 48324   [1.492 sec/step, loss=0.18087, avg_loss=0.17476]\n","Step 48325   [1.503 sec/step, loss=0.18299, avg_loss=0.17505]\n","Step 48326   [1.502 sec/step, loss=0.15597, avg_loss=0.17479]\n","Step 48327   [1.507 sec/step, loss=0.16475, avg_loss=0.17495]\n","Step 48328   [1.508 sec/step, loss=0.17010, avg_loss=0.17476]\n","Step 48329   [1.512 sec/step, loss=0.18678, avg_loss=0.17476]\n","Generated 32 batches of size 32 in 10.680 sec\n","Step 48330   [1.515 sec/step, loss=0.19002, avg_loss=0.17492]\n","Step 48331   [1.516 sec/step, loss=0.18516, avg_loss=0.17521]\n","Step 48332   [1.514 sec/step, loss=0.18800, avg_loss=0.17538]\n","Step 48333   [1.517 sec/step, loss=0.18555, avg_loss=0.17567]\n","Step 48334   [1.513 sec/step, loss=0.16444, avg_loss=0.17562]\n","Step 48335   [1.514 sec/step, loss=0.16995, avg_loss=0.17582]\n","Step 48336   [1.511 sec/step, loss=0.18164, avg_loss=0.17571]\n","Step 48337   [1.505 sec/step, loss=0.15350, avg_loss=0.17531]\n","Step 48338   [1.499 sec/step, loss=0.14962, avg_loss=0.17493]\n","Step 48339   [1.498 sec/step, loss=0.14660, avg_loss=0.17472]\n","Step 48340   [1.498 sec/step, loss=0.18143, avg_loss=0.17467]\n","Step 48341   [1.498 sec/step, loss=0.15206, avg_loss=0.17433]\n","Step 48342   [1.498 sec/step, loss=0.17955, avg_loss=0.17426]\n","Step 48343   [1.498 sec/step, loss=0.19274, avg_loss=0.17433]\n","Step 48344   [1.495 sec/step, loss=0.16997, avg_loss=0.17422]\n","Step 48345   [1.502 sec/step, loss=0.18913, avg_loss=0.17446]\n","Step 48346   [1.505 sec/step, loss=0.18258, avg_loss=0.17447]\n","Step 48347   [1.505 sec/step, loss=0.19097, avg_loss=0.17448]\n","Step 48348   [1.507 sec/step, loss=0.17044, avg_loss=0.17466]\n","Step 48349   [1.511 sec/step, loss=0.18272, avg_loss=0.17477]\n","Step 48350   [1.515 sec/step, loss=0.18622, avg_loss=0.17496]\n","Step 48351   [1.508 sec/step, loss=0.17286, avg_loss=0.17482]\n","Step 48352   [1.508 sec/step, loss=0.17987, avg_loss=0.17475]\n","Step 48353   [1.503 sec/step, loss=0.16735, avg_loss=0.17456]\n","Step 48354   [1.503 sec/step, loss=0.19185, avg_loss=0.17464]\n","Step 48355   [1.506 sec/step, loss=0.18920, avg_loss=0.17477]\n","Step 48356   [1.508 sec/step, loss=0.16492, avg_loss=0.17474]\n","Step 48357   [1.512 sec/step, loss=0.18624, avg_loss=0.17473]\n","Step 48358   [1.521 sec/step, loss=0.18563, avg_loss=0.17493]\n","Step 48359   [1.522 sec/step, loss=0.17761, avg_loss=0.17483]\n","Step 48360   [1.529 sec/step, loss=0.18430, avg_loss=0.17492]\n","Step 48361   [1.526 sec/step, loss=0.15383, avg_loss=0.17449]\n","Generated 32 batches of size 32 in 10.444 sec\n","Step 48362   [1.519 sec/step, loss=0.16717, avg_loss=0.17427]\n","Step 48363   [1.518 sec/step, loss=0.18753, avg_loss=0.17443]\n","Step 48364   [1.511 sec/step, loss=0.17149, avg_loss=0.17426]\n","Step 48365   [1.506 sec/step, loss=0.15259, avg_loss=0.17409]\n","Step 48366   [1.505 sec/step, loss=0.16956, avg_loss=0.17405]\n","Step 48367   [1.502 sec/step, loss=0.15014, avg_loss=0.17395]\n","Step 48368   [1.500 sec/step, loss=0.16358, avg_loss=0.17399]\n","Step 48369   [1.506 sec/step, loss=0.18903, avg_loss=0.17435]\n","Step 48370   [1.506 sec/step, loss=0.18437, avg_loss=0.17433]\n","Step 48371   [1.506 sec/step, loss=0.18514, avg_loss=0.17427]\n","Step 48372   [1.507 sec/step, loss=0.17482, avg_loss=0.17428]\n","Step 48373   [1.501 sec/step, loss=0.15267, avg_loss=0.17387]\n","Step 48374   [1.498 sec/step, loss=0.16919, avg_loss=0.17371]\n","Step 48375   [1.497 sec/step, loss=0.18587, avg_loss=0.17372]\n","Step 48376   [1.493 sec/step, loss=0.16847, avg_loss=0.17357]\n","Step 48377   [1.495 sec/step, loss=0.18526, avg_loss=0.17371]\n","Step 48378   [1.492 sec/step, loss=0.16826, avg_loss=0.17355]\n","Step 48379   [1.488 sec/step, loss=0.15370, avg_loss=0.17322]\n","Step 48380   [1.492 sec/step, loss=0.15862, avg_loss=0.17308]\n","Step 48381   [1.490 sec/step, loss=0.16309, avg_loss=0.17300]\n","Step 48382   [1.495 sec/step, loss=0.19356, avg_loss=0.17318]\n","Step 48383   [1.490 sec/step, loss=0.15146, avg_loss=0.17286]\n","Step 48384   [1.494 sec/step, loss=0.18692, avg_loss=0.17317]\n","Step 48385   [1.493 sec/step, loss=0.18746, avg_loss=0.17321]\n","Step 48386   [1.497 sec/step, loss=0.18614, avg_loss=0.17341]\n","Step 48387   [1.496 sec/step, loss=0.18343, avg_loss=0.17339]\n","Step 48388   [1.492 sec/step, loss=0.17497, avg_loss=0.17329]\n","Step 48389   [1.494 sec/step, loss=0.15332, avg_loss=0.17329]\n","Step 48390   [1.498 sec/step, loss=0.18687, avg_loss=0.17332]\n","Step 48391   [1.505 sec/step, loss=0.18576, avg_loss=0.17354]\n","Step 48392   [1.510 sec/step, loss=0.19758, avg_loss=0.17368]\n","Step 48393   [1.513 sec/step, loss=0.18262, avg_loss=0.17369]\n","Generated 32 batches of size 32 in 10.382 sec\n","Step 48394   [1.514 sec/step, loss=0.16636, avg_loss=0.17383]\n","Step 48395   [1.515 sec/step, loss=0.18701, avg_loss=0.17403]\n","Step 48396   [1.502 sec/step, loss=0.15233, avg_loss=0.17359]\n","Step 48397   [1.505 sec/step, loss=0.18488, avg_loss=0.17394]\n","Step 48398   [1.506 sec/step, loss=0.16163, avg_loss=0.17406]\n","Step 48399   [1.502 sec/step, loss=0.18474, avg_loss=0.17406]\n","Step 48400   [1.499 sec/step, loss=0.16376, avg_loss=0.17397]\n","Writing summary at step: 48400\n","Step 48401   [1.499 sec/step, loss=0.17293, avg_loss=0.17400]\n","Step 48402   [1.503 sec/step, loss=0.18706, avg_loss=0.17418]\n","Step 48403   [1.505 sec/step, loss=0.18145, avg_loss=0.17439]\n","Step 48404   [1.503 sec/step, loss=0.16548, avg_loss=0.17435]\n","Step 48405   [1.508 sec/step, loss=0.17830, avg_loss=0.17455]\n","Step 48406   [1.503 sec/step, loss=0.16137, avg_loss=0.17425]\n","Step 48407   [1.497 sec/step, loss=0.15615, avg_loss=0.17399]\n","Step 48408   [1.501 sec/step, loss=0.18652, avg_loss=0.17417]\n","Step 48409   [1.506 sec/step, loss=0.18931, avg_loss=0.17457]\n","Step 48410   [1.506 sec/step, loss=0.18535, avg_loss=0.17454]\n","Step 48411   [1.506 sec/step, loss=0.18506, avg_loss=0.17459]\n","Step 48412   [1.502 sec/step, loss=0.15437, avg_loss=0.17439]\n","Step 48413   [1.506 sec/step, loss=0.18424, avg_loss=0.17457]\n","Step 48414   [1.504 sec/step, loss=0.17831, avg_loss=0.17449]\n","Step 48415   [1.503 sec/step, loss=0.18570, avg_loss=0.17446]\n","Step 48416   [1.507 sec/step, loss=0.17150, avg_loss=0.17466]\n","Step 48417   [1.508 sec/step, loss=0.16981, avg_loss=0.17472]\n","Step 48418   [1.508 sec/step, loss=0.15460, avg_loss=0.17479]\n","Step 48419   [1.503 sec/step, loss=0.15786, avg_loss=0.17450]\n","Step 48420   [1.508 sec/step, loss=0.16781, avg_loss=0.17471]\n","Step 48421   [1.513 sec/step, loss=0.19005, avg_loss=0.17476]\n","Step 48422   [1.520 sec/step, loss=0.18947, avg_loss=0.17494]\n","Step 48423   [1.525 sec/step, loss=0.19213, avg_loss=0.17504]\n","Step 48424   [1.522 sec/step, loss=0.15811, avg_loss=0.17481]\n","Generated 32 batches of size 32 in 10.656 sec\n","Step 48425   [1.520 sec/step, loss=0.18602, avg_loss=0.17484]\n","Step 48426   [1.519 sec/step, loss=0.18939, avg_loss=0.17517]\n","Step 48427   [1.514 sec/step, loss=0.15168, avg_loss=0.17504]\n","Step 48428   [1.513 sec/step, loss=0.19103, avg_loss=0.17525]\n","Step 48429   [1.509 sec/step, loss=0.18389, avg_loss=0.17522]\n","Step 48430   [1.501 sec/step, loss=0.17387, avg_loss=0.17506]\n","Step 48431   [1.497 sec/step, loss=0.16323, avg_loss=0.17484]\n","Step 48432   [1.493 sec/step, loss=0.17826, avg_loss=0.17474]\n","Step 48433   [1.493 sec/step, loss=0.19176, avg_loss=0.17481]\n","Step 48434   [1.490 sec/step, loss=0.15504, avg_loss=0.17471]\n","Step 48435   [1.492 sec/step, loss=0.18425, avg_loss=0.17486]\n","Step 48436   [1.492 sec/step, loss=0.18721, avg_loss=0.17491]\n","Step 48437   [1.498 sec/step, loss=0.18205, avg_loss=0.17520]\n","Step 48438   [1.502 sec/step, loss=0.17990, avg_loss=0.17550]\n","Step 48439   [1.504 sec/step, loss=0.17556, avg_loss=0.17579]\n","Step 48440   [1.505 sec/step, loss=0.18683, avg_loss=0.17584]\n","Step 48441   [1.505 sec/step, loss=0.18655, avg_loss=0.17619]\n","Step 48442   [1.501 sec/step, loss=0.18101, avg_loss=0.17620]\n","Step 48443   [1.499 sec/step, loss=0.16780, avg_loss=0.17595]\n","Step 48444   [1.502 sec/step, loss=0.18850, avg_loss=0.17614]\n","Step 48445   [1.496 sec/step, loss=0.15794, avg_loss=0.17583]\n","Step 48446   [1.496 sec/step, loss=0.18662, avg_loss=0.17587]\n","Step 48447   [1.491 sec/step, loss=0.16997, avg_loss=0.17566]\n","Step 48448   [1.493 sec/step, loss=0.18740, avg_loss=0.17583]\n","Step 48449   [1.488 sec/step, loss=0.15013, avg_loss=0.17550]\n","Step 48450   [1.484 sec/step, loss=0.16756, avg_loss=0.17531]\n","Step 48451   [1.491 sec/step, loss=0.19043, avg_loss=0.17549]\n","Step 48452   [1.496 sec/step, loss=0.19173, avg_loss=0.17561]\n","Step 48453   [1.502 sec/step, loss=0.17327, avg_loss=0.17567]\n","Step 48454   [1.507 sec/step, loss=0.19158, avg_loss=0.17566]\n","Step 48455   [1.511 sec/step, loss=0.18299, avg_loss=0.17560]\n","Step 48456   [1.519 sec/step, loss=0.18217, avg_loss=0.17577]\n","Generated 32 batches of size 32 in 10.616 sec\n","Step 48457   [1.510 sec/step, loss=0.15740, avg_loss=0.17549]\n","Step 48458   [1.506 sec/step, loss=0.19040, avg_loss=0.17553]\n","Step 48459   [1.505 sec/step, loss=0.18848, avg_loss=0.17564]\n","Step 48460   [1.495 sec/step, loss=0.15116, avg_loss=0.17531]\n","Step 48461   [1.498 sec/step, loss=0.18759, avg_loss=0.17565]\n","Step 48462   [1.496 sec/step, loss=0.17504, avg_loss=0.17573]\n","Step 48463   [1.497 sec/step, loss=0.18847, avg_loss=0.17574]\n","Step 48464   [1.499 sec/step, loss=0.18560, avg_loss=0.17588]\n","Step 48465   [1.497 sec/step, loss=0.15413, avg_loss=0.17589]\n","Step 48466   [1.493 sec/step, loss=0.15095, avg_loss=0.17571]\n","Step 48467   [1.495 sec/step, loss=0.16554, avg_loss=0.17586]\n","Step 48468   [1.499 sec/step, loss=0.18752, avg_loss=0.17610]\n","Step 48469   [1.495 sec/step, loss=0.15303, avg_loss=0.17574]\n","Step 48470   [1.494 sec/step, loss=0.16230, avg_loss=0.17552]\n","Step 48471   [1.491 sec/step, loss=0.17921, avg_loss=0.17546]\n","Step 48472   [1.494 sec/step, loss=0.18767, avg_loss=0.17559]\n","Step 48473   [1.496 sec/step, loss=0.16746, avg_loss=0.17574]\n","Step 48474   [1.497 sec/step, loss=0.19469, avg_loss=0.17599]\n","Step 48475   [1.498 sec/step, loss=0.18962, avg_loss=0.17603]\n","Step 48476   [1.501 sec/step, loss=0.18612, avg_loss=0.17621]\n","Step 48477   [1.496 sec/step, loss=0.16174, avg_loss=0.17597]\n","Step 48478   [1.498 sec/step, loss=0.18444, avg_loss=0.17613]\n","Step 48479   [1.500 sec/step, loss=0.19128, avg_loss=0.17651]\n","Step 48480   [1.496 sec/step, loss=0.15115, avg_loss=0.17643]\n","Step 48481   [1.501 sec/step, loss=0.19236, avg_loss=0.17673]\n","Step 48482   [1.501 sec/step, loss=0.18844, avg_loss=0.17668]\n","Step 48483   [1.501 sec/step, loss=0.15161, avg_loss=0.17668]\n","Step 48484   [1.502 sec/step, loss=0.17751, avg_loss=0.17658]\n","Step 48485   [1.503 sec/step, loss=0.16702, avg_loss=0.17638]\n","Step 48486   [1.508 sec/step, loss=0.19363, avg_loss=0.17645]\n","Step 48487   [1.513 sec/step, loss=0.19016, avg_loss=0.17652]\n","Step 48488   [1.522 sec/step, loss=0.19403, avg_loss=0.17671]\n","Generated 32 batches of size 32 in 10.574 sec\n","Step 48489   [1.527 sec/step, loss=0.18637, avg_loss=0.17704]\n","Step 48490   [1.521 sec/step, loss=0.17888, avg_loss=0.17696]\n","Step 48491   [1.517 sec/step, loss=0.18699, avg_loss=0.17697]\n","Step 48492   [1.508 sec/step, loss=0.16748, avg_loss=0.17667]\n","Step 48493   [1.499 sec/step, loss=0.15949, avg_loss=0.17644]\n","Step 48494   [1.497 sec/step, loss=0.17621, avg_loss=0.17654]\n","Step 48495   [1.498 sec/step, loss=0.18763, avg_loss=0.17655]\n","Step 48496   [1.501 sec/step, loss=0.17253, avg_loss=0.17675]\n","Step 48497   [1.502 sec/step, loss=0.18923, avg_loss=0.17679]\n","Step 48498   [1.504 sec/step, loss=0.18582, avg_loss=0.17703]\n","Step 48499   [1.504 sec/step, loss=0.18585, avg_loss=0.17705]\n","Step 48500   [1.507 sec/step, loss=0.18476, avg_loss=0.17726]\n","Writing summary at step: 48500\n","Saving audio and alignment...\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/train-step-000048500-align000.png\n","100% 1/1 [00:02<00:00,  2.53s/it]\n","Test finished for step 48500.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000048500-align000.png\n"," 50% 1/2 [00:12<00:12, 12.05s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron2/son_2021-02-06_23-59-17/test-step-000048500-align001.png\n","100% 2/2 [00:24<00:00, 12.02s/it]\n","Test finished for step 48500.\n","Step 48501   [1.508 sec/step, loss=0.17958, avg_loss=0.17732]\n","Step 48502   [1.505 sec/step, loss=0.17430, avg_loss=0.17719]\n","Step 48503   [1.505 sec/step, loss=0.18788, avg_loss=0.17726]\n","Step 48504   [1.509 sec/step, loss=0.19208, avg_loss=0.17752]\n","Step 48505   [1.509 sec/step, loss=0.18377, avg_loss=0.17758]\n","Step 48506   [1.513 sec/step, loss=0.18791, avg_loss=0.17785]\n","Step 48507   [1.518 sec/step, loss=0.18464, avg_loss=0.17813]\n","Step 48508   [1.514 sec/step, loss=0.17467, avg_loss=0.17801]\n","Step 48509   [1.511 sec/step, loss=0.15961, avg_loss=0.17771]\n","Step 48510   [1.510 sec/step, loss=0.19041, avg_loss=0.17776]\n","Step 48511   [1.505 sec/step, loss=0.15488, avg_loss=0.17746]\n","Step 48512   [1.508 sec/step, loss=0.18609, avg_loss=0.17778]\n","Step 48513   [1.508 sec/step, loss=0.19095, avg_loss=0.17785]\n","Step 48514   [1.510 sec/step, loss=0.18601, avg_loss=0.17792]\n","Step 48515   [1.507 sec/step, loss=0.15618, avg_loss=0.17763]\n","Step 48516   [1.513 sec/step, loss=0.18512, avg_loss=0.17777]\n","Step 48517   [1.521 sec/step, loss=0.18994, avg_loss=0.17797]\n","Step 48518   [1.527 sec/step, loss=0.16960, avg_loss=0.17812]\n","Generated 32 batches of size 32 in 10.546 sec\n","Step 48519   [1.535 sec/step, loss=0.18343, avg_loss=0.17837]\n","Step 48520   [1.536 sec/step, loss=0.19590, avg_loss=0.17865]\n","Step 48521   [1.527 sec/step, loss=0.16568, avg_loss=0.17841]\n","Step 48522   [1.516 sec/step, loss=0.15643, avg_loss=0.17808]\n","Step 48523   [1.512 sec/step, loss=0.18458, avg_loss=0.17800]\n","Step 48524   [1.515 sec/step, loss=0.19369, avg_loss=0.17836]\n","Step 48525   [1.512 sec/step, loss=0.18914, avg_loss=0.17839]\n","Step 48526   [1.514 sec/step, loss=0.18494, avg_loss=0.17835]\n","Step 48527   [1.520 sec/step, loss=0.18548, avg_loss=0.17868]\n","Step 48528   [1.520 sec/step, loss=0.18627, avg_loss=0.17864]\n","Step 48529   [1.520 sec/step, loss=0.18155, avg_loss=0.17861]\n","Step 48530   [1.523 sec/step, loss=0.18533, avg_loss=0.17873]\n","Step 48531   [1.525 sec/step, loss=0.16488, avg_loss=0.17874]\n","Step 48532   [1.529 sec/step, loss=0.18610, avg_loss=0.17882]\n","Step 48533   [1.525 sec/step, loss=0.16751, avg_loss=0.17858]\n","Step 48534   [1.525 sec/step, loss=0.15505, avg_loss=0.17858]\n","Step 48535   [1.522 sec/step, loss=0.17694, avg_loss=0.17851]\n","Step 48536   [1.518 sec/step, loss=0.16594, avg_loss=0.17829]\n","Step 48537   [1.512 sec/step, loss=0.15072, avg_loss=0.17798]\n","Step 48538   [1.514 sec/step, loss=0.19320, avg_loss=0.17811]\n","Step 48539   [1.517 sec/step, loss=0.20224, avg_loss=0.17838]\n","Step 48540   [1.513 sec/step, loss=0.15540, avg_loss=0.17807]\n","Step 48541   [1.509 sec/step, loss=0.16789, avg_loss=0.17788]\n","Step 48542   [1.514 sec/step, loss=0.18777, avg_loss=0.17795]\n","Step 48543   [1.516 sec/step, loss=0.18035, avg_loss=0.17807]\n","Step 48544   [1.510 sec/step, loss=0.15496, avg_loss=0.17774]\n","Step 48545   [1.514 sec/step, loss=0.17824, avg_loss=0.17794]\n","Step 48546   [1.515 sec/step, loss=0.17487, avg_loss=0.17782]\n","Step 48547   [1.520 sec/step, loss=0.17150, avg_loss=0.17784]\n","Step 48548   [1.527 sec/step, loss=0.18903, avg_loss=0.17785]\n","Step 48549   [1.533 sec/step, loss=0.17647, avg_loss=0.17812]\n","Step 48550   [1.538 sec/step, loss=0.17776, avg_loss=0.17822]\n","Step 48551   [1.534 sec/step, loss=0.15542, avg_loss=0.17787]\n","Generated 32 batches of size 32 in 10.499 sec\n","Step 48552   [1.531 sec/step, loss=0.18741, avg_loss=0.17783]\n","Step 48553   [1.523 sec/step, loss=0.14953, avg_loss=0.17759]\n","Step 48554   [1.518 sec/step, loss=0.18519, avg_loss=0.17753]\n","Step 48555   [1.514 sec/step, loss=0.18921, avg_loss=0.17759]\n","Step 48556   [1.503 sec/step, loss=0.15183, avg_loss=0.17728]\n","Step 48557   [1.508 sec/step, loss=0.18459, avg_loss=0.17756]\n","Step 48558   [1.504 sec/step, loss=0.16707, avg_loss=0.17732]\n","Step 48559   [1.504 sec/step, loss=0.18778, avg_loss=0.17732]\n","Step 48560   [1.507 sec/step, loss=0.17668, avg_loss=0.17757]\n","Step 48561   [1.505 sec/step, loss=0.17356, avg_loss=0.17743]\n","Step 48562   [1.503 sec/step, loss=0.15460, avg_loss=0.17723]\n","Step 48563   [1.498 sec/step, loss=0.16526, avg_loss=0.17699]\n","Step 48564   [1.495 sec/step, loss=0.16346, avg_loss=0.17677]\n","Step 48565   [1.500 sec/step, loss=0.18790, avg_loss=0.17711]\n","Step 48566   [1.500 sec/step, loss=0.14969, avg_loss=0.17710]\n","Step 48567   [1.498 sec/step, loss=0.14815, avg_loss=0.17692]\n","Step 48568   [1.498 sec/step, loss=0.18978, avg_loss=0.17695]\n","Step 48569   [1.502 sec/step, loss=0.18325, avg_loss=0.17725]\n","Step 48570   [1.501 sec/step, loss=0.18200, avg_loss=0.17745]\n","Step 48571   [1.504 sec/step, loss=0.19131, avg_loss=0.17757]\n","Step 48572   [1.504 sec/step, loss=0.18611, avg_loss=0.17755]\n","Step 48573   [1.505 sec/step, loss=0.17027, avg_loss=0.17758]\n","Step 48574   [1.506 sec/step, loss=0.16230, avg_loss=0.17726]\n","Step 48575   [1.501 sec/step, loss=0.15848, avg_loss=0.17694]\n","Step 48576   [1.501 sec/step, loss=0.18531, avg_loss=0.17694]\n","Step 48577   [1.502 sec/step, loss=0.16409, avg_loss=0.17696]\n","Step 48578   [1.508 sec/step, loss=0.18921, avg_loss=0.17701]\n","Step 48579   [1.514 sec/step, loss=0.19267, avg_loss=0.17702]\n","Step 48580   [1.525 sec/step, loss=0.18564, avg_loss=0.17737]\n","Step 48581   [1.529 sec/step, loss=0.18360, avg_loss=0.17728]\n","Step 48582   [1.530 sec/step, loss=0.17702, avg_loss=0.17716]\n","Generated 32 batches of size 32 in 10.885 sec\n","Step 48583   [1.538 sec/step, loss=0.18883, avg_loss=0.17754]\n","Step 48584   [1.530 sec/step, loss=0.16063, avg_loss=0.17737]\n","Step 48585   [1.527 sec/step, loss=0.17244, avg_loss=0.17742]\n","Step 48586   [1.523 sec/step, loss=0.18546, avg_loss=0.17734]\n","Step 48587   [1.513 sec/step, loss=0.15042, avg_loss=0.17694]\n","Step 48588   [1.505 sec/step, loss=0.17185, avg_loss=0.17672]\n","Step 48589   [1.504 sec/step, loss=0.19357, avg_loss=0.17679]\n","Step 48590   [1.502 sec/step, loss=0.16014, avg_loss=0.17661]\n","Step 48591   [1.503 sec/step, loss=0.18570, avg_loss=0.17659]\n","Step 48592   [1.507 sec/step, loss=0.18864, avg_loss=0.17680]\n","Step 48593   [1.507 sec/step, loss=0.15190, avg_loss=0.17673]\n","Step 48594   [1.504 sec/step, loss=0.14904, avg_loss=0.17646]\n","Step 48595   [1.504 sec/step, loss=0.18234, avg_loss=0.17640]\n","Step 48596   [1.506 sec/step, loss=0.18543, avg_loss=0.17653]\n","Step 48597   [1.506 sec/step, loss=0.18056, avg_loss=0.17645]\n","Step 48598   [1.502 sec/step, loss=0.16673, avg_loss=0.17626]\n","Step 48599   [1.499 sec/step, loss=0.17862, avg_loss=0.17618]\n","Step 48600   [1.499 sec/step, loss=0.18915, avg_loss=0.17623]\n","Writing summary at step: 48600\n","Step 48601   [1.501 sec/step, loss=0.18691, avg_loss=0.17630]\n","Step 48602   [1.499 sec/step, loss=0.16480, avg_loss=0.17621]\n","Step 48603   [1.499 sec/step, loss=0.18500, avg_loss=0.17618]\n","Step 48604   [1.498 sec/step, loss=0.18552, avg_loss=0.17611]\n","Step 48605   [1.494 sec/step, loss=0.16465, avg_loss=0.17592]\n","Step 48606   [1.494 sec/step, loss=0.18064, avg_loss=0.17585]\n","Step 48607   [1.495 sec/step, loss=0.18513, avg_loss=0.17585]\n","Step 48608   [1.499 sec/step, loss=0.18337, avg_loss=0.17594]\n","Step 48609   [1.507 sec/step, loss=0.18655, avg_loss=0.17621]\n","Step 48610   [1.513 sec/step, loss=0.18470, avg_loss=0.17615]\n","Step 48611   [1.517 sec/step, loss=0.17005, avg_loss=0.17630]\n","Step 48612   [1.520 sec/step, loss=0.17478, avg_loss=0.17619]\n","Step 48613   [1.521 sec/step, loss=0.16267, avg_loss=0.17591]\n","Generated 32 batches of size 32 in 10.666 sec\n","Step 48614   [1.524 sec/step, loss=0.17096, avg_loss=0.17576]\n","Step 48615   [1.525 sec/step, loss=0.17001, avg_loss=0.17589]\n","Step 48616   [1.521 sec/step, loss=0.18815, avg_loss=0.17592]\n","Step 48617   [1.511 sec/step, loss=0.16488, avg_loss=0.17567]\n","Step 48618   [1.509 sec/step, loss=0.17284, avg_loss=0.17571]\n","Step 48619   [1.502 sec/step, loss=0.15232, avg_loss=0.17540]\n","Step 48620   [1.497 sec/step, loss=0.16176, avg_loss=0.17505]\n","Step 48621   [1.495 sec/step, loss=0.15447, avg_loss=0.17494]\n","Step 48622   [1.497 sec/step, loss=0.16118, avg_loss=0.17499]\n","Step 48623   [1.498 sec/step, loss=0.19368, avg_loss=0.17508]\n","Step 48624   [1.497 sec/step, loss=0.18808, avg_loss=0.17502]\n","Step 48625   [1.495 sec/step, loss=0.16005, avg_loss=0.17473]\n","Step 48626   [1.495 sec/step, loss=0.19465, avg_loss=0.17483]\n","Step 48627   [1.494 sec/step, loss=0.18434, avg_loss=0.17482]\n","Step 48628   [1.493 sec/step, loss=0.18596, avg_loss=0.17482]\n","Step 48629   [1.490 sec/step, loss=0.17446, avg_loss=0.17475]\n","Step 48630   [1.490 sec/step, loss=0.18438, avg_loss=0.17474]\n","Step 48631   [1.492 sec/step, loss=0.18090, avg_loss=0.17490]\n","Step 48632   [1.486 sec/step, loss=0.15913, avg_loss=0.17463]\n","Step 48633   [1.488 sec/step, loss=0.16931, avg_loss=0.17464]\n","Step 48634   [1.490 sec/step, loss=0.16636, avg_loss=0.17476]\n","Step 48635   [1.493 sec/step, loss=0.19082, avg_loss=0.17490]\n","Step 48636   [1.498 sec/step, loss=0.18767, avg_loss=0.17511]\n","Step 48637   [1.502 sec/step, loss=0.16933, avg_loss=0.17530]\n","Step 48638   [1.496 sec/step, loss=0.15369, avg_loss=0.17490]\n","Step 48639   [1.490 sec/step, loss=0.15081, avg_loss=0.17439]\n","Step 48640   [1.489 sec/step, loss=0.14857, avg_loss=0.17432]\n","Step 48641   [1.496 sec/step, loss=0.18446, avg_loss=0.17449]\n","Step 48642   [1.501 sec/step, loss=0.19433, avg_loss=0.17455]\n","Step 48643   [1.502 sec/step, loss=0.17103, avg_loss=0.17446]\n","Step 48644   [1.510 sec/step, loss=0.18099, avg_loss=0.17472]\n","Step 48645   [1.517 sec/step, loss=0.18726, avg_loss=0.17481]\n","Generated 32 batches of size 32 in 10.343 sec\n","Step 48646   [1.517 sec/step, loss=0.19096, avg_loss=0.17497]\n","Step 48647   [1.517 sec/step, loss=0.18294, avg_loss=0.17509]\n","Step 48648   [1.512 sec/step, loss=0.18199, avg_loss=0.17502]\n","Step 48649   [1.508 sec/step, loss=0.16820, avg_loss=0.17493]\n","Step 48650   [1.506 sec/step, loss=0.18493, avg_loss=0.17500]\n","Step 48651   [1.510 sec/step, loss=0.18067, avg_loss=0.17526]\n","Step 48652   [1.505 sec/step, loss=0.16323, avg_loss=0.17501]\n","Step 48653   [1.511 sec/step, loss=0.18116, avg_loss=0.17533]\n","Step 48654   [1.505 sec/step, loss=0.17267, avg_loss=0.17521]\n","Step 48655   [1.505 sec/step, loss=0.17865, avg_loss=0.17510]\n","Step 48656   [1.512 sec/step, loss=0.19544, avg_loss=0.17554]\n","Step 48657   [1.509 sec/step, loss=0.17590, avg_loss=0.17545]\n","Step 48658   [1.513 sec/step, loss=0.18729, avg_loss=0.17565]\n","Step 48659   [1.513 sec/step, loss=0.18436, avg_loss=0.17562]\n","Step 48660   [1.515 sec/step, loss=0.18276, avg_loss=0.17568]\n","Step 48661   [1.518 sec/step, loss=0.18878, avg_loss=0.17583]\n","Step 48662   [1.524 sec/step, loss=0.18885, avg_loss=0.17617]\n","Step 48663   [1.524 sec/step, loss=0.17136, avg_loss=0.17623]\n","Step 48664   [1.523 sec/step, loss=0.15612, avg_loss=0.17616]\n","Step 48665   [1.517 sec/step, loss=0.15223, avg_loss=0.17580]\n","Step 48666   [1.523 sec/step, loss=0.18135, avg_loss=0.17612]\n","Step 48667   [1.529 sec/step, loss=0.17940, avg_loss=0.17643]\n","Step 48668   [1.525 sec/step, loss=0.17663, avg_loss=0.17630]\n","Step 48669   [1.526 sec/step, loss=0.18644, avg_loss=0.17633]\n","Step 48670   [1.522 sec/step, loss=0.15851, avg_loss=0.17610]\n","Step 48671   [1.520 sec/step, loss=0.17096, avg_loss=0.17589]\n","Step 48672   [1.519 sec/step, loss=0.18449, avg_loss=0.17588]\n","Step 48673   [1.526 sec/step, loss=0.15036, avg_loss=0.17568]\n","Step 48674   [1.524 sec/step, loss=0.16485, avg_loss=0.17571]\n","Step 48675   [1.527 sec/step, loss=0.16548, avg_loss=0.17578]\n","Step 48676   [1.529 sec/step, loss=0.17191, avg_loss=0.17564]\n","Step 48677   [1.538 sec/step, loss=0.18519, avg_loss=0.17585]\n","Generated 32 batches of size 32 in 10.523 sec\n","Step 48678   [1.535 sec/step, loss=0.15816, avg_loss=0.17554]\n","Step 48679   [1.525 sec/step, loss=0.15064, avg_loss=0.17512]\n","Step 48680   [1.520 sec/step, loss=0.19170, avg_loss=0.17518]\n","Step 48681   [1.510 sec/step, loss=0.16391, avg_loss=0.17499]\n","Step 48682   [1.509 sec/step, loss=0.19174, avg_loss=0.17513]\n","Step 48683   [1.504 sec/step, loss=0.16198, avg_loss=0.17486]\n","Step 48684   [1.511 sec/step, loss=0.18578, avg_loss=0.17512]\n","Step 48685   [1.511 sec/step, loss=0.15865, avg_loss=0.17498]\n","Step 48686   [1.510 sec/step, loss=0.18375, avg_loss=0.17496]\n","Step 48687   [1.512 sec/step, loss=0.16116, avg_loss=0.17507]\n","Step 48688   [1.514 sec/step, loss=0.17981, avg_loss=0.17515]\n","Step 48689   [1.514 sec/step, loss=0.18566, avg_loss=0.17507]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d21AnViUZdXD"},"source":["function ClickConnect(){\r\n","    console.log(\"코랩 연결 끊김 방지\"); \r\n","    document.querySelector(\"colab-toolbar-button#connect\").click() \r\n","}\r\n","setInterval(ClickConnect, 60 * 1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNboYk1v6B_s","executionInfo":{"status":"ok","timestamp":1612831496394,"user_tz":-540,"elapsed":27511,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"8fa96b44-d4fa-4f26-e7c5-dca0bcd07af2"},"source":["!python synthesizer.py --load_path logdir-tacotron2/son_2021-02-06_23-59-17 --num_speakers 1 --speaker_id 0 --text \"김현우 화이팅\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From synthesizer.py:37: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n","\n","WARNING:tensorflow:From synthesizer.py:37: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n","\n"," [*] Found lastest checkpoint: logdir-tacotron2/son_2021-02-06_23-59-17/model.ckpt-44000\n","Constructing model: tacotron\n","UPDATE attention_kernel: (31,) -> [31]\n","UPDATE postnet_kernel_size: (5,) -> [5]\n","========================================\n"," model_type: single\n","========================================\n","Initialized Tacotron model. Dimensions: \n","    embedding:                512\n","    encoder conv out:               512\n","    encoder out:              512\n","    attention out:            1024\n","    decoder prenet lstm concat out :        1536\n","    decoder cell out:         162\n","    decoder out (2 frames):  162\n","    decoder mel out:    80\n","    mel out:    80\n","    postnet out:              256\n","    linear out:               1025\n","  Tacotron Parameters       29.142 Million.\n","Loading checkpoint: logdir-tacotron2/son_2021-02-06_23-59-17/model.ckpt-44000\n","2021-02-09 00:44:45.788492: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n","2021-02-09 00:44:45.788782: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x70a0700 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-02-09 00:44:45.788863: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-02-09 00:44:45.791474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-02-09 00:44:45.912520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:44:45.913867: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x70a08c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-02-09 00:44:45.913919: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2021-02-09 00:44:45.914193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:44:45.915399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n","pciBusID: 0000:00:04.0\n","2021-02-09 00:44:45.915899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-02-09 00:44:45.917905: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-02-09 00:44:45.919799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-02-09 00:44:45.920161: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-02-09 00:44:45.921979: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-02-09 00:44:45.922913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-02-09 00:44:45.926538: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-02-09 00:44:45.926768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:44:45.927828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:44:45.928693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-02-09 00:44:45.928810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-02-09 00:44:45.930734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-02-09 00:44:45.930763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-02-09 00:44:45.930773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-02-09 00:44:45.930934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:44:45.931912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-09 00:44:45.932760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","2021-02-09 00:44:49.403647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-02-09 00:44:49.676546: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","plot_graph_and_save_audio:   0% 0/1 [00:00<?, ?it/s]findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12593 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12643 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12609 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12622 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12629 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12596 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12615 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12636 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12632 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12620 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44608 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54788 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50864 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54868 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54021 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12593 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12643 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12609 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12622 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12629 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12596 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12615 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12636 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12632 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12620 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44608 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54788 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50864 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54868 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54021 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron2/generate/2021-02-09_00-44-52.png\n","plot_graph_and_save_audio: 100% 1/1 [00:01<00:00,  1.36s/it]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8wMjVs4A6Na-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612797505173,"user_tz":-540,"elapsed":1578219,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"3133fd15-4279-4939-c063-50c31385c3da"},"source":["!python train_vocoder.py --data_dir ./data/son --logdir ./logdir-wavenet/train/2021-02-08T14-52-11"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron2/wavenet/ops.py:51: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From train_vocoder.py:24: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n","\n","WARNING:tensorflow:From train_vocoder.py:24: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n","\n","Using default logdir: ./logdir-wavenet/train/2021-02-08T14-52-11\n"," [*] MODEL dir: ./logdir-wavenet/train/2021-02-08T14-52-11\n"," [*] PARAM path: ./logdir-wavenet/train/2021-02-08T14-52-11/params.json\n","2021-02-08 14:52:33.239134: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n","2021-02-08 14:52:33.239416: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x638d880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-02-08 14:52:33.239453: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-02-08 14:52:33.241510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-02-08 14:52:33.252712: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2021-02-08 14:52:33.252770: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (71f3b5062c30): /proc/driver/nvidia/version does not exist\n","Trying to restore saved checkpoints from ./logdir-wavenet/train/2021-02-08T14-52-11 ... No checkpoint found.\n","step 1 - loss = 11.735, (55.667 sec/step)\n","step 2 - loss = 11.503, (17.349 sec/step)\n","Traceback (most recent call last):\n","  File \"train_vocoder.py\", line 315, in <module>\n","    main()\n","  File \"train_vocoder.py\", line 299, in main\n","    eval_step(sess,logdir,step,waveform,upsampled_local_condition_data_,speaker_id_test,mel_input_test,samples,speaker_id,upsampled_local_condition,next_sample)\n","  File \"train_vocoder.py\", line 38, in eval_step\n","    prediction = sess.run(next_sample, feed_dict={samples: window,upsampled_local_condition: upsampled_local_condition_data[:,step2,:],speaker_id: speaker_id_data })\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 956, in run\n","    run_metadata_ptr)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1180, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n","    run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n","    return fn(*args)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n","    target_list, run_metadata)\n","  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iugvMg-G6lCK"},"source":["!python generate.py --mel ./logdir-wavenet/mel-moon.npy --gc_cardinality 2 --gc_id 0 ./logdir-wavenet/train/2018-12-21T22-58-10"],"execution_count":null,"outputs":[]}]}