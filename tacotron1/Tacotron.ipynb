{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tacotron.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1A5quxI3KZ7wGxD9A9ZyTxxJc0WMRlwR1","authorship_tag":"ABX9TyOdjRe9szJjqpoJLVp7bL3n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HEJUZjDI2_9e","executionInfo":{"status":"ok","timestamp":1612370916704,"user_tz":-540,"elapsed":856,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"f80525d8-8e74-4634-d290-2b4c4a116d4b"},"source":["from google.colab import  drive \r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":134},"id":"Kd5_DCtb9ozH","executionInfo":{"status":"error","timestamp":1612339893326,"user_tz":-540,"elapsed":419,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"bcf401b4-cbba-4fd5-8a0c-da050bd2eed4"},"source":["function ClickConnect(){\r\n","    console.log(\"코랩 연결 끊김 방지\"); \r\n","    document.querySelector(\"colab-toolbar-button#connect\").click() \r\n","}\r\n","setInterval(ClickConnect, 60 * 1000)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-b32b2eaebf57>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    function ClickConnect(){\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","metadata":{"id":"WN-S_86dPsLP"},"source":["https://github.com/hccho2/Tacotron-Wavenet-Vocoder-Korean"]},{"cell_type":"markdown","metadata":{"id":"63JO1IRUA9xM"},"source":["File \"C:\\Users\\user\\Desktop\\deep\\synthesizer.py\", line 294, in get_most_recent_checkpoint\r\n","    max_idx = max(idxes)\r\n","ValueError: max() arg is an empty sequence\r\n","\r\n","위와 같은 오류시 \r\n","train_tacotron.py 파일에서 270줄에 batch_size의 default값을 32에서 16으로 줄인다.\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"UoF8N0oI3WlK","executionInfo":{"status":"ok","timestamp":1612370919862,"user_tz":-540,"elapsed":790,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"bd900cbc-8457-44e9-be19-72cad40b6b95"},"source":["import os \r\n","\r\n","os.getcwd()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYQGT6DF3oFL","executionInfo":{"status":"ok","timestamp":1612370919864,"user_tz":-540,"elapsed":453,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"2527295e-1aa4-4eb5-b87d-fb3e43f2282d"},"source":["cd drive/MyDrive/AI대학원/tacotron1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/AI대학원/tacotron1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJqdsJ5q3tHD","executionInfo":{"status":"ok","timestamp":1612370956341,"user_tz":-540,"elapsed":35089,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"767150c3-d587-46e5-dc91-25b3dfb79b42"},"source":["pip install tensorflow-gpu==1.8"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-gpu==1.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/41/ba6ac9b63c5bfb90377784e29c4f4c478c74f53e020fa56237c939674f2d/tensorflow_gpu-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (216.2MB)\n","\u001b[K     |████████████████████████████████| 216.3MB 75kB/s \n","\u001b[?25hCollecting tensorboard<1.9.0,>=1.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 46.9MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (1.15.0)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (0.10.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (1.1.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (0.3.3)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (0.8.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (1.32.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (1.19.5)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (0.36.2)\n","Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.8) (3.12.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (3.3.3)\n","Collecting bleach==1.5.0\n","  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (1.0.1)\n","Collecting html5lib==0.9999999\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n","\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-gpu==1.8) (53.0.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.9.0,>=1.8.0->tensorflow-gpu==1.8) (3.4.0)\n","Building wheels for collected packages: html5lib\n","  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107222 sha256=25a05ea0ec07485e194c0ce1e697377a6e46b422e88373114a2a6e027facdcf7\n","  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n","Successfully built html5lib\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.8.0 which is incompatible.\u001b[0m\n","Installing collected packages: html5lib, bleach, tensorboard, tensorflow-gpu\n","  Found existing installation: html5lib 1.0.1\n","    Uninstalling html5lib-1.0.1:\n","      Successfully uninstalled html5lib-1.0.1\n","  Found existing installation: bleach 3.2.3\n","    Uninstalling bleach-3.2.3:\n","      Successfully uninstalled bleach-3.2.3\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.8.0 tensorflow-gpu-1.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4NgEXSel3_7M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612370959219,"user_tz":-540,"elapsed":1965,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"d4d6e85e-b427-4673-d624-b5f4ebd029c3"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZZV6e1f4BBd","executionInfo":{"status":"ok","timestamp":1612370964015,"user_tz":-540,"elapsed":6307,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"97e3537d-7a1e-4dad-d419-f016da3c6ed0"},"source":["import tensorflow as tf\r\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.15.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dz2a9vB4CKm","executionInfo":{"status":"ok","timestamp":1612370969983,"user_tz":-540,"elapsed":5551,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"69267239-c1a3-47ed-9797-406336144dc4"},"source":["!pip install jamo\r\n","!pip install Unidecode"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting jamo\n","  Downloading https://files.pythonhosted.org/packages/ac/cc/49812faae67f9a24be6ddaf58a2cf7e8c3cbfcf5b762d9414f7103d2ea2c/jamo-0.4.1-py3-none-any.whl\n","Installing collected packages: jamo\n","Successfully installed jamo-0.4.1\n","Collecting Unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/65/91eab655041e9e92f948cb7302e54962035762ce7b518272ed9d6b269e93/Unidecode-1.1.2-py2.py3-none-any.whl (239kB)\n","\u001b[K     |████████████████████████████████| 245kB 18.8MB/s \n","\u001b[?25hInstalling collected packages: Unidecode\n","Successfully installed Unidecode-1.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E_KHeIti4FAw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612371028663,"user_tz":-540,"elapsed":55953,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"7b6ecfc0-2764-4cab-eb3b-3c8448dbbdd3"},"source":["!python preprocess.py --num_workers 8 --name moon --in_dir ./datasets/moon --out_dir ./data/moon"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","Hyperparameters:\n","  adam_beta1: 0.9\n","  adam_beta2: 0.999\n","  allow_clipping_in_normalization: True\n","  attention_size: 256\n","  attention_state_size: 256\n","  attention_type: bah_mon_norm\n","  cleaners: korean_cleaners\n","  clip_mels_length: True\n","  dec_layer_num: 2\n","  dec_prenet_sizes: [256, 128]\n","  dec_rnn_size: 256\n","  decay_learning_rate_mode: 0\n","  dilation_channels: 32\n","  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n","  dropout_prob: 0.5\n","  embedding_size: 256\n","  enc_bank_channel_size: 128\n","  enc_bank_size: 16\n","  enc_highway_depth: 4\n","  enc_maxpool_width: 2\n","  enc_prenet_sizes: [256, 128]\n","  enc_proj_sizes: [128, 128]\n","  enc_proj_width: 3\n","  enc_rnn_size: 128\n","  fft_size: 2048\n","  filter_width: 2\n","  gc_channels: 32\n","  griffin_lim_iters: 60\n","  hop_size: 300\n","  ignore_recognition_level: 0\n","  initial_data_greedy: True\n","  initial_filter_width: 32\n","  initial_phase_step: 8000\n","  input_type: raw\n","  l2_regularization_strength: 0\n","  main_data: ['']\n","  main_data_greedy_factor: 0\n","  max_abs_value: 4.0\n","  max_checkpoints: 3\n","  max_iters: 200\n","  max_mel_frames: 1000\n","  min_iters: 30\n","  min_level_db: -100\n","  min_tokens: 30\n","  model_type: single\n","  momentum: 0.9\n","  name: Tacotron-Wavenet-Vocoder\n","  num_mels: 80\n","  num_steps: 200000\n","  optimizer: adam\n","  out_channels: 30\n","  post_bank_channel_size: 128\n","  post_bank_size: 8\n","  post_highway_depth: 4\n","  post_maxpool_width: 2\n","  post_proj_sizes: [256, 80]\n","  post_proj_width: 3\n","  post_rnn_size: 128\n","  power: 1.5\n","  preemphasis: 0.97\n","  preemphasize: True\n","  prioritize_loss: False\n","  quantization_channels: 256\n","  recognition_loss_coeff: 0.2\n","  reduction_factor: 5\n","  ref_level_db: 20\n","  rescaling: True\n","  rescaling_max: 0.999\n","  residual_channels: 32\n","  sample_rate: 24000\n","  sample_size: 15000\n","  scalar_input: True\n","  signal_normalization: True\n","  silence_threshold: 0\n","  skip_channels: 512\n","  skip_inadequate: False\n","  skip_path_filter: False\n","  speaker_embedding_size: 16\n","  store_metadata: False\n","  symmetric_mels: True\n","  tacotron_initial_learning_rate: 0.001\n","  trim_fft_size: 512\n","  trim_hop_size: 128\n","  trim_silence: True\n","  trim_top_db: 23\n","  upsample_factor: [5, 5, 12]\n","  use_biases: True\n","  use_fixed_test_inputs: False\n","  use_lws: False\n","  wavenet_batch_size: 8\n","  wavenet_clip_gradients: False\n","  wavenet_decay_rate: 0.5\n","  wavenet_decay_steps: 300000\n","  wavenet_learning_rate: 0.001\n","  win_size: 1200\n","Sampling frequency: 24000\n","100% 110/110 [00:39<00:00,  2.79it/s]\n","Write 110 utterances, 13776 mel frames, 4132800 audio timesteps, (0.05 hours)\n","Max input length (text chars): 25\n","Max mel frames length: 340\n","Max audio timesteps length: 102000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rb5i-oo9e_ai","executionInfo":{"status":"ok","timestamp":1612365763634,"user_tz":-540,"elapsed":38295,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"0d7395d8-b16b-4ced-c74e-5581a0e5f2ff"},"source":["!python preprocess.py --num_workers 8 --name son --in_dir ./datasets/son --out_dir ./data/son"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","Hyperparameters:\n","  adam_beta1: 0.9\n","  adam_beta2: 0.999\n","  allow_clipping_in_normalization: True\n","  attention_size: 256\n","  attention_state_size: 256\n","  attention_type: bah_mon_norm\n","  cleaners: korean_cleaners\n","  clip_mels_length: True\n","  dec_layer_num: 2\n","  dec_prenet_sizes: [256, 128]\n","  dec_rnn_size: 256\n","  decay_learning_rate_mode: 0\n","  dilation_channels: 32\n","  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n","  dropout_prob: 0.5\n","  embedding_size: 256\n","  enc_bank_channel_size: 128\n","  enc_bank_size: 16\n","  enc_highway_depth: 4\n","  enc_maxpool_width: 2\n","  enc_prenet_sizes: [256, 128]\n","  enc_proj_sizes: [128, 128]\n","  enc_proj_width: 3\n","  enc_rnn_size: 128\n","  fft_size: 2048\n","  filter_width: 2\n","  gc_channels: 32\n","  griffin_lim_iters: 60\n","  hop_size: 300\n","  ignore_recognition_level: 0\n","  initial_data_greedy: True\n","  initial_filter_width: 32\n","  initial_phase_step: 8000\n","  input_type: raw\n","  l2_regularization_strength: 0\n","  main_data: ['']\n","  main_data_greedy_factor: 0\n","  max_abs_value: 4.0\n","  max_checkpoints: 3\n","  max_iters: 200\n","  max_mel_frames: 1000\n","  min_iters: 30\n","  min_level_db: -100\n","  min_tokens: 30\n","  model_type: single\n","  momentum: 0.9\n","  name: Tacotron-Wavenet-Vocoder\n","  num_mels: 80\n","  num_steps: 200000\n","  optimizer: adam\n","  out_channels: 30\n","  post_bank_channel_size: 128\n","  post_bank_size: 8\n","  post_highway_depth: 4\n","  post_maxpool_width: 2\n","  post_proj_sizes: [256, 80]\n","  post_proj_width: 3\n","  post_rnn_size: 128\n","  power: 1.5\n","  preemphasis: 0.97\n","  preemphasize: True\n","  prioritize_loss: False\n","  quantization_channels: 256\n","  recognition_loss_coeff: 0.2\n","  reduction_factor: 5\n","  ref_level_db: 20\n","  rescaling: True\n","  rescaling_max: 0.999\n","  residual_channels: 32\n","  sample_rate: 24000\n","  sample_size: 15000\n","  scalar_input: True\n","  signal_normalization: True\n","  silence_threshold: 0\n","  skip_channels: 512\n","  skip_inadequate: False\n","  skip_path_filter: False\n","  speaker_embedding_size: 16\n","  store_metadata: False\n","  symmetric_mels: True\n","  tacotron_initial_learning_rate: 0.001\n","  trim_fft_size: 512\n","  trim_hop_size: 128\n","  trim_silence: True\n","  trim_top_db: 23\n","  upsample_factor: [5, 5, 12]\n","  use_biases: True\n","  use_fixed_test_inputs: False\n","  use_lws: False\n","  wavenet_batch_size: 8\n","  wavenet_clip_gradients: False\n","  wavenet_decay_rate: 0.5\n","  wavenet_decay_steps: 300000\n","  wavenet_learning_rate: 0.001\n","  win_size: 1200\n","Sampling frequency: 24000\n","100% 50/50 [00:31<00:00,  1.59it/s]\n","Write 50 utterances, 15036 mel frames, 4510800 audio timesteps, (0.05 hours)\n","Max input length (text chars): 83\n","Max mel frames length: 827\n","Max audio timesteps length: 248100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L9p_lJLSFhC4"},"source":["python preprocess.py --num_workers 8 --name son --in_dir .\\datasets\\son --out_dir .\\data\\son"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-de4S_8jPp7"},"source":["!python train_tacotron.py --data_paths /data/moon --load_path ''"]},{"cell_type":"markdown","metadata":{"id":"ErIYPnCEkFng"},"source":["  File \"/content/drive/My Drive/AI대학원/tacotron1/datasets/datafeeder_tacotron.py\", line 57, in get_path_dict\r\n","    log(' [{}] Max length: {}'.format(data_dir, max(new_n_frames)))\r\n","ValueError: max() arg is an empty sequence\r\n","\r\n","위와 같은 오류가 생기면 \r\n","\r\n","train_tacotron.py파일에서 아래와 같이 32--> 16으로 바꿔줌\r\n","\r\n","\r\n","train_feeder = DataFeederTacotron(coord, data_dirs, hparams, config, 16,data_type='train', batch_size=config.batch_size)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qh_GOYQC4MgB","outputId":"f721467a-f5d3-4abb-f89d-e77a69e043c9"},"source":["!python train_tacotron.py --data_paths ./data/moon"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"," [*] MODEL dir: logdir-tacotron/moon_2021-02-03_23-08-12\n"," [*] PARAM path: logdir-tacotron/moon_2021-02-03_23-08-12/params.json\n","WARNING:tensorflow:From train_tacotron.py:290: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n","\n","['./data/moon']\n","========================================\n"," [!] Detect non-krbook dataset. May need to set sampling rate from 24000 to 20000\n","========================================\n","\n","==================================================\n","==================================================\n"," [*] Checkpoint path: logdir-tacotron/moon_2021-02-03_23-08-12/model.ckpt\n"," [*] Loading training data from: ['./data/moon']\n"," [*] Using model: logdir-tacotron/moon_2021-02-03_23-08-12\n","Hyperparameters:\n","  adam_beta1: 0.9\n","  adam_beta2: 0.999\n","  allow_clipping_in_normalization: True\n","  attention_size: 256\n","  attention_state_size: 256\n","  attention_type: bah_mon_norm\n","  cleaners: korean_cleaners\n","  clip_mels_length: True\n","  dec_layer_num: 2\n","  dec_prenet_sizes: [256, 128]\n","  dec_rnn_size: 256\n","  decay_learning_rate_mode: 0\n","  dilation_channels: 32\n","  dilations: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n","  dropout_prob: 0.5\n","  embedding_size: 256\n","  enc_bank_channel_size: 128\n","  enc_bank_size: 16\n","  enc_highway_depth: 4\n","  enc_maxpool_width: 2\n","  enc_prenet_sizes: [256, 128]\n","  enc_proj_sizes: [128, 128]\n","  enc_proj_width: 3\n","  enc_rnn_size: 128\n","  fft_size: 2048\n","  filter_width: 2\n","  gc_channels: 32\n","  griffin_lim_iters: 60\n","  hop_size: 300\n","  ignore_recognition_level: 0\n","  initial_data_greedy: True\n","  initial_filter_width: 32\n","  initial_phase_step: 8000\n","  input_type: raw\n","  l2_regularization_strength: 0\n","  main_data: ['']\n","  main_data_greedy_factor: 0\n","  max_abs_value: 4.0\n","  max_checkpoints: 3\n","  max_iters: 200\n","  max_mel_frames: 1000\n","  min_iters: 30\n","  min_level_db: -100\n","  min_tokens: 30\n","  model_type: single\n","  momentum: 0.9\n","  name: Tacotron-Wavenet-Vocoder\n","  num_mels: 80\n","  num_steps: 200000\n","  optimizer: adam\n","  out_channels: 30\n","  post_bank_channel_size: 128\n","  post_bank_size: 8\n","  post_highway_depth: 4\n","  post_maxpool_width: 2\n","  post_proj_sizes: [256, 80]\n","  post_proj_width: 3\n","  post_rnn_size: 128\n","  power: 1.5\n","  preemphasis: 0.97\n","  preemphasize: True\n","  prioritize_loss: False\n","  quantization_channels: 256\n","  recognition_loss_coeff: 0.2\n","  reduction_factor: 5\n","  ref_level_db: 20\n","  rescaling: True\n","  rescaling_max: 0.999\n","  residual_channels: 32\n","  sample_rate: 24000\n","  sample_size: 15000\n","  scalar_input: True\n","  signal_normalization: True\n","  silence_threshold: 0\n","  skip_channels: 512\n","  skip_inadequate: False\n","  skip_path_filter: False\n","  speaker_embedding_size: 16\n","  store_metadata: False\n","  symmetric_mels: True\n","  tacotron_initial_learning_rate: 0.001\n","  trim_fft_size: 512\n","  trim_hop_size: 128\n","  trim_silence: True\n","  trim_top_db: 23\n","  upsample_factor: [5, 5, 12]\n","  use_biases: True\n","  use_fixed_test_inputs: False\n","  use_lws: False\n","  wavenet_batch_size: 8\n","  wavenet_clip_gradients: False\n","  wavenet_decay_rate: 0.5\n","  wavenet_decay_steps: 300000\n","  wavenet_learning_rate: 0.001\n","  win_size: 1200\n","WARNING:tensorflow:From train_tacotron.py:134: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","filter_by_min_max_frame_batch: 100% 110/110 [00:00<00:00, 348.90it/s]\n"," [./data/moon] Loaded metadata for 34 examples (0.02 hours)\n"," [./data/moon] Max length: 340\n"," [./data/moon] Min length: 152\n","========================================\n","{'./data/moon': 1.0}\n","========================================\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/datasets/datafeeder_tacotron.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/datasets/datafeeder_tacotron.py:142: The name tf.FIFOQueue is deprecated. Please use tf.queue.FIFOQueue instead.\n","\n","filter_by_min_max_frame_batch: 100% 110/110 [00:00<00:00, 332.70it/s]\n"," [./data/moon] Loaded metadata for 34 examples (0.02 hours)\n"," [./data/moon] Max length: 340\n"," [./data/moon] Min length: 152\n","========================================\n","{'./data/moon': 1.0}\n","========================================\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/tacotron.py:51: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/modules.py:21: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/modules.py:22: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dropout instead.\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/modules.py:95: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.keras.layers.Conv1D` instead.\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/modules.py:96: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/modules.py:38: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.MaxPooling1D instead.\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/modules.py:71: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/modules.py:73: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/tacotron.py:173: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/rnn_wrappers.py:184: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n","\n","WARNING:tensorflow:Entity <bound method ConcatOutputAndAttentionWrapper.call of <tacotron.rnn_wrappers.ConcatOutputAndAttentionWrapper object at 0x7f7e63c286d8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method DecoderPrenetWrapper.call of <tacotron.rnn_wrappers.DecoderPrenetWrapper object at 0x7f7e63c89550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tacotron.rnn_wrappers.AttentionWrapper object at 0x7f7e63c93128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","========================================\n"," model_type: single\n","========================================\n","Initialized Tacotron model. Dimensions: \n","    embedding:                256\n","    speaker embedding:        None\n","    prenet out:               128\n","    encoder out:              256\n","    attention out:            256\n","    concat attn & out:        512\n","    decoder cell out:         256\n","    decoder out (5 frames):  400\n","    decoder out (1 frame):    80\n","    postnet out:              256\n","    linear out:               1025\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/tacotron.py:305: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/tacotron.py:312: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/AI대학원/tacotron1/tacotron/tacotron.py:312: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From train_tacotron.py:57: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n","WARNING:tensorflow:From train_tacotron.py:81: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n","\n","WARNING:tensorflow:Entity <bound method ConcatOutputAndAttentionWrapper.call of <tacotron.rnn_wrappers.ConcatOutputAndAttentionWrapper object at 0x7f7e5f6fd438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method DecoderPrenetWrapper.call of <tacotron.rnn_wrappers.DecoderPrenetWrapper object at 0x7f7e5f6f5f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tacotron.rnn_wrappers.AttentionWrapper object at 0x7f7e5f6e54a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","========================================\n"," model_type: single\n","========================================\n","Initialized Tacotron model. Dimensions: \n","    embedding:                256\n","    speaker embedding:        None\n","    prenet out:               128\n","    encoder out:              256\n","    attention out:            256\n","    concat attn & out:        512\n","    decoder cell out:         256\n","    decoder out (5 frames):  400\n","    decoder out (1 frame):    80\n","    postnet out:              256\n","    linear out:               1025\n","WARNING:tensorflow:From train_tacotron.py:165: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","WARNING:tensorflow:From train_tacotron.py:167: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From train_tacotron.py:172: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","2021-02-03 23:08:24.622807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-02-03 23:08:24.659106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 23:08:24.659683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n","pciBusID: 0000:00:04.0\n","2021-02-03 23:08:24.659971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-02-03 23:08:24.661728: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-02-03 23:08:24.663985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-02-03 23:08:24.664441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-02-03 23:08:24.666398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-02-03 23:08:24.667595: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-02-03 23:08:24.671709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-02-03 23:08:24.671838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 23:08:24.672435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 23:08:24.672949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-02-03 23:08:24.678117: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n","2021-02-03 23:08:24.678331: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x96172c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-02-03 23:08:24.678359: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-02-03 23:08:24.779871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 23:08:24.780538: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9617480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-02-03 23:08:24.780572: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2021-02-03 23:08:24.780841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 23:08:24.781419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n","pciBusID: 0000:00:04.0\n","2021-02-03 23:08:24.781501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-02-03 23:08:24.781533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-02-03 23:08:24.781557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-02-03 23:08:24.781580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-02-03 23:08:24.781602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-02-03 23:08:24.781623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-02-03 23:08:24.781645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-02-03 23:08:24.781734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 23:08:24.782289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 23:08:24.782784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-02-03 23:08:24.782862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-02-03 23:08:24.784076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-02-03 23:08:24.784102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-02-03 23:08:24.784112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-02-03 23:08:24.784228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 23:08:24.784836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 23:08:24.785322: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2021-02-03 23:08:24.785417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n","WARNING:tensorflow:From train_tacotron.py:174: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","WARNING:tensorflow:From train_tacotron.py:175: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","Starting new training run at commit: None\n","Generated 8 batches of size 2 in 0.000 sec\n","Generated 16 batches of size 16 in 1.707 sec\n","2021-02-03 23:08:34.507918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-02-03 23:08:35.068315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","Step 1       [9.947 sec/step, loss=2.03722, avg_loss=2.03722]\n","Step 2       [5.735 sec/step, loss=1.89763, avg_loss=1.96743]\n","Step 3       [4.349 sec/step, loss=1.91586, avg_loss=1.95024]\n","Step 4       [3.793 sec/step, loss=1.89091, avg_loss=1.93541]\n","Step 5       [3.706 sec/step, loss=1.89475, avg_loss=1.92727]\n","Step 6       [3.339 sec/step, loss=1.92739, avg_loss=1.92729]\n","Step 7       [2.974 sec/step, loss=2.02470, avg_loss=1.94121]\n","Generated 16 batches of size 16 in 1.986 sec\n","Step 8       [2.900 sec/step, loss=1.92498, avg_loss=1.93918]\n","Step 9       [2.720 sec/step, loss=2.02408, avg_loss=1.94861]\n","Step 10      [2.607 sec/step, loss=1.95706, avg_loss=1.94946]\n","Step 11      [2.431 sec/step, loss=1.96801, avg_loss=1.95114]\n","Step 12      [2.357 sec/step, loss=1.98291, avg_loss=1.95379]\n","Step 13      [2.217 sec/step, loss=2.02468, avg_loss=1.95924]\n","Step 14      [2.100 sec/step, loss=2.09890, avg_loss=1.96922]\n","Step 15      [1.995 sec/step, loss=1.95100, avg_loss=1.96800]\n","Step 16      [1.933 sec/step, loss=1.88506, avg_loss=1.96282]\n","Step 17      [1.850 sec/step, loss=2.02377, avg_loss=1.96641]\n","Step 18      [1.789 sec/step, loss=1.89535, avg_loss=1.96246]\n","Step 19      [1.720 sec/step, loss=1.87846, avg_loss=1.95804]\n","Step 20      [1.660 sec/step, loss=1.91309, avg_loss=1.95579]\n","Step 21      [1.609 sec/step, loss=2.02462, avg_loss=1.95907]\n","Step 22      [1.562 sec/step, loss=2.08353, avg_loss=1.96472]\n","Step 23      [1.516 sec/step, loss=1.99524, avg_loss=1.96605]\n","Step 24      [1.505 sec/step, loss=1.92134, avg_loss=1.96419]\n","Step 25      [1.476 sec/step, loss=1.91627, avg_loss=1.96227]\n","Generated 16 batches of size 16 in 2.248 sec\n","Step 26      [1.446 sec/step, loss=1.86444, avg_loss=1.95851]\n","Step 27      [1.411 sec/step, loss=1.95063, avg_loss=1.95822]\n","Step 28      [1.385 sec/step, loss=1.94242, avg_loss=1.95765]\n","Step 29      [1.371 sec/step, loss=1.86965, avg_loss=1.95462]\n","Step 30      [1.344 sec/step, loss=1.99658, avg_loss=1.95602]\n","Step 31      [1.317 sec/step, loss=1.93830, avg_loss=1.95545]\n","Step 32      [1.293 sec/step, loss=1.91849, avg_loss=1.95429]\n","Step 33      [1.272 sec/step, loss=2.00094, avg_loss=1.95570]\n","Step 34      [1.263 sec/step, loss=1.88318, avg_loss=1.95357]\n","Step 35      [1.244 sec/step, loss=1.98446, avg_loss=1.95445]\n","Step 36      [1.223 sec/step, loss=1.92817, avg_loss=1.95372]\n","Step 37      [1.210 sec/step, loss=1.86279, avg_loss=1.95127]\n","Step 38      [1.191 sec/step, loss=1.84549, avg_loss=1.94848]\n","Step 39      [1.173 sec/step, loss=1.96794, avg_loss=1.94898]\n","Step 40      [1.168 sec/step, loss=1.87705, avg_loss=1.94718]\n","Generated 16 batches of size 16 in 2.077 sec\n","Step 41      [1.170 sec/step, loss=1.86911, avg_loss=1.94528]\n","Step 42      [1.158 sec/step, loss=1.91476, avg_loss=1.94455]\n","Step 43      [1.145 sec/step, loss=1.83734, avg_loss=1.94206]\n","Step 44      [1.132 sec/step, loss=2.04520, avg_loss=1.94440]\n","Step 45      [1.120 sec/step, loss=1.89046, avg_loss=1.94320]\n","Step 46      [1.107 sec/step, loss=1.91191, avg_loss=1.94252]\n","Step 47      [1.094 sec/step, loss=1.97715, avg_loss=1.94326]\n","Step 48      [1.081 sec/step, loss=1.86749, avg_loss=1.94168]\n","Step 49      [1.072 sec/step, loss=1.89997, avg_loss=1.94083]\n","Step 50      [1.063 sec/step, loss=1.88385, avg_loss=1.93969]\n","Step 51      [1.052 sec/step, loss=1.84028, avg_loss=1.93774]\n","Step 52      [1.041 sec/step, loss=1.86107, avg_loss=1.93627]\n","Step 53      [1.034 sec/step, loss=1.85132, avg_loss=1.93466]\n","Step 54      [1.026 sec/step, loss=2.03263, avg_loss=1.93648]\n","Step 55      [1.018 sec/step, loss=1.95626, avg_loss=1.93684]\n","Step 56      [1.019 sec/step, loss=1.93162, avg_loss=1.93675]\n","Generated 16 batches of size 16 in 2.026 sec\n","Step 57      [1.019 sec/step, loss=1.91489, avg_loss=1.93636]\n","Step 58      [1.010 sec/step, loss=1.79971, avg_loss=1.93401]\n","Step 59      [1.002 sec/step, loss=1.93601, avg_loss=1.93404]\n","Step 60      [1.002 sec/step, loss=1.81179, avg_loss=1.93200]\n","Step 61      [0.995 sec/step, loss=1.77922, avg_loss=1.92950]\n","Step 62      [0.986 sec/step, loss=1.92330, avg_loss=1.92940]\n","Step 63      [0.983 sec/step, loss=1.79613, avg_loss=1.92728]\n","Step 64      [0.976 sec/step, loss=1.85249, avg_loss=1.92611]\n","Step 65      [0.968 sec/step, loss=1.92976, avg_loss=1.92617]\n","Step 66      [0.961 sec/step, loss=1.89472, avg_loss=1.92569]\n","Step 67      [0.956 sec/step, loss=1.76993, avg_loss=1.92337]\n","Step 68      [0.950 sec/step, loss=1.84534, avg_loss=1.92222]\n","Step 69      [0.947 sec/step, loss=1.78214, avg_loss=1.92019]\n","Step 70      [0.942 sec/step, loss=1.92497, avg_loss=1.92026]\n","Step 71      [0.937 sec/step, loss=1.99826, avg_loss=1.92136]\n","Step 72      [0.938 sec/step, loss=1.76082, avg_loss=1.91913]\n","Generated 16 batches of size 16 in 2.078 sec\n","Step 73      [0.942 sec/step, loss=1.81127, avg_loss=1.91765]\n","Step 74      [0.942 sec/step, loss=1.77980, avg_loss=1.91579]\n","Step 75      [0.937 sec/step, loss=1.82589, avg_loss=1.91459]\n","Step 76      [0.931 sec/step, loss=1.78380, avg_loss=1.91287]\n","Step 77      [0.928 sec/step, loss=1.84052, avg_loss=1.91193]\n","Step 78      [0.922 sec/step, loss=1.78398, avg_loss=1.91029]\n","Step 79      [0.918 sec/step, loss=1.87360, avg_loss=1.90982]\n","Step 80      [0.914 sec/step, loss=1.88214, avg_loss=1.90948]\n","Step 81      [0.909 sec/step, loss=1.72005, avg_loss=1.90714]\n","Step 82      [0.908 sec/step, loss=1.74441, avg_loss=1.90515]\n","Step 83      [0.902 sec/step, loss=1.86017, avg_loss=1.90461]\n","Step 84      [0.898 sec/step, loss=1.81447, avg_loss=1.90354]\n","Step 85      [0.893 sec/step, loss=1.79161, avg_loss=1.90222]\n","Step 86      [0.889 sec/step, loss=1.79666, avg_loss=1.90100]\n","Step 87      [0.884 sec/step, loss=1.71563, avg_loss=1.89886]\n","Step 88      [0.886 sec/step, loss=1.85402, avg_loss=1.89836]\n","Generated 16 batches of size 16 in 2.157 sec\n","Step 89      [0.889 sec/step, loss=1.87103, avg_loss=1.89805]\n","Step 90      [0.890 sec/step, loss=1.73141, avg_loss=1.89620]\n","Step 91      [0.886 sec/step, loss=1.94251, avg_loss=1.89671]\n","Step 92      [0.883 sec/step, loss=1.73811, avg_loss=1.89498]\n","Step 93      [0.880 sec/step, loss=1.75160, avg_loss=1.89344]\n","Step 94      [0.876 sec/step, loss=1.78395, avg_loss=1.89228]\n","Step 95      [0.874 sec/step, loss=1.78717, avg_loss=1.89117]\n","Step 96      [0.871 sec/step, loss=1.82316, avg_loss=1.89046]\n","Step 97      [0.868 sec/step, loss=1.83849, avg_loss=1.88992]\n","Step 98      [0.864 sec/step, loss=1.81466, avg_loss=1.88916]\n","Step 99      [0.862 sec/step, loss=1.73286, avg_loss=1.88758]\n","Step 100     [0.859 sec/step, loss=1.79355, avg_loss=1.88664]\n","Step 101     [0.769 sec/step, loss=1.70756, avg_loss=1.88334]\n","Step 102     [0.759 sec/step, loss=1.77109, avg_loss=1.88208]\n","Step 103     [0.751 sec/step, loss=1.67012, avg_loss=1.87962]\n","Step 104     [0.741 sec/step, loss=1.71845, avg_loss=1.87789]\n","Generated 16 batches of size 16 in 2.022 sec\n","Step 105     [0.718 sec/step, loss=1.61685, avg_loss=1.87511]\n","Step 106     [0.708 sec/step, loss=1.61954, avg_loss=1.87204]\n","Step 107     [0.705 sec/step, loss=1.66771, avg_loss=1.86847]\n","Step 108     [0.687 sec/step, loss=1.88105, avg_loss=1.86803]\n","Step 109     [0.679 sec/step, loss=1.66567, avg_loss=1.86444]\n","Step 110     [0.669 sec/step, loss=1.80053, avg_loss=1.86288]\n","Step 111     [0.667 sec/step, loss=1.66604, avg_loss=1.85986]\n","Step 112     [0.658 sec/step, loss=1.70721, avg_loss=1.85710]\n","Step 113     [0.661 sec/step, loss=1.61365, avg_loss=1.85299]\n","Step 114     [0.660 sec/step, loss=1.67634, avg_loss=1.84876]\n","Step 115     [0.664 sec/step, loss=1.60405, avg_loss=1.84530]\n","Step 116     [0.659 sec/step, loss=1.55024, avg_loss=1.84195]\n","Step 117     [0.659 sec/step, loss=1.81519, avg_loss=1.83986]\n","Step 118     [0.657 sec/step, loss=1.63867, avg_loss=1.83729]\n","Step 119     [0.658 sec/step, loss=1.54824, avg_loss=1.83399]\n","Step 120     [0.661 sec/step, loss=1.61191, avg_loss=1.83098]\n","Step 121     [0.666 sec/step, loss=1.69551, avg_loss=1.82769]\n","Generated 16 batches of size 16 in 2.146 sec\n","Step 122     [0.667 sec/step, loss=1.58549, avg_loss=1.82271]\n","Step 123     [0.669 sec/step, loss=1.64015, avg_loss=1.81916]\n","Step 124     [0.663 sec/step, loss=1.62324, avg_loss=1.81618]\n","Step 125     [0.661 sec/step, loss=1.66692, avg_loss=1.81368]\n","Step 126     [0.659 sec/step, loss=1.56677, avg_loss=1.81071]\n","Step 127     [0.659 sec/step, loss=1.65840, avg_loss=1.80778]\n","Step 128     [0.658 sec/step, loss=1.67608, avg_loss=1.80512]\n","Step 129     [0.654 sec/step, loss=1.54260, avg_loss=1.80185]\n","Step 130     [0.654 sec/step, loss=1.62270, avg_loss=1.79811]\n","Step 131     [0.655 sec/step, loss=1.73077, avg_loss=1.79604]\n","Step 132     [0.654 sec/step, loss=1.51297, avg_loss=1.79198]\n","Step 133     [0.656 sec/step, loss=1.48677, avg_loss=1.78684]\n","Step 134     [0.652 sec/step, loss=1.55362, avg_loss=1.78354]\n","Step 135     [0.652 sec/step, loss=1.62705, avg_loss=1.77997]\n","Step 136     [0.660 sec/step, loss=1.55687, avg_loss=1.77626]\n","Generated 16 batches of size 16 in 2.010 sec\n","Step 137     [0.661 sec/step, loss=1.59080, avg_loss=1.77354]\n","Step 138     [0.666 sec/step, loss=1.43603, avg_loss=1.76944]\n","Step 139     [0.667 sec/step, loss=1.40970, avg_loss=1.76386]\n","Step 140     [0.663 sec/step, loss=1.58418, avg_loss=1.76093]\n","Step 141     [0.657 sec/step, loss=1.52229, avg_loss=1.75746]\n","Step 142     [0.655 sec/step, loss=1.47185, avg_loss=1.75303]\n","Step 143     [0.654 sec/step, loss=1.50777, avg_loss=1.74974]\n","Step 144     [0.654 sec/step, loss=1.34260, avg_loss=1.74271]\n","Step 145     [0.653 sec/step, loss=1.49410, avg_loss=1.73875]\n","Step 146     [0.654 sec/step, loss=1.49225, avg_loss=1.73455]\n","Step 147     [0.659 sec/step, loss=1.36814, avg_loss=1.72846]\n","Step 148     [0.659 sec/step, loss=1.52274, avg_loss=1.72501]\n","Step 149     [0.659 sec/step, loss=1.49664, avg_loss=1.72098]\n","Step 150     [0.659 sec/step, loss=1.50138, avg_loss=1.71716]\n","Step 151     [0.659 sec/step, loss=1.33681, avg_loss=1.71212]\n","Step 152     [0.665 sec/step, loss=1.48988, avg_loss=1.70841]\n","Step 153     [0.669 sec/step, loss=1.52552, avg_loss=1.70515]\n","Generated 16 batches of size 16 in 2.198 sec\n","Step 154     [0.671 sec/step, loss=1.37462, avg_loss=1.69857]\n","Step 155     [0.671 sec/step, loss=1.38249, avg_loss=1.69283]\n","Step 156     [0.665 sec/step, loss=1.26147, avg_loss=1.68613]\n","Step 157     [0.661 sec/step, loss=1.58807, avg_loss=1.68286]\n","Step 158     [0.661 sec/step, loss=1.38423, avg_loss=1.67871]\n","Step 159     [0.661 sec/step, loss=1.35181, avg_loss=1.67287]\n","Step 160     [0.657 sec/step, loss=1.37986, avg_loss=1.66855]\n","Step 161     [0.656 sec/step, loss=1.35603, avg_loss=1.66432]\n","Step 162     [0.656 sec/step, loss=1.45085, avg_loss=1.65959]\n","Step 163     [0.655 sec/step, loss=1.30445, avg_loss=1.65468]\n","Step 164     [0.655 sec/step, loss=1.32711, avg_loss=1.64942]\n","Step 165     [0.656 sec/step, loss=1.37081, avg_loss=1.64383]\n","Step 166     [0.660 sec/step, loss=1.27103, avg_loss=1.63760]\n","Step 167     [0.661 sec/step, loss=1.41654, avg_loss=1.63406]\n","Step 168     [0.666 sec/step, loss=1.51445, avg_loss=1.63075]\n","Step 169     [0.667 sec/step, loss=1.38673, avg_loss=1.62680]\n","Generated 16 batches of size 16 in 2.119 sec\n","Step 170     [0.668 sec/step, loss=1.48087, avg_loss=1.62236]\n","Step 171     [0.667 sec/step, loss=1.18950, avg_loss=1.61427]\n","Step 172     [0.663 sec/step, loss=1.37055, avg_loss=1.61037]\n","Step 173     [0.659 sec/step, loss=1.30973, avg_loss=1.60535]\n","Step 174     [0.655 sec/step, loss=1.46909, avg_loss=1.60224]\n","Step 175     [0.655 sec/step, loss=1.42510, avg_loss=1.59824]\n","Step 176     [0.655 sec/step, loss=1.31948, avg_loss=1.59359]\n","Step 177     [0.654 sec/step, loss=1.44622, avg_loss=1.58965]\n","Step 178     [0.656 sec/step, loss=1.35741, avg_loss=1.58538]\n","Step 179     [0.656 sec/step, loss=1.24039, avg_loss=1.57905]\n","Step 180     [0.656 sec/step, loss=1.29814, avg_loss=1.57321]\n","Step 181     [0.657 sec/step, loss=1.38744, avg_loss=1.56989]\n","Step 182     [0.655 sec/step, loss=1.49609, avg_loss=1.56740]\n","Step 183     [0.656 sec/step, loss=1.25442, avg_loss=1.56135]\n","Step 184     [0.661 sec/step, loss=1.37937, avg_loss=1.55699]\n","Generated 16 batches of size 16 in 2.034 sec\n","Step 185     [0.668 sec/step, loss=1.43083, avg_loss=1.55339]\n","Step 186     [0.667 sec/step, loss=1.31650, avg_loss=1.54859]\n","Step 187     [0.667 sec/step, loss=1.32596, avg_loss=1.54469]\n","Step 188     [0.665 sec/step, loss=1.27210, avg_loss=1.53887]\n","Step 189     [0.658 sec/step, loss=1.39169, avg_loss=1.53408]\n","Step 190     [0.658 sec/step, loss=1.20349, avg_loss=1.52880]\n","Step 191     [0.658 sec/step, loss=1.27595, avg_loss=1.52213]\n","Step 192     [0.657 sec/step, loss=1.14320, avg_loss=1.51618]\n","Step 193     [0.656 sec/step, loss=1.27895, avg_loss=1.51146]\n","Step 194     [0.657 sec/step, loss=1.31673, avg_loss=1.50678]\n","Step 195     [0.657 sec/step, loss=1.20729, avg_loss=1.50098]\n","Step 196     [0.656 sec/step, loss=1.24431, avg_loss=1.49520]\n","Step 197     [0.656 sec/step, loss=1.39867, avg_loss=1.49080]\n","Step 198     [0.657 sec/step, loss=1.46789, avg_loss=1.48733]\n","Step 199     [0.660 sec/step, loss=1.17964, avg_loss=1.48180]\n","Step 200     [0.662 sec/step, loss=1.34396, avg_loss=1.47730]\n","Step 201     [0.665 sec/step, loss=1.34053, avg_loss=1.47363]\n","Generated 16 batches of size 16 in 2.139 sec\n","Step 202     [0.666 sec/step, loss=1.34061, avg_loss=1.46933]\n","Step 203     [0.666 sec/step, loss=1.25732, avg_loss=1.46520]\n","Step 204     [0.660 sec/step, loss=1.30107, avg_loss=1.46103]\n","Step 205     [0.655 sec/step, loss=1.21391, avg_loss=1.45700]\n","Step 206     [0.656 sec/step, loss=1.29104, avg_loss=1.45371]\n","Step 207     [0.657 sec/step, loss=1.38900, avg_loss=1.45092]\n","Step 208     [0.656 sec/step, loss=1.11268, avg_loss=1.44324]\n","Step 209     [0.656 sec/step, loss=1.11621, avg_loss=1.43775]\n","Step 210     [0.656 sec/step, loss=1.32463, avg_loss=1.43299]\n","Step 211     [0.655 sec/step, loss=1.24768, avg_loss=1.42880]\n","Step 212     [0.655 sec/step, loss=1.39235, avg_loss=1.42565]\n","Step 213     [0.653 sec/step, loss=1.21224, avg_loss=1.42164]\n","Step 214     [0.653 sec/step, loss=1.18677, avg_loss=1.41674]\n","Step 215     [0.650 sec/step, loss=1.42904, avg_loss=1.41499]\n","Generated 16 batches of size 16 in 1.945 sec\n","Step 216     [0.665 sec/step, loss=1.14896, avg_loss=1.41098]\n","Step 217     [0.666 sec/step, loss=1.27734, avg_loss=1.40560]\n","Step 218     [0.668 sec/step, loss=1.23427, avg_loss=1.40156]\n","Step 219     [0.668 sec/step, loss=1.38503, avg_loss=1.39993]\n","Step 220     [0.667 sec/step, loss=1.30568, avg_loss=1.39686]\n","Step 221     [0.661 sec/step, loss=1.23762, avg_loss=1.39229]\n","Step 222     [0.659 sec/step, loss=1.31869, avg_loss=1.38962]\n","Step 223     [0.658 sec/step, loss=1.23858, avg_loss=1.38560]\n","Step 224     [0.656 sec/step, loss=1.28326, avg_loss=1.38220]\n","Step 225     [0.656 sec/step, loss=1.21141, avg_loss=1.37765]\n","Step 226     [0.657 sec/step, loss=1.35091, avg_loss=1.37549]\n","Step 227     [0.657 sec/step, loss=1.08925, avg_loss=1.36980]\n","Step 228     [0.658 sec/step, loss=1.29234, avg_loss=1.36596]\n","Step 229     [0.657 sec/step, loss=1.22492, avg_loss=1.36278]\n","Step 230     [0.657 sec/step, loss=1.18613, avg_loss=1.35842]\n","Step 231     [0.657 sec/step, loss=1.26686, avg_loss=1.35378]\n","Step 232     [0.662 sec/step, loss=1.35220, avg_loss=1.35217]\n","Step 233     [0.665 sec/step, loss=1.29863, avg_loss=1.35029]\n","Generated 16 batches of size 16 in 2.140 sec\n","Step 234     [0.665 sec/step, loss=1.30198, avg_loss=1.34777]\n","Step 235     [0.665 sec/step, loss=1.41791, avg_loss=1.34568]\n","Step 236     [0.659 sec/step, loss=1.21197, avg_loss=1.34223]\n","Step 237     [0.661 sec/step, loss=1.13164, avg_loss=1.33764]\n","Step 238     [0.658 sec/step, loss=1.25217, avg_loss=1.33580]\n","Step 239     [0.658 sec/step, loss=1.16457, avg_loss=1.33335]\n","Step 240     [0.657 sec/step, loss=1.23515, avg_loss=1.32986]\n","Step 241     [0.657 sec/step, loss=1.27612, avg_loss=1.32740]\n","Step 242     [0.657 sec/step, loss=1.08025, avg_loss=1.32348]\n","Step 243     [0.658 sec/step, loss=1.34491, avg_loss=1.32185]\n","Step 244     [0.659 sec/step, loss=1.15030, avg_loss=1.31993]\n","Step 245     [0.659 sec/step, loss=1.18736, avg_loss=1.31686]\n","Step 246     [0.658 sec/step, loss=1.39568, avg_loss=1.31590]\n","Step 247     [0.654 sec/step, loss=1.25012, avg_loss=1.31472]\n","Step 248     [0.667 sec/step, loss=1.12676, avg_loss=1.31076]\n","Generated 16 batches of size 16 in 2.083 sec\n","Step 249     [0.669 sec/step, loss=1.19651, avg_loss=1.30776]\n","Step 250     [0.671 sec/step, loss=1.24212, avg_loss=1.30516]\n","Step 251     [0.670 sec/step, loss=1.16656, avg_loss=1.30346]\n","Step 252     [0.666 sec/step, loss=1.34347, avg_loss=1.30200]\n","Step 253     [0.661 sec/step, loss=1.19409, avg_loss=1.29868]\n","Step 254     [0.657 sec/step, loss=1.31517, avg_loss=1.29809]\n","Step 255     [0.658 sec/step, loss=1.27554, avg_loss=1.29702]\n","Step 256     [0.658 sec/step, loss=1.22381, avg_loss=1.29664]\n","Step 257     [0.658 sec/step, loss=1.23411, avg_loss=1.29310]\n","Step 258     [0.659 sec/step, loss=1.33128, avg_loss=1.29257]\n","Step 259     [0.658 sec/step, loss=1.23747, avg_loss=1.29143]\n","Step 260     [0.661 sec/step, loss=1.18902, avg_loss=1.28952]\n","Step 261     [0.661 sec/step, loss=1.26878, avg_loss=1.28865]\n","Step 262     [0.663 sec/step, loss=1.26464, avg_loss=1.28679]\n","Step 263     [0.662 sec/step, loss=1.13735, avg_loss=1.28512]\n","Step 264     [0.669 sec/step, loss=1.37229, avg_loss=1.28557]\n","Generated 16 batches of size 16 in 2.122 sec\n","Step 265     [0.673 sec/step, loss=1.06384, avg_loss=1.28250]\n","Step 266     [0.669 sec/step, loss=1.15908, avg_loss=1.28138]\n","Step 267     [0.667 sec/step, loss=1.17924, avg_loss=1.27901]\n","Step 268     [0.662 sec/step, loss=1.25531, avg_loss=1.27641]\n","Step 269     [0.659 sec/step, loss=1.32050, avg_loss=1.27575]\n","Step 270     [0.662 sec/step, loss=1.13059, avg_loss=1.27225]\n","Step 271     [0.662 sec/step, loss=1.21917, avg_loss=1.27255]\n","Step 272     [0.661 sec/step, loss=1.17883, avg_loss=1.27063]\n","Step 273     [0.659 sec/step, loss=1.16496, avg_loss=1.26918]\n","Step 274     [0.658 sec/step, loss=1.24776, avg_loss=1.26697]\n","Step 275     [0.659 sec/step, loss=1.14785, avg_loss=1.26419]\n","Step 276     [0.659 sec/step, loss=1.22542, avg_loss=1.26325]\n","Step 277     [0.659 sec/step, loss=1.31393, avg_loss=1.26193]\n","Step 278     [0.658 sec/step, loss=1.37303, avg_loss=1.26209]\n","Step 279     [0.657 sec/step, loss=1.20788, avg_loss=1.26176]\n","Step 280     [0.661 sec/step, loss=1.17507, avg_loss=1.26053]\n","Generated 16 batches of size 16 in 2.206 sec\n","Step 281     [0.668 sec/step, loss=1.16638, avg_loss=1.25832]\n","Step 282     [0.668 sec/step, loss=1.24165, avg_loss=1.25578]\n","Step 283     [0.668 sec/step, loss=1.12912, avg_loss=1.25452]\n","Step 284     [0.665 sec/step, loss=1.21238, avg_loss=1.25285]\n","Step 285     [0.658 sec/step, loss=1.05677, avg_loss=1.24911]\n","Step 286     [0.663 sec/step, loss=1.09291, avg_loss=1.24688]\n","Step 287     [0.663 sec/step, loss=1.24961, avg_loss=1.24611]\n","Step 288     [0.661 sec/step, loss=1.30774, avg_loss=1.24647]\n","Step 289     [0.662 sec/step, loss=1.12080, avg_loss=1.24376]\n","Step 290     [0.657 sec/step, loss=1.17010, avg_loss=1.24343]\n","Step 291     [0.658 sec/step, loss=1.12601, avg_loss=1.24193]\n","Step 292     [0.658 sec/step, loss=1.15367, avg_loss=1.24203]\n","Step 293     [0.661 sec/step, loss=1.15383, avg_loss=1.24078]\n","Step 294     [0.660 sec/step, loss=1.22191, avg_loss=1.23983]\n","Step 295     [0.660 sec/step, loss=1.22821, avg_loss=1.24004]\n","Step 296     [0.667 sec/step, loss=1.36265, avg_loss=1.24123]\n","Generated 16 batches of size 16 in 2.167 sec\n","Step 297     [0.676 sec/step, loss=1.08697, avg_loss=1.23811]\n","Step 298     [0.675 sec/step, loss=1.14894, avg_loss=1.23492]\n","Step 299     [0.670 sec/step, loss=1.04535, avg_loss=1.23358]\n","Step 300     [0.667 sec/step, loss=1.23985, avg_loss=1.23254]\n","Step 301     [0.662 sec/step, loss=1.19237, avg_loss=1.23105]\n","Step 302     [0.661 sec/step, loss=1.22477, avg_loss=1.22990]\n","Step 303     [0.659 sec/step, loss=1.28564, avg_loss=1.23018]\n","Step 304     [0.660 sec/step, loss=1.28543, avg_loss=1.23002]\n","Step 305     [0.660 sec/step, loss=1.11607, avg_loss=1.22904]\n","Step 306     [0.661 sec/step, loss=1.19412, avg_loss=1.22807]\n","Step 307     [0.662 sec/step, loss=1.20055, avg_loss=1.22619]\n","Step 308     [0.662 sec/step, loss=1.13990, avg_loss=1.22646]\n","Step 309     [0.663 sec/step, loss=1.32698, avg_loss=1.22857]\n","Step 310     [0.664 sec/step, loss=1.21055, avg_loss=1.22743]\n","Step 311     [0.666 sec/step, loss=1.15096, avg_loss=1.22646]\n","Step 312     [0.670 sec/step, loss=1.17264, avg_loss=1.22427]\n","Step 313     [0.675 sec/step, loss=1.22430, avg_loss=1.22439]\n","Generated 16 batches of size 16 in 2.043 sec\n","Step 314     [0.675 sec/step, loss=1.26589, avg_loss=1.22518]\n","Step 315     [0.675 sec/step, loss=1.09084, avg_loss=1.22180]\n","Step 316     [0.665 sec/step, loss=1.07918, avg_loss=1.22110]\n","Step 317     [0.663 sec/step, loss=1.11858, avg_loss=1.21951]\n","Step 318     [0.660 sec/step, loss=1.03702, avg_loss=1.21754]\n","Step 319     [0.659 sec/step, loss=1.22320, avg_loss=1.21592]\n","Step 320     [0.658 sec/step, loss=1.29411, avg_loss=1.21580]\n","Step 321     [0.658 sec/step, loss=1.12344, avg_loss=1.21466]\n","Step 322     [0.658 sec/step, loss=1.13820, avg_loss=1.21286]\n","Step 323     [0.659 sec/step, loss=1.08956, avg_loss=1.21137]\n","Step 324     [0.659 sec/step, loss=1.24955, avg_loss=1.21103]\n","Step 325     [0.659 sec/step, loss=1.03679, avg_loss=1.20928]\n","Step 326     [0.663 sec/step, loss=1.06664, avg_loss=1.20644]\n","Step 327     [0.663 sec/step, loss=1.20624, avg_loss=1.20761]\n","Step 328     [0.664 sec/step, loss=1.12565, avg_loss=1.20594]\n","Step 329     [0.668 sec/step, loss=1.21352, avg_loss=1.20583]\n","Generated 16 batches of size 16 in 2.367 sec\n","Step 330     [0.673 sec/step, loss=1.12830, avg_loss=1.20525]\n","Step 331     [0.674 sec/step, loss=1.32094, avg_loss=1.20579]\n","Step 332     [0.669 sec/step, loss=1.09399, avg_loss=1.20321]\n","Step 333     [0.665 sec/step, loss=1.15973, avg_loss=1.20182]\n","Step 334     [0.666 sec/step, loss=1.24287, avg_loss=1.20123]\n","Step 335     [0.665 sec/step, loss=1.16559, avg_loss=1.19871]\n","Step 336     [0.664 sec/step, loss=1.17611, avg_loss=1.19835]\n","Step 337     [0.661 sec/step, loss=1.15755, avg_loss=1.19861]\n","Step 338     [0.662 sec/step, loss=1.10840, avg_loss=1.19717]\n","Step 339     [0.660 sec/step, loss=1.03133, avg_loss=1.19584]\n","Step 340     [0.665 sec/step, loss=1.05750, avg_loss=1.19406]\n","Step 341     [0.664 sec/step, loss=1.24603, avg_loss=1.19376]\n","Step 342     [0.664 sec/step, loss=1.19757, avg_loss=1.19493]\n","Step 343     [0.665 sec/step, loss=1.15851, avg_loss=1.19307]\n","Step 344     [0.669 sec/step, loss=1.09433, avg_loss=1.19251]\n","Step 345     [0.675 sec/step, loss=1.05817, avg_loss=1.19122]\n","Generated 16 batches of size 16 in 2.103 sec\n","Step 346     [0.675 sec/step, loss=1.23764, avg_loss=1.18964]\n","Step 347     [0.676 sec/step, loss=1.28086, avg_loss=1.18994]\n","Step 348     [0.662 sec/step, loss=1.16969, avg_loss=1.19037]\n","Step 349     [0.660 sec/step, loss=1.08745, avg_loss=1.18928]\n","Step 350     [0.658 sec/step, loss=1.13064, avg_loss=1.18817]\n","Step 351     [0.658 sec/step, loss=1.10035, avg_loss=1.18751]\n","Step 352     [0.657 sec/step, loss=1.16546, avg_loss=1.18573]\n","Step 353     [0.660 sec/step, loss=1.08496, avg_loss=1.18463]\n","Step 354     [0.660 sec/step, loss=1.15402, avg_loss=1.18302]\n","Step 355     [0.660 sec/step, loss=1.19821, avg_loss=1.18225]\n","Step 356     [0.661 sec/step, loss=1.27971, avg_loss=1.18281]\n","Step 357     [0.660 sec/step, loss=1.21179, avg_loss=1.18259]\n","Step 358     [0.661 sec/step, loss=1.14692, avg_loss=1.18074]\n","Step 359     [0.663 sec/step, loss=1.14132, avg_loss=1.17978]\n","Step 360     [0.665 sec/step, loss=1.09784, avg_loss=1.17887]\n","Generated 16 batches of size 16 in 2.025 sec\n","Step 361     [0.671 sec/step, loss=1.06128, avg_loss=1.17679]\n","Step 362     [0.670 sec/step, loss=1.17914, avg_loss=1.17594]\n","Step 363     [0.670 sec/step, loss=1.11783, avg_loss=1.17574]\n","Step 364     [0.662 sec/step, loss=1.01981, avg_loss=1.17222]\n","Step 365     [0.658 sec/step, loss=1.06084, avg_loss=1.17219]\n","Step 366     [0.657 sec/step, loss=1.15306, avg_loss=1.17213]\n","Step 367     [0.662 sec/step, loss=1.03336, avg_loss=1.17067]\n","Step 368     [0.662 sec/step, loss=1.06818, avg_loss=1.16880]\n","Step 369     [0.662 sec/step, loss=1.05225, avg_loss=1.16612]\n","Step 370     [0.657 sec/step, loss=1.13457, avg_loss=1.16616]\n","Step 371     [0.657 sec/step, loss=1.04775, avg_loss=1.16444]\n","Step 372     [0.659 sec/step, loss=1.13823, avg_loss=1.16403]\n","Step 373     [0.659 sec/step, loss=1.06031, avg_loss=1.16299]\n","Step 374     [0.659 sec/step, loss=1.05091, avg_loss=1.16102]\n","Step 375     [0.658 sec/step, loss=1.15597, avg_loss=1.16110]\n","Step 376     [0.671 sec/step, loss=1.02648, avg_loss=1.15911]\n","Generated 16 batches of size 16 in 2.238 sec\n","Step 377     [0.672 sec/step, loss=1.17516, avg_loss=1.15772]\n","Step 378     [0.672 sec/step, loss=1.19082, avg_loss=1.15590]\n","Step 379     [0.674 sec/step, loss=1.07205, avg_loss=1.15454]\n","Step 380     [0.670 sec/step, loss=1.10497, avg_loss=1.15384]\n","Step 381     [0.662 sec/step, loss=1.19557, avg_loss=1.15413]\n","Step 382     [0.663 sec/step, loss=1.11922, avg_loss=1.15291]\n","Step 383     [0.662 sec/step, loss=1.00546, avg_loss=1.15167]\n","Step 384     [0.661 sec/step, loss=1.26374, avg_loss=1.15219]\n","Step 385     [0.661 sec/step, loss=1.04205, avg_loss=1.15204]\n","Step 386     [0.657 sec/step, loss=1.21784, avg_loss=1.15329]\n","Step 387     [0.657 sec/step, loss=1.08354, avg_loss=1.15163]\n","Step 388     [0.658 sec/step, loss=1.10559, avg_loss=1.14961]\n","Step 389     [0.658 sec/step, loss=1.06369, avg_loss=1.14904]\n","Step 390     [0.660 sec/step, loss=1.06089, avg_loss=1.14794]\n","Step 391     [0.660 sec/step, loss=1.12081, avg_loss=1.14789]\n","Step 392     [0.666 sec/step, loss=1.22977, avg_loss=1.14865]\n","Generated 16 batches of size 16 in 2.237 sec\n","Step 393     [0.673 sec/step, loss=1.04361, avg_loss=1.14755]\n","Step 394     [0.673 sec/step, loss=1.00823, avg_loss=1.14541]\n","Step 395     [0.673 sec/step, loss=1.10381, avg_loss=1.14417]\n","Step 396     [0.667 sec/step, loss=1.14637, avg_loss=1.14201]\n","Step 397     [0.658 sec/step, loss=1.11927, avg_loss=1.14233]\n","Step 398     [0.658 sec/step, loss=1.08123, avg_loss=1.14165]\n","Step 399     [0.659 sec/step, loss=1.20458, avg_loss=1.14325]\n","Step 400     [0.659 sec/step, loss=1.01790, avg_loss=1.14103]\n","Step 401     [0.659 sec/step, loss=1.10791, avg_loss=1.14018]\n","Step 402     [0.659 sec/step, loss=1.10338, avg_loss=1.13897]\n","Step 403     [0.659 sec/step, loss=1.04626, avg_loss=1.13657]\n","Step 404     [0.659 sec/step, loss=1.16568, avg_loss=1.13538]\n","Step 405     [0.659 sec/step, loss=1.03386, avg_loss=1.13455]\n","Step 406     [0.660 sec/step, loss=1.05605, avg_loss=1.13317]\n","Step 407     [0.658 sec/step, loss=1.13307, avg_loss=1.13250]\n","Step 408     [0.671 sec/step, loss=1.01464, avg_loss=1.13125]\n","Generated 16 batches of size 16 in 2.006 sec\n","Step 409     [0.672 sec/step, loss=1.13617, avg_loss=1.12934]\n","Step 410     [0.672 sec/step, loss=1.16224, avg_loss=1.12885]\n","Step 411     [0.670 sec/step, loss=1.09411, avg_loss=1.12829]\n","Step 412     [0.665 sec/step, loss=1.03591, avg_loss=1.12692]\n","Step 413     [0.660 sec/step, loss=1.23008, avg_loss=1.12698]\n","Step 414     [0.659 sec/step, loss=1.04584, avg_loss=1.12478]\n","Step 415     [0.659 sec/step, loss=0.99325, avg_loss=1.12380]\n","Step 416     [0.656 sec/step, loss=1.09384, avg_loss=1.12395]\n","Step 417     [0.656 sec/step, loss=1.10438, avg_loss=1.12381]\n","Step 418     [0.661 sec/step, loss=1.00159, avg_loss=1.12345]\n","Step 419     [0.662 sec/step, loss=1.03765, avg_loss=1.12160]\n","Step 420     [0.663 sec/step, loss=1.18073, avg_loss=1.12046]\n","Step 421     [0.662 sec/step, loss=1.02021, avg_loss=1.11943]\n","Step 422     [0.665 sec/step, loss=1.04180, avg_loss=1.11847]\n","Step 423     [0.666 sec/step, loss=1.09001, avg_loss=1.11847]\n","Step 424     [0.670 sec/step, loss=1.08521, avg_loss=1.11683]\n","Step 425     [0.677 sec/step, loss=1.08967, avg_loss=1.11736]\n","Generated 16 batches of size 16 in 2.227 sec\n","Step 426     [0.673 sec/step, loss=1.14636, avg_loss=1.11815]\n","Step 427     [0.674 sec/step, loss=1.13381, avg_loss=1.11743]\n","Step 428     [0.671 sec/step, loss=1.21478, avg_loss=1.11832]\n","Step 429     [0.668 sec/step, loss=1.10299, avg_loss=1.11721]\n","Step 430     [0.663 sec/step, loss=1.00116, avg_loss=1.11594]\n","Step 431     [0.662 sec/step, loss=1.03701, avg_loss=1.11310]\n","Step 432     [0.662 sec/step, loss=1.00595, avg_loss=1.11222]\n","Step 433     [0.660 sec/step, loss=1.11053, avg_loss=1.11173]\n","Step 434     [0.664 sec/step, loss=1.00213, avg_loss=1.10932]\n","Step 435     [0.664 sec/step, loss=1.08835, avg_loss=1.10855]\n","Step 436     [0.662 sec/step, loss=0.97325, avg_loss=1.10652]\n","Step 437     [0.661 sec/step, loss=1.01084, avg_loss=1.10506]\n","Step 438     [0.659 sec/step, loss=1.15641, avg_loss=1.10554]\n","Step 439     [0.660 sec/step, loss=1.02503, avg_loss=1.10547]\n","Step 440     [0.663 sec/step, loss=1.19782, avg_loss=1.10688]\n","Generated 16 batches of size 16 in 2.123 sec\n","Step 441     [0.669 sec/step, loss=1.08090, avg_loss=1.10522]\n","Step 442     [0.668 sec/step, loss=1.07101, avg_loss=1.10396]\n","Step 443     [0.668 sec/step, loss=1.14716, avg_loss=1.10385]\n","Step 444     [0.664 sec/step, loss=1.13885, avg_loss=1.10429]\n","Step 445     [0.660 sec/step, loss=1.03447, avg_loss=1.10405]\n","Step 446     [0.659 sec/step, loss=1.03870, avg_loss=1.10206]\n","Step 447     [0.659 sec/step, loss=1.00133, avg_loss=1.09927]\n","Step 448     [0.661 sec/step, loss=1.07055, avg_loss=1.09828]\n","Step 449     [0.660 sec/step, loss=0.98397, avg_loss=1.09724]\n","Step 450     [0.659 sec/step, loss=0.99678, avg_loss=1.09590]\n","Step 451     [0.659 sec/step, loss=1.06344, avg_loss=1.09554]\n","Step 452     [0.660 sec/step, loss=1.00483, avg_loss=1.09393]\n","Step 453     [0.657 sec/step, loss=1.08950, avg_loss=1.09397]\n","Step 454     [0.657 sec/step, loss=1.10974, avg_loss=1.09353]\n","Step 455     [0.657 sec/step, loss=1.15230, avg_loss=1.09307]\n","Step 456     [0.665 sec/step, loss=1.06866, avg_loss=1.09096]\n","Generated 16 batches of size 16 in 2.154 sec\n","Step 457     [0.671 sec/step, loss=1.02758, avg_loss=1.08912]\n","Step 458     [0.669 sec/step, loss=1.01012, avg_loss=1.08775]\n","Step 459     [0.672 sec/step, loss=1.01448, avg_loss=1.08648]\n","Step 460     [0.668 sec/step, loss=1.19876, avg_loss=1.08749]\n","Step 461     [0.662 sec/step, loss=1.11030, avg_loss=1.08798]\n","Step 462     [0.663 sec/step, loss=1.06491, avg_loss=1.08684]\n","Step 463     [0.664 sec/step, loss=1.11180, avg_loss=1.08678]\n","Step 464     [0.664 sec/step, loss=1.00726, avg_loss=1.08665]\n","Step 465     [0.666 sec/step, loss=1.00986, avg_loss=1.08614]\n","Step 466     [0.666 sec/step, loss=1.09245, avg_loss=1.08554]\n","Step 467     [0.661 sec/step, loss=1.00077, avg_loss=1.08521]\n","Step 468     [0.662 sec/step, loss=1.11043, avg_loss=1.08564]\n","Step 469     [0.661 sec/step, loss=0.99278, avg_loss=1.08504]\n","Step 470     [0.662 sec/step, loss=1.13372, avg_loss=1.08503]\n","Step 471     [0.662 sec/step, loss=1.06491, avg_loss=1.08520]\n","Step 472     [0.669 sec/step, loss=1.05655, avg_loss=1.08439]\n","Generated 16 batches of size 16 in 2.105 sec\n","Step 473     [0.672 sec/step, loss=0.97299, avg_loss=1.08351]\n","Step 474     [0.672 sec/step, loss=1.05840, avg_loss=1.08359]\n","Step 475     [0.674 sec/step, loss=1.06412, avg_loss=1.08267]\n","Step 476     [0.662 sec/step, loss=1.11490, avg_loss=1.08355]\n","Step 477     [0.660 sec/step, loss=1.02134, avg_loss=1.08202]\n","Step 478     [0.660 sec/step, loss=1.00539, avg_loss=1.08016]\n","Step 479     [0.662 sec/step, loss=0.98216, avg_loss=1.07926]\n","Step 480     [0.663 sec/step, loss=1.17778, avg_loss=1.07999]\n","Step 481     [0.663 sec/step, loss=1.11982, avg_loss=1.07923]\n","Step 482     [0.662 sec/step, loss=1.11039, avg_loss=1.07915]\n","Step 483     [0.663 sec/step, loss=0.99018, avg_loss=1.07899]\n","Step 484     [0.664 sec/step, loss=1.05312, avg_loss=1.07689]\n","Step 485     [0.665 sec/step, loss=0.97281, avg_loss=1.07619]\n","Step 486     [0.665 sec/step, loss=1.17168, avg_loss=1.07573]\n","Step 487     [0.665 sec/step, loss=0.98938, avg_loss=1.07479]\n","Step 488     [0.667 sec/step, loss=1.06562, avg_loss=1.07439]\n","Generated 16 batches of size 16 in 2.213 sec\n","Step 489     [0.676 sec/step, loss=0.97331, avg_loss=1.07349]\n","Step 490     [0.676 sec/step, loss=1.01087, avg_loss=1.07299]\n","Step 491     [0.677 sec/step, loss=1.03390, avg_loss=1.07212]\n","Step 492     [0.670 sec/step, loss=1.01762, avg_loss=1.07000]\n","Step 493     [0.660 sec/step, loss=0.96210, avg_loss=1.06918]\n","Step 494     [0.662 sec/step, loss=1.04935, avg_loss=1.06959]\n","Step 495     [0.660 sec/step, loss=1.06593, avg_loss=1.06921]\n","Step 496     [0.660 sec/step, loss=1.09337, avg_loss=1.06868]\n","Step 497     [0.661 sec/step, loss=1.09850, avg_loss=1.06848]\n","Step 498     [0.661 sec/step, loss=1.00496, avg_loss=1.06771]\n","Step 499     [0.661 sec/step, loss=1.08833, avg_loss=1.06655]\n","Step 500     [0.660 sec/step, loss=1.03745, avg_loss=1.06675]\n","Saving audio and alignment...\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n","findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12616 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12627 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12615 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12618 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12643 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12593 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12629 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12610 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12596 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12601 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12641 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12631 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12624 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12614 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12613 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12599 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12623 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51221 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52824 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51201 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44201 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48320 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47484 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48372 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45256 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49845 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45768 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12616 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12627 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12615 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12618 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12643 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12593 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12629 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12610 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12596 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12601 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12641 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12631 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12624 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12614 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12613 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12599 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12623 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51221 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52824 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51201 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44201 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48320 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47484 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48372 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45256 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49845 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45768 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/train-step-000000500-align000.png\n","100% 1/1 [00:01<00:00,  1.97s/it]\n","Test finished for step 500.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12622 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12609 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12636 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12642 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12621 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45824 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54620 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48124 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44397 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51032 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50526 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44600 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51012 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50676 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50612 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51452 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49512 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12622 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12609 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12636 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12642 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12621 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45824 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54620 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48124 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44397 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51032 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50526 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44600 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51012 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50676 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50612 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51452 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49512 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/test-step-000000500-align000.png\n"," 50% 1/2 [00:02<00:02,  2.21s/it]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51316 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44221 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49324 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46993 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50668 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47084 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48516 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51316 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44221 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49324 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46993 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50668 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47084 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48516 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/test-step-000000500-align001.png\n","100% 2/2 [00:04<00:00,  2.19s/it]\n","Test finished for step 500.\n","Step 501     [0.660 sec/step, loss=1.04785, avg_loss=1.06615]\n","Step 502     [0.663 sec/step, loss=0.99954, avg_loss=1.06511]\n","Step 503     [0.666 sec/step, loss=1.05827, avg_loss=1.06523]\n","Step 504     [0.669 sec/step, loss=0.95744, avg_loss=1.06314]\n","Generated 16 batches of size 16 in 2.261 sec\n","Step 505     [0.671 sec/step, loss=0.96964, avg_loss=1.06250]\n","Step 506     [0.673 sec/step, loss=0.96647, avg_loss=1.06161]\n","Step 507     [0.675 sec/step, loss=1.11411, avg_loss=1.06142]\n","Step 508     [0.662 sec/step, loss=0.97762, avg_loss=1.06105]\n","Step 509     [0.661 sec/step, loss=0.97881, avg_loss=1.05947]\n","Step 510     [0.662 sec/step, loss=1.03642, avg_loss=1.05821]\n","Step 511     [0.662 sec/step, loss=1.15180, avg_loss=1.05879]\n","Step 512     [0.663 sec/step, loss=1.09249, avg_loss=1.05936]\n","Step 513     [0.664 sec/step, loss=0.98713, avg_loss=1.05693]\n","Step 514     [0.665 sec/step, loss=1.15256, avg_loss=1.05800]\n","Step 515     [0.665 sec/step, loss=0.95670, avg_loss=1.05763]\n","Step 516     [0.664 sec/step, loss=0.97909, avg_loss=1.05648]\n","Step 517     [0.664 sec/step, loss=0.96263, avg_loss=1.05506]\n","Step 518     [0.660 sec/step, loss=1.09945, avg_loss=1.05604]\n","Step 519     [0.664 sec/step, loss=1.02158, avg_loss=1.05588]\n","Step 520     [0.671 sec/step, loss=1.03328, avg_loss=1.05441]\n","Generated 16 batches of size 16 in 2.229 sec\n","Step 521     [0.671 sec/step, loss=1.05571, avg_loss=1.05476]\n","Step 522     [0.669 sec/step, loss=0.97131, avg_loss=1.05406]\n","Step 523     [0.667 sec/step, loss=1.03620, avg_loss=1.05352]\n","Step 524     [0.663 sec/step, loss=1.00109, avg_loss=1.05268]\n","Step 525     [0.658 sec/step, loss=1.03346, avg_loss=1.05212]\n","Step 526     [0.657 sec/step, loss=1.07053, avg_loss=1.05136]\n","Step 527     [0.662 sec/step, loss=0.95186, avg_loss=1.04954]\n","Step 528     [0.661 sec/step, loss=0.98380, avg_loss=1.04723]\n","Step 529     [0.662 sec/step, loss=0.96460, avg_loss=1.04585]\n","Step 530     [0.667 sec/step, loss=0.95107, avg_loss=1.04534]\n","Step 531     [0.667 sec/step, loss=1.05431, avg_loss=1.04552]\n","Step 532     [0.669 sec/step, loss=0.98721, avg_loss=1.04533]\n","Step 533     [0.669 sec/step, loss=1.05281, avg_loss=1.04475]\n","Step 534     [0.666 sec/step, loss=1.02495, avg_loss=1.04498]\n","Step 535     [0.670 sec/step, loss=0.94793, avg_loss=1.04358]\n","Step 536     [0.675 sec/step, loss=1.00092, avg_loss=1.04385]\n","Generated 16 batches of size 16 in 2.246 sec\n","Step 537     [0.677 sec/step, loss=1.09052, avg_loss=1.04465]\n","Step 538     [0.676 sec/step, loss=0.93607, avg_loss=1.04245]\n","Step 539     [0.676 sec/step, loss=1.04756, avg_loss=1.04267]\n","Step 540     [0.670 sec/step, loss=1.14530, avg_loss=1.04215]\n","Step 541     [0.664 sec/step, loss=1.07615, avg_loss=1.04210]\n","Step 542     [0.666 sec/step, loss=1.03837, avg_loss=1.04177]\n","Step 543     [0.665 sec/step, loss=0.97235, avg_loss=1.04002]\n","Step 544     [0.665 sec/step, loss=1.08476, avg_loss=1.03948]\n","Step 545     [0.663 sec/step, loss=0.94730, avg_loss=1.03861]\n","Step 546     [0.663 sec/step, loss=0.96152, avg_loss=1.03784]\n","Step 547     [0.664 sec/step, loss=1.01688, avg_loss=1.03800]\n","Step 548     [0.663 sec/step, loss=1.00682, avg_loss=1.03736]\n","Step 549     [0.663 sec/step, loss=0.99760, avg_loss=1.03749]\n","Step 550     [0.666 sec/step, loss=0.97978, avg_loss=1.03732]\n","Step 551     [0.670 sec/step, loss=1.02982, avg_loss=1.03699]\n","Generated 16 batches of size 16 in 2.123 sec\n","Step 552     [0.677 sec/step, loss=1.02675, avg_loss=1.03721]\n","Step 553     [0.677 sec/step, loss=1.08107, avg_loss=1.03712]\n","Step 554     [0.678 sec/step, loss=0.94764, avg_loss=1.03550]\n","Step 555     [0.678 sec/step, loss=1.12598, avg_loss=1.03524]\n","Step 556     [0.671 sec/step, loss=1.06606, avg_loss=1.03521]\n","Step 557     [0.664 sec/step, loss=0.94997, avg_loss=1.03444]\n","Step 558     [0.668 sec/step, loss=0.94902, avg_loss=1.03383]\n","Step 559     [0.663 sec/step, loss=1.02928, avg_loss=1.03397]\n","Step 560     [0.665 sec/step, loss=0.98122, avg_loss=1.03180]\n","Step 561     [0.665 sec/step, loss=1.02993, avg_loss=1.03100]\n","Step 562     [0.665 sec/step, loss=1.01317, avg_loss=1.03048]\n","Step 563     [0.664 sec/step, loss=0.94796, avg_loss=1.02884]\n","Step 564     [0.664 sec/step, loss=0.94986, avg_loss=1.02827]\n","Step 565     [0.663 sec/step, loss=1.04169, avg_loss=1.02858]\n","Step 566     [0.664 sec/step, loss=1.07060, avg_loss=1.02837]\n","Step 567     [0.671 sec/step, loss=1.11451, avg_loss=1.02950]\n","Generated 16 batches of size 16 in 2.165 sec\n","Step 568     [0.676 sec/step, loss=0.95166, avg_loss=1.02791]\n","Step 569     [0.677 sec/step, loss=1.02887, avg_loss=1.02828]\n","Step 570     [0.676 sec/step, loss=1.01126, avg_loss=1.02705]\n","Step 571     [0.681 sec/step, loss=0.95164, avg_loss=1.02592]\n","Step 572     [0.672 sec/step, loss=1.06750, avg_loss=1.02603]\n","Step 573     [0.669 sec/step, loss=1.01767, avg_loss=1.02647]\n","Step 574     [0.669 sec/step, loss=0.98537, avg_loss=1.02574]\n","Step 575     [0.668 sec/step, loss=0.93959, avg_loss=1.02450]\n","Step 576     [0.666 sec/step, loss=1.03811, avg_loss=1.02373]\n","Step 577     [0.667 sec/step, loss=0.96670, avg_loss=1.02318]\n","Step 578     [0.667 sec/step, loss=1.11200, avg_loss=1.02425]\n","Step 579     [0.664 sec/step, loss=1.06151, avg_loss=1.02504]\n","Step 580     [0.663 sec/step, loss=1.00962, avg_loss=1.02336]\n","Step 581     [0.662 sec/step, loss=0.96874, avg_loss=1.02185]\n","Step 582     [0.664 sec/step, loss=1.01177, avg_loss=1.02087]\n","Step 583     [0.675 sec/step, loss=0.92822, avg_loss=1.02025]\n","Generated 16 batches of size 16 in 2.271 sec\n","Step 584     [0.676 sec/step, loss=0.99856, avg_loss=1.01970]\n","Step 585     [0.676 sec/step, loss=1.01088, avg_loss=1.02008]\n","Step 586     [0.675 sec/step, loss=1.05712, avg_loss=1.01894]\n","Step 587     [0.676 sec/step, loss=0.95324, avg_loss=1.01857]\n","Step 588     [0.675 sec/step, loss=0.97288, avg_loss=1.01765]\n","Step 589     [0.664 sec/step, loss=0.92916, avg_loss=1.01721]\n","Step 590     [0.663 sec/step, loss=1.02058, avg_loss=1.01730]\n","Step 591     [0.663 sec/step, loss=0.93087, avg_loss=1.01627]\n","Step 592     [0.664 sec/step, loss=1.09887, avg_loss=1.01708]\n","Step 593     [0.665 sec/step, loss=1.05934, avg_loss=1.01806]\n","Step 594     [0.665 sec/step, loss=1.01195, avg_loss=1.01768]\n","Step 595     [0.666 sec/step, loss=0.99924, avg_loss=1.01702]\n","Step 596     [0.666 sec/step, loss=1.04126, avg_loss=1.01650]\n","Step 597     [0.666 sec/step, loss=0.99646, avg_loss=1.01548]\n","Step 598     [0.667 sec/step, loss=1.01547, avg_loss=1.01558]\n","Step 599     [0.672 sec/step, loss=0.93099, avg_loss=1.01401]\n","Generated 16 batches of size 16 in 2.251 sec\n","Step 600     [0.682 sec/step, loss=0.93428, avg_loss=1.01298]\n","Step 601     [0.680 sec/step, loss=0.98148, avg_loss=1.01231]\n","Step 602     [0.679 sec/step, loss=0.94636, avg_loss=1.01178]\n","Step 603     [0.675 sec/step, loss=0.93229, avg_loss=1.01052]\n","Step 604     [0.670 sec/step, loss=1.00650, avg_loss=1.01101]\n","Step 605     [0.667 sec/step, loss=0.99088, avg_loss=1.01122]\n","Step 606     [0.663 sec/step, loss=0.93029, avg_loss=1.01086]\n","Step 607     [0.664 sec/step, loss=0.96943, avg_loss=1.00941]\n","Step 608     [0.664 sec/step, loss=0.93235, avg_loss=1.00896]\n","Step 609     [0.665 sec/step, loss=1.09172, avg_loss=1.01009]\n","Step 610     [0.663 sec/step, loss=0.91610, avg_loss=1.00889]\n","Step 611     [0.664 sec/step, loss=0.99727, avg_loss=1.00734]\n","Step 612     [0.669 sec/step, loss=0.92021, avg_loss=1.00562]\n","Step 613     [0.666 sec/step, loss=0.91868, avg_loss=1.00494]\n","Step 614     [0.666 sec/step, loss=0.96317, avg_loss=1.00304]\n","Step 615     [0.676 sec/step, loss=0.95672, avg_loss=1.00304]\n","Generated 16 batches of size 16 in 2.100 sec\n","Step 616     [0.678 sec/step, loss=1.04801, avg_loss=1.00373]\n","Step 617     [0.679 sec/step, loss=1.02866, avg_loss=1.00439]\n","Step 618     [0.678 sec/step, loss=0.98214, avg_loss=1.00322]\n","Step 619     [0.674 sec/step, loss=0.98333, avg_loss=1.00284]\n","Step 620     [0.666 sec/step, loss=1.01703, avg_loss=1.00267]\n","Step 621     [0.668 sec/step, loss=1.00752, avg_loss=1.00219]\n","Step 622     [0.668 sec/step, loss=1.04617, avg_loss=1.00294]\n","Step 623     [0.669 sec/step, loss=0.93470, avg_loss=1.00192]\n","Step 624     [0.670 sec/step, loss=0.98426, avg_loss=1.00176]\n","Step 625     [0.668 sec/step, loss=0.90980, avg_loss=1.00052]\n","Step 626     [0.668 sec/step, loss=1.04728, avg_loss=1.00029]\n","Step 627     [0.664 sec/step, loss=0.93592, avg_loss=1.00013]\n","Step 628     [0.663 sec/step, loss=0.99851, avg_loss=1.00027]\n","Step 629     [0.666 sec/step, loss=0.94466, avg_loss=1.00008]\n","Step 630     [0.661 sec/step, loss=0.96627, avg_loss=1.00023]\n","Step 631     [0.668 sec/step, loss=0.99772, avg_loss=0.99966]\n","Generated 16 batches of size 16 in 2.220 sec\n","Step 632     [0.671 sec/step, loss=1.00616, avg_loss=0.99985]\n","Step 633     [0.672 sec/step, loss=1.08368, avg_loss=1.00016]\n","Step 634     [0.670 sec/step, loss=0.92493, avg_loss=0.99916]\n","Step 635     [0.667 sec/step, loss=0.92563, avg_loss=0.99894]\n","Step 636     [0.664 sec/step, loss=1.03337, avg_loss=0.99926]\n","Step 637     [0.661 sec/step, loss=0.98761, avg_loss=0.99823]\n","Step 638     [0.662 sec/step, loss=0.96663, avg_loss=0.99854]\n","Step 639     [0.665 sec/step, loss=0.91219, avg_loss=0.99718]\n","Step 640     [0.665 sec/step, loss=1.03456, avg_loss=0.99608]\n","Step 641     [0.664 sec/step, loss=0.95844, avg_loss=0.99490]\n","Step 642     [0.663 sec/step, loss=0.99881, avg_loss=0.99450]\n","Step 643     [0.662 sec/step, loss=0.90303, avg_loss=0.99381]\n","Step 644     [0.666 sec/step, loss=0.90554, avg_loss=0.99202]\n","Step 645     [0.667 sec/step, loss=1.06916, avg_loss=0.99324]\n","Step 646     [0.666 sec/step, loss=0.97327, avg_loss=0.99335]\n","Step 647     [0.670 sec/step, loss=1.02000, avg_loss=0.99339]\n","Generated 16 batches of size 16 in 2.226 sec\n","Step 648     [0.677 sec/step, loss=0.98136, avg_loss=0.99313]\n","Step 649     [0.677 sec/step, loss=0.91263, avg_loss=0.99228]\n","Step 650     [0.675 sec/step, loss=0.92590, avg_loss=0.99174]\n","Step 651     [0.672 sec/step, loss=0.93417, avg_loss=0.99079]\n","Step 652     [0.667 sec/step, loss=0.94224, avg_loss=0.98994]\n","Step 653     [0.667 sec/step, loss=1.00102, avg_loss=0.98914]\n","Step 654     [0.668 sec/step, loss=1.00753, avg_loss=0.98974]\n","Step 655     [0.668 sec/step, loss=0.95270, avg_loss=0.98801]\n","Step 656     [0.668 sec/step, loss=0.99475, avg_loss=0.98729]\n","Step 657     [0.669 sec/step, loss=0.90522, avg_loss=0.98685]\n","Step 658     [0.670 sec/step, loss=0.90628, avg_loss=0.98642]\n","Step 659     [0.670 sec/step, loss=0.91574, avg_loss=0.98528]\n","Step 660     [0.667 sec/step, loss=0.90333, avg_loss=0.98450]\n","Step 661     [0.668 sec/step, loss=0.99756, avg_loss=0.98418]\n","Step 662     [0.666 sec/step, loss=0.99394, avg_loss=0.98399]\n","Step 663     [0.672 sec/step, loss=1.04510, avg_loss=0.98496]\n","Generated 16 batches of size 16 in 2.065 sec\n","Step 664     [0.678 sec/step, loss=1.03004, avg_loss=0.98576]\n","Step 665     [0.678 sec/step, loss=0.97084, avg_loss=0.98505]\n","Step 666     [0.678 sec/step, loss=0.94928, avg_loss=0.98384]\n","Step 667     [0.674 sec/step, loss=0.94697, avg_loss=0.98217]\n","Step 668     [0.669 sec/step, loss=0.96545, avg_loss=0.98230]\n","Step 669     [0.668 sec/step, loss=1.07297, avg_loss=0.98274]\n","Step 670     [0.668 sec/step, loss=0.96321, avg_loss=0.98226]\n","Step 671     [0.664 sec/step, loss=0.92626, avg_loss=0.98201]\n","Step 672     [0.665 sec/step, loss=0.96682, avg_loss=0.98100]\n","Step 673     [0.666 sec/step, loss=1.06139, avg_loss=0.98144]\n","Step 674     [0.665 sec/step, loss=0.91164, avg_loss=0.98070]\n","Step 675     [0.666 sec/step, loss=0.90989, avg_loss=0.98041]\n","Step 676     [0.671 sec/step, loss=0.90056, avg_loss=0.97903]\n","Step 677     [0.670 sec/step, loss=0.98451, avg_loss=0.97921]\n","Step 678     [0.670 sec/step, loss=0.92464, avg_loss=0.97734]\n","Step 679     [0.672 sec/step, loss=0.94688, avg_loss=0.97619]\n","Step 680     [0.679 sec/step, loss=0.98620, avg_loss=0.97595]\n","Generated 16 batches of size 16 in 2.059 sec\n","Step 681     [0.680 sec/step, loss=1.01347, avg_loss=0.97640]\n","Step 682     [0.678 sec/step, loss=0.93126, avg_loss=0.97560]\n","Step 683     [0.666 sec/step, loss=1.03569, avg_loss=0.97667]\n","Step 684     [0.667 sec/step, loss=0.93741, avg_loss=0.97606]\n","Step 685     [0.667 sec/step, loss=0.99694, avg_loss=0.97592]\n","Step 686     [0.667 sec/step, loss=0.96508, avg_loss=0.97500]\n","Step 687     [0.667 sec/step, loss=0.97341, avg_loss=0.97520]\n","Step 688     [0.665 sec/step, loss=1.01799, avg_loss=0.97565]\n","Step 689     [0.665 sec/step, loss=0.99986, avg_loss=0.97636]\n","Step 690     [0.664 sec/step, loss=0.93379, avg_loss=0.97549]\n","Step 691     [0.666 sec/step, loss=0.98538, avg_loss=0.97604]\n","Step 692     [0.664 sec/step, loss=0.90519, avg_loss=0.97410]\n","Step 693     [0.664 sec/step, loss=0.99183, avg_loss=0.97343]\n","Step 694     [0.664 sec/step, loss=1.05804, avg_loss=0.97389]\n","Step 695     [0.669 sec/step, loss=1.01215, avg_loss=0.97402]\n","Generated 16 batches of size 16 in 2.158 sec\n","Step 696     [0.675 sec/step, loss=0.94080, avg_loss=0.97301]\n","Step 697     [0.677 sec/step, loss=0.96959, avg_loss=0.97274]\n","Step 698     [0.677 sec/step, loss=0.93865, avg_loss=0.97197]\n","Step 699     [0.670 sec/step, loss=0.89130, avg_loss=0.97158]\n","Step 700     [0.661 sec/step, loss=0.91456, avg_loss=0.97138]\n","Step 701     [0.662 sec/step, loss=0.94213, avg_loss=0.97099]\n","Step 702     [0.665 sec/step, loss=0.89982, avg_loss=0.97052]\n","Step 703     [0.665 sec/step, loss=0.96365, avg_loss=0.97083]\n","Step 704     [0.666 sec/step, loss=0.94079, avg_loss=0.97018]\n","Step 705     [0.668 sec/step, loss=0.92470, avg_loss=0.96952]\n","Step 706     [0.673 sec/step, loss=0.90187, avg_loss=0.96923]\n","Step 707     [0.672 sec/step, loss=0.98489, avg_loss=0.96939]\n","Step 708     [0.672 sec/step, loss=0.98272, avg_loss=0.96989]\n","Step 709     [0.671 sec/step, loss=0.90904, avg_loss=0.96806]\n","Step 710     [0.671 sec/step, loss=0.89144, avg_loss=0.96782]\n","Step 711     [0.674 sec/step, loss=0.89063, avg_loss=0.96675]\n","Generated 16 batches of size 16 in 2.114 sec\n","Step 712     [0.677 sec/step, loss=1.01511, avg_loss=0.96770]\n","Step 713     [0.678 sec/step, loss=0.90153, avg_loss=0.96753]\n","Step 714     [0.678 sec/step, loss=1.05528, avg_loss=0.96845]\n","Step 715     [0.668 sec/step, loss=0.94695, avg_loss=0.96835]\n","Step 716     [0.666 sec/step, loss=0.94638, avg_loss=0.96733]\n","Step 717     [0.665 sec/step, loss=1.02690, avg_loss=0.96732]\n","Step 718     [0.667 sec/step, loss=0.96119, avg_loss=0.96711]\n","Step 719     [0.667 sec/step, loss=0.97858, avg_loss=0.96706]\n","Step 720     [0.669 sec/step, loss=0.94843, avg_loss=0.96637]\n","Step 721     [0.668 sec/step, loss=0.97661, avg_loss=0.96606]\n","Step 722     [0.667 sec/step, loss=0.90671, avg_loss=0.96467]\n","Step 723     [0.668 sec/step, loss=0.98229, avg_loss=0.96515]\n","Step 724     [0.669 sec/step, loss=0.92283, avg_loss=0.96453]\n","Step 725     [0.669 sec/step, loss=0.89545, avg_loss=0.96439]\n","Step 726     [0.668 sec/step, loss=0.99035, avg_loss=0.96382]\n","Step 727     [0.672 sec/step, loss=0.93818, avg_loss=0.96384]\n","Step 728     [0.679 sec/step, loss=1.03485, avg_loss=0.96420]\n","Generated 16 batches of size 16 in 2.221 sec\n","Step 729     [0.676 sec/step, loss=0.88332, avg_loss=0.96359]\n","Step 730     [0.676 sec/step, loss=0.93750, avg_loss=0.96330]\n","Step 731     [0.673 sec/step, loss=0.88539, avg_loss=0.96218]\n","Step 732     [0.668 sec/step, loss=0.99915, avg_loss=0.96211]\n","Step 733     [0.668 sec/step, loss=0.91548, avg_loss=0.96043]\n","Step 734     [0.668 sec/step, loss=0.93916, avg_loss=0.96057]\n","Step 735     [0.667 sec/step, loss=1.01985, avg_loss=0.96151]\n","Step 736     [0.671 sec/step, loss=0.88308, avg_loss=0.96001]\n","Step 737     [0.673 sec/step, loss=0.97320, avg_loss=0.95986]\n","Step 738     [0.673 sec/step, loss=0.92784, avg_loss=0.95948]\n","Step 739     [0.670 sec/step, loss=0.91480, avg_loss=0.95950]\n","Step 740     [0.671 sec/step, loss=0.89951, avg_loss=0.95815]\n","Step 741     [0.671 sec/step, loss=0.96661, avg_loss=0.95823]\n","Step 742     [0.671 sec/step, loss=0.88537, avg_loss=0.95710]\n","Step 743     [0.678 sec/step, loss=0.99623, avg_loss=0.95803]\n","Generated 16 batches of size 16 in 2.112 sec\n","Step 744     [0.678 sec/step, loss=0.92409, avg_loss=0.95822]\n","Step 745     [0.678 sec/step, loss=0.91804, avg_loss=0.95671]\n","Step 746     [0.678 sec/step, loss=0.90324, avg_loss=0.95601]\n","Step 747     [0.672 sec/step, loss=0.99899, avg_loss=0.95580]\n","Step 748     [0.667 sec/step, loss=0.94371, avg_loss=0.95542]\n","Step 749     [0.667 sec/step, loss=0.88102, avg_loss=0.95510]\n","Step 750     [0.667 sec/step, loss=1.02996, avg_loss=0.95614]\n","Step 751     [0.667 sec/step, loss=0.95685, avg_loss=0.95637]\n","Step 752     [0.665 sec/step, loss=0.89414, avg_loss=0.95589]\n","Step 753     [0.664 sec/step, loss=0.92475, avg_loss=0.95513]\n","Step 754     [0.663 sec/step, loss=1.00996, avg_loss=0.95515]\n","Step 755     [0.665 sec/step, loss=0.90740, avg_loss=0.95470]\n","Step 756     [0.665 sec/step, loss=0.96637, avg_loss=0.95441]\n","Step 757     [0.665 sec/step, loss=0.96072, avg_loss=0.95497]\n","Step 758     [0.661 sec/step, loss=0.88753, avg_loss=0.95478]\n","Step 759     [0.666 sec/step, loss=0.88804, avg_loss=0.95450]\n","Step 760     [0.672 sec/step, loss=1.01284, avg_loss=0.95560]\n","Generated 16 batches of size 16 in 2.207 sec\n","Step 761     [0.672 sec/step, loss=0.92482, avg_loss=0.95487]\n","Step 762     [0.677 sec/step, loss=0.88459, avg_loss=0.95378]\n","Step 763     [0.674 sec/step, loss=0.94310, avg_loss=0.95276]\n","Step 764     [0.667 sec/step, loss=0.92898, avg_loss=0.95175]\n","Step 765     [0.665 sec/step, loss=0.97396, avg_loss=0.95178]\n","Step 766     [0.665 sec/step, loss=0.87203, avg_loss=0.95101]\n","Step 767     [0.663 sec/step, loss=0.98929, avg_loss=0.95143]\n","Step 768     [0.664 sec/step, loss=0.88615, avg_loss=0.95064]\n","Step 769     [0.663 sec/step, loss=0.91656, avg_loss=0.94907]\n","Step 770     [0.663 sec/step, loss=0.95727, avg_loss=0.94901]\n","Step 771     [0.663 sec/step, loss=0.98515, avg_loss=0.94960]\n","Step 772     [0.662 sec/step, loss=1.01410, avg_loss=0.95008]\n","Step 773     [0.662 sec/step, loss=1.00153, avg_loss=0.94948]\n","Step 774     [0.663 sec/step, loss=0.94389, avg_loss=0.94980]\n","Step 775     [0.671 sec/step, loss=0.91448, avg_loss=0.94985]\n","Generated 16 batches of size 16 in 2.206 sec\n","Step 776     [0.670 sec/step, loss=0.89281, avg_loss=0.94977]\n","Step 777     [0.672 sec/step, loss=0.93688, avg_loss=0.94929]\n","Step 778     [0.672 sec/step, loss=0.92405, avg_loss=0.94929]\n","Step 779     [0.671 sec/step, loss=0.96978, avg_loss=0.94951]\n","Step 780     [0.665 sec/step, loss=0.88955, avg_loss=0.94855]\n","Step 781     [0.663 sec/step, loss=0.87161, avg_loss=0.94713]\n","Step 782     [0.663 sec/step, loss=0.91960, avg_loss=0.94701]\n","Step 783     [0.667 sec/step, loss=0.87897, avg_loss=0.94545]\n","Step 784     [0.666 sec/step, loss=0.87712, avg_loss=0.94484]\n","Step 785     [0.665 sec/step, loss=1.00292, avg_loss=0.94490]\n","Step 786     [0.665 sec/step, loss=0.91334, avg_loss=0.94438]\n","Step 787     [0.665 sec/step, loss=0.89980, avg_loss=0.94365]\n","Step 788     [0.666 sec/step, loss=0.97470, avg_loss=0.94322]\n","Step 789     [0.668 sec/step, loss=0.95749, avg_loss=0.94279]\n","Step 790     [0.667 sec/step, loss=0.86346, avg_loss=0.94209]\n","Step 791     [0.670 sec/step, loss=0.87810, avg_loss=0.94102]\n","Generated 16 batches of size 16 in 2.165 sec\n","Step 792     [0.680 sec/step, loss=0.86700, avg_loss=0.94063]\n","Step 793     [0.680 sec/step, loss=0.98430, avg_loss=0.94056]\n","Step 794     [0.680 sec/step, loss=0.94102, avg_loss=0.93939]\n","Step 795     [0.675 sec/step, loss=0.90589, avg_loss=0.93833]\n","Step 796     [0.668 sec/step, loss=0.95900, avg_loss=0.93851]\n","Step 797     [0.666 sec/step, loss=0.91376, avg_loss=0.93795]\n","Step 798     [0.667 sec/step, loss=0.92737, avg_loss=0.93784]\n","Step 799     [0.668 sec/step, loss=0.91964, avg_loss=0.93812]\n","Step 800     [0.670 sec/step, loss=0.89447, avg_loss=0.93792]\n","Step 801     [0.674 sec/step, loss=0.85722, avg_loss=0.93707]\n","Step 802     [0.669 sec/step, loss=0.85172, avg_loss=0.93659]\n","Step 803     [0.670 sec/step, loss=0.99702, avg_loss=0.93692]\n","Step 804     [0.669 sec/step, loss=0.86731, avg_loss=0.93619]\n","Step 805     [0.667 sec/step, loss=0.90991, avg_loss=0.93604]\n","Step 806     [0.663 sec/step, loss=0.99424, avg_loss=0.93696]\n","Step 807     [0.665 sec/step, loss=0.94941, avg_loss=0.93661]\n","Step 808     [0.670 sec/step, loss=0.88081, avg_loss=0.93559]\n","Generated 16 batches of size 16 in 2.199 sec\n","Step 809     [0.670 sec/step, loss=0.90648, avg_loss=0.93556]\n","Step 810     [0.671 sec/step, loss=0.93427, avg_loss=0.93599]\n","Step 811     [0.667 sec/step, loss=0.90909, avg_loss=0.93618]\n","Step 812     [0.661 sec/step, loss=0.87990, avg_loss=0.93483]\n","Step 813     [0.661 sec/step, loss=0.95724, avg_loss=0.93538]\n","Step 814     [0.661 sec/step, loss=0.97478, avg_loss=0.93458]\n","Step 815     [0.663 sec/step, loss=0.92291, avg_loss=0.93434]\n","Step 816     [0.663 sec/step, loss=0.89803, avg_loss=0.93385]\n","Step 817     [0.663 sec/step, loss=0.88165, avg_loss=0.93240]\n","Step 818     [0.661 sec/step, loss=0.86520, avg_loss=0.93144]\n","Step 819     [0.662 sec/step, loss=0.96924, avg_loss=0.93135]\n","Step 820     [0.661 sec/step, loss=0.88050, avg_loss=0.93067]\n","Step 821     [0.662 sec/step, loss=0.94897, avg_loss=0.93039]\n","Step 822     [0.662 sec/step, loss=0.93635, avg_loss=0.93069]\n","Step 823     [0.670 sec/step, loss=0.91031, avg_loss=0.92997]\n","Generated 16 batches of size 16 in 2.184 sec\n","Step 824     [0.672 sec/step, loss=0.91649, avg_loss=0.92991]\n","Step 825     [0.673 sec/step, loss=0.99371, avg_loss=0.93089]\n","Step 826     [0.673 sec/step, loss=0.85418, avg_loss=0.92953]\n","Step 827     [0.673 sec/step, loss=0.86982, avg_loss=0.92884]\n","Step 828     [0.667 sec/step, loss=0.99286, avg_loss=0.92842]\n","Step 829     [0.667 sec/step, loss=0.90814, avg_loss=0.92867]\n","Step 830     [0.667 sec/step, loss=0.90260, avg_loss=0.92832]\n","Step 831     [0.662 sec/step, loss=0.95136, avg_loss=0.92898]\n","Step 832     [0.662 sec/step, loss=0.95471, avg_loss=0.92854]\n","Step 833     [0.662 sec/step, loss=0.85447, avg_loss=0.92793]\n","Step 834     [0.662 sec/step, loss=0.98442, avg_loss=0.92838]\n","Step 835     [0.664 sec/step, loss=0.88817, avg_loss=0.92706]\n","Step 836     [0.661 sec/step, loss=0.91489, avg_loss=0.92738]\n","Step 837     [0.664 sec/step, loss=0.85547, avg_loss=0.92620]\n","Step 838     [0.664 sec/step, loss=0.93346, avg_loss=0.92626]\n","Step 839     [0.666 sec/step, loss=0.89963, avg_loss=0.92611]\n","Step 840     [0.669 sec/step, loss=0.89816, avg_loss=0.92609]\n","Generated 16 batches of size 16 in 2.093 sec\n","Step 841     [0.671 sec/step, loss=0.90147, avg_loss=0.92544]\n","Step 842     [0.672 sec/step, loss=0.86950, avg_loss=0.92528]\n","Step 843     [0.665 sec/step, loss=0.93593, avg_loss=0.92468]\n","Step 844     [0.661 sec/step, loss=0.86826, avg_loss=0.92412]\n","Step 845     [0.661 sec/step, loss=0.99362, avg_loss=0.92488]\n","Step 846     [0.661 sec/step, loss=0.87184, avg_loss=0.92456]\n","Step 847     [0.662 sec/step, loss=0.95150, avg_loss=0.92409]\n","Step 848     [0.662 sec/step, loss=0.91608, avg_loss=0.92381]\n","Step 849     [0.662 sec/step, loss=0.84632, avg_loss=0.92347]\n","Step 850     [0.662 sec/step, loss=0.89108, avg_loss=0.92208]\n","Step 851     [0.661 sec/step, loss=0.91474, avg_loss=0.92166]\n","Step 852     [0.660 sec/step, loss=0.94901, avg_loss=0.92221]\n","Step 853     [0.661 sec/step, loss=0.91668, avg_loss=0.92212]\n","Step 854     [0.665 sec/step, loss=0.85417, avg_loss=0.92057]\n","Step 855     [0.670 sec/step, loss=0.95371, avg_loss=0.92103]\n","Generated 16 batches of size 16 in 2.190 sec\n","Step 856     [0.676 sec/step, loss=0.88935, avg_loss=0.92026]\n","Step 857     [0.676 sec/step, loss=0.88267, avg_loss=0.91948]\n","Step 858     [0.675 sec/step, loss=0.96655, avg_loss=0.92027]\n","Step 859     [0.671 sec/step, loss=0.90048, avg_loss=0.92039]\n","Step 860     [0.667 sec/step, loss=0.96867, avg_loss=0.91995]\n","Step 861     [0.666 sec/step, loss=0.88346, avg_loss=0.91954]\n","Step 862     [0.662 sec/step, loss=0.98375, avg_loss=0.92053]\n","Step 863     [0.660 sec/step, loss=0.86325, avg_loss=0.91973]\n","Step 864     [0.659 sec/step, loss=0.93109, avg_loss=0.91975]\n","Step 865     [0.660 sec/step, loss=0.97737, avg_loss=0.91979]\n","Step 866     [0.661 sec/step, loss=0.98803, avg_loss=0.92095]\n","Step 867     [0.660 sec/step, loss=0.87489, avg_loss=0.91980]\n","Step 868     [0.661 sec/step, loss=0.91777, avg_loss=0.92012]\n","Step 869     [0.661 sec/step, loss=0.89114, avg_loss=0.91986]\n","Step 870     [0.662 sec/step, loss=0.92439, avg_loss=0.91954]\n","Step 871     [0.666 sec/step, loss=0.86863, avg_loss=0.91837]\n","Step 872     [0.670 sec/step, loss=0.90347, avg_loss=0.91726]\n","Generated 16 batches of size 16 in 2.294 sec\n","Step 873     [0.672 sec/step, loss=0.86216, avg_loss=0.91587]\n","Step 874     [0.671 sec/step, loss=0.90430, avg_loss=0.91547]\n","Step 875     [0.662 sec/step, loss=0.85449, avg_loss=0.91488]\n","Step 876     [0.659 sec/step, loss=0.95763, avg_loss=0.91552]\n","Step 877     [0.659 sec/step, loss=0.94347, avg_loss=0.91559]\n","Step 878     [0.662 sec/step, loss=0.89401, avg_loss=0.91529]\n","Step 879     [0.665 sec/step, loss=0.85510, avg_loss=0.91414]\n","Step 880     [0.666 sec/step, loss=0.91539, avg_loss=0.91440]\n","Step 881     [0.668 sec/step, loss=0.93144, avg_loss=0.91500]\n","Step 882     [0.668 sec/step, loss=0.86254, avg_loss=0.91443]\n","Step 883     [0.666 sec/step, loss=0.88611, avg_loss=0.91450]\n","Step 884     [0.670 sec/step, loss=0.84987, avg_loss=0.91423]\n","Step 885     [0.671 sec/step, loss=0.97351, avg_loss=0.91393]\n","Step 886     [0.671 sec/step, loss=0.89390, avg_loss=0.91374]\n","Step 887     [0.676 sec/step, loss=0.91873, avg_loss=0.91393]\n","Step 888     [0.680 sec/step, loss=0.86337, avg_loss=0.91281]\n","Generated 16 batches of size 16 in 2.240 sec\n","Step 889     [0.679 sec/step, loss=0.99013, avg_loss=0.91314]\n","Step 890     [0.680 sec/step, loss=0.89468, avg_loss=0.91345]\n","Step 891     [0.675 sec/step, loss=0.88645, avg_loss=0.91354]\n","Step 892     [0.666 sec/step, loss=0.87038, avg_loss=0.91357]\n","Step 893     [0.666 sec/step, loss=0.94472, avg_loss=0.91317]\n","Step 894     [0.665 sec/step, loss=0.86655, avg_loss=0.91243]\n","Step 895     [0.663 sec/step, loss=0.91545, avg_loss=0.91253]\n","Step 896     [0.664 sec/step, loss=0.86840, avg_loss=0.91162]\n","Step 897     [0.664 sec/step, loss=0.87220, avg_loss=0.91120]\n","Step 898     [0.667 sec/step, loss=0.85146, avg_loss=0.91045]\n","Step 899     [0.667 sec/step, loss=0.95537, avg_loss=0.91080]\n","Step 900     [0.666 sec/step, loss=0.90131, avg_loss=0.91087]\n","Step 901     [0.662 sec/step, loss=0.87741, avg_loss=0.91107]\n","Step 902     [0.662 sec/step, loss=0.93082, avg_loss=0.91186]\n","Step 903     [0.670 sec/step, loss=0.87761, avg_loss=0.91067]\n","Generated 16 batches of size 16 in 2.268 sec\n","Step 904     [0.675 sec/step, loss=0.96917, avg_loss=0.91169]\n","Step 905     [0.676 sec/step, loss=0.91536, avg_loss=0.91174]\n","Step 906     [0.677 sec/step, loss=0.93994, avg_loss=0.91120]\n","Step 907     [0.673 sec/step, loss=0.84224, avg_loss=0.91013]\n","Step 908     [0.668 sec/step, loss=0.88809, avg_loss=0.91020]\n","Step 909     [0.667 sec/step, loss=0.85574, avg_loss=0.90969]\n","Step 910     [0.667 sec/step, loss=0.93440, avg_loss=0.90969]\n","Step 911     [0.668 sec/step, loss=0.87668, avg_loss=0.90937]\n","Step 912     [0.668 sec/step, loss=0.94958, avg_loss=0.91007]\n","Step 913     [0.666 sec/step, loss=0.96183, avg_loss=0.91011]\n","Step 914     [0.666 sec/step, loss=0.91065, avg_loss=0.90947]\n","Step 915     [0.664 sec/step, loss=0.92505, avg_loss=0.90949]\n","Step 916     [0.667 sec/step, loss=0.88505, avg_loss=0.90936]\n","Step 917     [0.667 sec/step, loss=0.86503, avg_loss=0.90920]\n","Step 918     [0.667 sec/step, loss=0.86147, avg_loss=0.90916]\n","Step 919     [0.673 sec/step, loss=0.90074, avg_loss=0.90847]\n","Generated 16 batches of size 16 in 2.168 sec\n","Step 920     [0.678 sec/step, loss=0.95725, avg_loss=0.90924]\n","Step 921     [0.677 sec/step, loss=0.87495, avg_loss=0.90850]\n","Step 922     [0.680 sec/step, loss=0.86644, avg_loss=0.90780]\n","Step 923     [0.671 sec/step, loss=0.88393, avg_loss=0.90754]\n","Step 924     [0.667 sec/step, loss=0.86851, avg_loss=0.90706]\n","Step 925     [0.666 sec/step, loss=0.89661, avg_loss=0.90609]\n","Step 926     [0.668 sec/step, loss=0.92934, avg_loss=0.90684]\n","Step 927     [0.664 sec/step, loss=0.85293, avg_loss=0.90667]\n","Step 928     [0.663 sec/step, loss=0.87767, avg_loss=0.90552]\n","Step 929     [0.664 sec/step, loss=0.93535, avg_loss=0.90579]\n","Step 930     [0.664 sec/step, loss=0.86399, avg_loss=0.90541]\n","Step 931     [0.666 sec/step, loss=0.93104, avg_loss=0.90520]\n","Step 932     [0.665 sec/step, loss=0.85307, avg_loss=0.90419]\n","Step 933     [0.665 sec/step, loss=0.95627, avg_loss=0.90520]\n","Step 934     [0.665 sec/step, loss=0.97520, avg_loss=0.90511]\n","Step 935     [0.677 sec/step, loss=0.85136, avg_loss=0.90474]\n","Generated 16 batches of size 16 in 2.045 sec\n","Step 936     [0.676 sec/step, loss=0.88714, avg_loss=0.90447]\n","Step 937     [0.672 sec/step, loss=0.85692, avg_loss=0.90448]\n","Step 938     [0.673 sec/step, loss=0.91552, avg_loss=0.90430]\n","Step 939     [0.669 sec/step, loss=0.85819, avg_loss=0.90389]\n","Step 940     [0.665 sec/step, loss=0.96047, avg_loss=0.90451]\n","Step 941     [0.664 sec/step, loss=0.88519, avg_loss=0.90435]\n","Step 942     [0.665 sec/step, loss=0.91175, avg_loss=0.90477]\n","Step 943     [0.667 sec/step, loss=0.90705, avg_loss=0.90448]\n","Step 944     [0.667 sec/step, loss=0.84836, avg_loss=0.90428]\n","Step 945     [0.666 sec/step, loss=0.85564, avg_loss=0.90290]\n","Step 946     [0.666 sec/step, loss=0.85098, avg_loss=0.90269]\n","Step 947     [0.666 sec/step, loss=0.93661, avg_loss=0.90254]\n","Step 948     [0.665 sec/step, loss=0.96200, avg_loss=0.90300]\n","Step 949     [0.666 sec/step, loss=0.85428, avg_loss=0.90308]\n","Step 950     [0.667 sec/step, loss=0.95924, avg_loss=0.90377]\n","Step 951     [0.674 sec/step, loss=0.93707, avg_loss=0.90399]\n","Generated 16 batches of size 16 in 2.136 sec\n","Step 952     [0.680 sec/step, loss=0.90742, avg_loss=0.90357]\n","Step 953     [0.679 sec/step, loss=0.88226, avg_loss=0.90323]\n","Step 954     [0.676 sec/step, loss=0.88900, avg_loss=0.90358]\n","Step 955     [0.668 sec/step, loss=0.87871, avg_loss=0.90283]\n","Step 956     [0.664 sec/step, loss=0.87019, avg_loss=0.90263]\n","Step 957     [0.663 sec/step, loss=0.87183, avg_loss=0.90253]\n","Step 958     [0.667 sec/step, loss=0.83906, avg_loss=0.90125]\n","Step 959     [0.667 sec/step, loss=0.91963, avg_loss=0.90144]\n","Step 960     [0.667 sec/step, loss=0.94926, avg_loss=0.90125]\n","Step 961     [0.668 sec/step, loss=0.93664, avg_loss=0.90178]\n","Step 962     [0.668 sec/step, loss=0.88443, avg_loss=0.90079]\n","Step 963     [0.668 sec/step, loss=0.86795, avg_loss=0.90083]\n","Step 964     [0.669 sec/step, loss=0.93084, avg_loss=0.90083]\n","Step 965     [0.669 sec/step, loss=0.94212, avg_loss=0.90048]\n","Step 966     [0.669 sec/step, loss=0.87210, avg_loss=0.89932]\n","Step 967     [0.674 sec/step, loss=0.89488, avg_loss=0.89952]\n","Generated 16 batches of size 16 in 2.247 sec\n","Step 968     [0.682 sec/step, loss=0.86849, avg_loss=0.89903]\n","Step 969     [0.682 sec/step, loss=0.85768, avg_loss=0.89869]\n","Step 970     [0.681 sec/step, loss=0.92629, avg_loss=0.89871]\n","Step 971     [0.681 sec/step, loss=0.84024, avg_loss=0.89843]\n","Step 972     [0.676 sec/step, loss=0.86394, avg_loss=0.89803]\n","Step 973     [0.675 sec/step, loss=0.84675, avg_loss=0.89788]\n","Step 974     [0.676 sec/step, loss=0.90762, avg_loss=0.89791]\n","Step 975     [0.676 sec/step, loss=0.84694, avg_loss=0.89784]\n","Step 976     [0.676 sec/step, loss=0.92221, avg_loss=0.89748]\n","Step 977     [0.675 sec/step, loss=0.82608, avg_loss=0.89631]\n","Step 978     [0.672 sec/step, loss=0.85697, avg_loss=0.89594]\n","Step 979     [0.671 sec/step, loss=0.82980, avg_loss=0.89568]\n","Step 980     [0.670 sec/step, loss=0.90020, avg_loss=0.89553]\n","Step 981     [0.669 sec/step, loss=0.84995, avg_loss=0.89472]\n","Step 982     [0.670 sec/step, loss=0.88740, avg_loss=0.89497]\n","Step 983     [0.675 sec/step, loss=0.86097, avg_loss=0.89471]\n","Generated 16 batches of size 16 in 2.183 sec\n","Step 984     [0.675 sec/step, loss=0.86278, avg_loss=0.89484]\n","Step 985     [0.675 sec/step, loss=0.95083, avg_loss=0.89462]\n","Step 986     [0.675 sec/step, loss=0.89261, avg_loss=0.89460]\n","Step 987     [0.670 sec/step, loss=0.87117, avg_loss=0.89413]\n","Step 988     [0.665 sec/step, loss=0.94587, avg_loss=0.89495]\n","Step 989     [0.663 sec/step, loss=0.83393, avg_loss=0.89339]\n","Step 990     [0.664 sec/step, loss=0.84208, avg_loss=0.89287]\n","Step 991     [0.665 sec/step, loss=0.92103, avg_loss=0.89321]\n","Step 992     [0.665 sec/step, loss=0.92883, avg_loss=0.89380]\n","Step 993     [0.665 sec/step, loss=0.87945, avg_loss=0.89314]\n","Step 994     [0.665 sec/step, loss=0.85145, avg_loss=0.89299]\n","Step 995     [0.665 sec/step, loss=0.91526, avg_loss=0.89299]\n","Step 996     [0.669 sec/step, loss=0.83111, avg_loss=0.89262]\n","Step 997     [0.669 sec/step, loss=0.93668, avg_loss=0.89326]\n","Step 998     [0.665 sec/step, loss=0.84267, avg_loss=0.89317]\n","Step 999     [0.672 sec/step, loss=0.91998, avg_loss=0.89282]\n","Generated 16 batches of size 16 in 2.142 sec\n","Step 1000    [0.675 sec/step, loss=0.82943, avg_loss=0.89210]\n","Saving audio and alignment...\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12632 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12620 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45224 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48513 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54868 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54644 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54801 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47141 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53952 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47560 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47144 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12632 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12620 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45224 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48513 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54868 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54644 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54801 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47141 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53952 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47560 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47144 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/train-step-000001000-align000.png\n","100% 1/1 [00:02<00:00,  2.78s/it]\n","Test finished for step 1000.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/test-step-000001000-align000.png\n"," 50% 1/2 [00:02<00:02,  2.25s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/test-step-000001000-align001.png\n","100% 2/2 [00:04<00:00,  2.23s/it]\n","Test finished for step 1000.\n","Step 1001    [0.675 sec/step, loss=0.85713, avg_loss=0.89190]\n","Step 1002    [0.675 sec/step, loss=0.86337, avg_loss=0.89122]\n","Step 1003    [0.668 sec/step, loss=0.87982, avg_loss=0.89125]\n","Step 1004    [0.664 sec/step, loss=0.92103, avg_loss=0.89077]\n","Step 1005    [0.664 sec/step, loss=0.83927, avg_loss=0.89000]\n","Step 1006    [0.663 sec/step, loss=0.83514, avg_loss=0.88896]\n","Step 1007    [0.664 sec/step, loss=0.83515, avg_loss=0.88889]\n","Step 1008    [0.665 sec/step, loss=0.91029, avg_loss=0.88911]\n","Step 1009    [0.666 sec/step, loss=0.92929, avg_loss=0.88984]\n","Step 1010    [0.665 sec/step, loss=0.80691, avg_loss=0.88857]\n","Step 1011    [0.665 sec/step, loss=0.87276, avg_loss=0.88853]\n","Step 1012    [0.664 sec/step, loss=0.89305, avg_loss=0.88796]\n","Step 1013    [0.664 sec/step, loss=0.83029, avg_loss=0.88665]\n","Step 1014    [0.668 sec/step, loss=0.84601, avg_loss=0.88600]\n","Generated 16 batches of size 16 in 2.133 sec\n","Step 1015    [0.676 sec/step, loss=0.90564, avg_loss=0.88581]\n","Step 1016    [0.673 sec/step, loss=0.95158, avg_loss=0.88647]\n","Step 1017    [0.673 sec/step, loss=0.88549, avg_loss=0.88668]\n","Step 1018    [0.676 sec/step, loss=0.86852, avg_loss=0.88675]\n","Step 1019    [0.669 sec/step, loss=0.86980, avg_loss=0.88644]\n","Step 1020    [0.668 sec/step, loss=0.82113, avg_loss=0.88508]\n","Step 1021    [0.668 sec/step, loss=0.82939, avg_loss=0.88462]\n","Step 1022    [0.664 sec/step, loss=0.86493, avg_loss=0.88461]\n","Step 1023    [0.665 sec/step, loss=0.93517, avg_loss=0.88512]\n","Step 1024    [0.665 sec/step, loss=0.87700, avg_loss=0.88520]\n","Step 1025    [0.667 sec/step, loss=0.85836, avg_loss=0.88482]\n","Step 1026    [0.665 sec/step, loss=0.85453, avg_loss=0.88407]\n","Step 1027    [0.667 sec/step, loss=0.92959, avg_loss=0.88484]\n","Step 1028    [0.666 sec/step, loss=0.83288, avg_loss=0.88439]\n","Step 1029    [0.665 sec/step, loss=0.83273, avg_loss=0.88337]\n","Step 1030    [0.670 sec/step, loss=0.94780, avg_loss=0.88420]\n","Step 1031    [0.674 sec/step, loss=0.84661, avg_loss=0.88336]\n","Generated 16 batches of size 16 in 2.167 sec\n","Step 1032    [0.675 sec/step, loss=0.85651, avg_loss=0.88339]\n","Step 1033    [0.675 sec/step, loss=0.86158, avg_loss=0.88245]\n","Step 1034    [0.675 sec/step, loss=0.92741, avg_loss=0.88197]\n","Step 1035    [0.662 sec/step, loss=0.85346, avg_loss=0.88199]\n","Step 1036    [0.661 sec/step, loss=0.90124, avg_loss=0.88213]\n","Step 1037    [0.662 sec/step, loss=0.87504, avg_loss=0.88231]\n","Step 1038    [0.666 sec/step, loss=0.83601, avg_loss=0.88152]\n","Step 1039    [0.670 sec/step, loss=0.82603, avg_loss=0.88120]\n","Step 1040    [0.669 sec/step, loss=0.82696, avg_loss=0.87986]\n","Step 1041    [0.669 sec/step, loss=0.84654, avg_loss=0.87947]\n","Step 1042    [0.668 sec/step, loss=0.84364, avg_loss=0.87879]\n","Step 1043    [0.665 sec/step, loss=0.81272, avg_loss=0.87785]\n","Step 1044    [0.666 sec/step, loss=0.83673, avg_loss=0.87773]\n","Step 1045    [0.667 sec/step, loss=0.91538, avg_loss=0.87833]\n","Step 1046    [0.674 sec/step, loss=0.88602, avg_loss=0.87868]\n","Generated 16 batches of size 16 in 2.099 sec\n","Step 1047    [0.679 sec/step, loss=0.87009, avg_loss=0.87802]\n","Step 1048    [0.679 sec/step, loss=0.86693, avg_loss=0.87707]\n","Step 1049    [0.677 sec/step, loss=0.91058, avg_loss=0.87763]\n","Step 1050    [0.678 sec/step, loss=0.87993, avg_loss=0.87684]\n","Step 1051    [0.671 sec/step, loss=0.94118, avg_loss=0.87688]\n","Step 1052    [0.666 sec/step, loss=0.87368, avg_loss=0.87654]\n","Step 1053    [0.667 sec/step, loss=0.93028, avg_loss=0.87702]\n","Step 1054    [0.667 sec/step, loss=0.91326, avg_loss=0.87726]\n","Step 1055    [0.667 sec/step, loss=0.85255, avg_loss=0.87700]\n","Step 1056    [0.665 sec/step, loss=0.87175, avg_loss=0.87702]\n","Step 1057    [0.669 sec/step, loss=0.84252, avg_loss=0.87672]\n","Step 1058    [0.668 sec/step, loss=0.85574, avg_loss=0.87689]\n","Step 1059    [0.670 sec/step, loss=0.91543, avg_loss=0.87685]\n","Step 1060    [0.670 sec/step, loss=0.92396, avg_loss=0.87659]\n","Step 1061    [0.668 sec/step, loss=0.92161, avg_loss=0.87644]\n","Step 1062    [0.674 sec/step, loss=0.84036, avg_loss=0.87600]\n","Step 1063    [0.678 sec/step, loss=0.90542, avg_loss=0.87638]\n","Generated 16 batches of size 16 in 2.165 sec\n","Step 1064    [0.678 sec/step, loss=0.81842, avg_loss=0.87525]\n","Step 1065    [0.679 sec/step, loss=0.87892, avg_loss=0.87462]\n","Step 1066    [0.679 sec/step, loss=0.90522, avg_loss=0.87495]\n","Step 1067    [0.673 sec/step, loss=0.84824, avg_loss=0.87449]\n","Step 1068    [0.665 sec/step, loss=0.84460, avg_loss=0.87425]\n","Step 1069    [0.664 sec/step, loss=0.83140, avg_loss=0.87399]\n","Step 1070    [0.664 sec/step, loss=0.85673, avg_loss=0.87329]\n","Step 1071    [0.660 sec/step, loss=0.88326, avg_loss=0.87372]\n","Step 1072    [0.660 sec/step, loss=0.81747, avg_loss=0.87326]\n","Step 1073    [0.660 sec/step, loss=0.88610, avg_loss=0.87365]\n","Step 1074    [0.658 sec/step, loss=0.85573, avg_loss=0.87313]\n","Step 1075    [0.660 sec/step, loss=0.87407, avg_loss=0.87340]\n","Step 1076    [0.660 sec/step, loss=0.90147, avg_loss=0.87319]\n","Step 1077    [0.661 sec/step, loss=0.83168, avg_loss=0.87325]\n","Step 1078    [0.666 sec/step, loss=0.83072, avg_loss=0.87299]\n","Step 1079    [0.667 sec/step, loss=0.86073, avg_loss=0.87330]\n","Generated 16 batches of size 16 in 2.165 sec\n","Step 1080    [0.670 sec/step, loss=0.86022, avg_loss=0.87290]\n","Step 1081    [0.670 sec/step, loss=0.91760, avg_loss=0.87357]\n","Step 1082    [0.668 sec/step, loss=0.82984, avg_loss=0.87300]\n","Step 1083    [0.661 sec/step, loss=0.93353, avg_loss=0.87372]\n","Step 1084    [0.661 sec/step, loss=0.82438, avg_loss=0.87334]\n","Step 1085    [0.661 sec/step, loss=0.90494, avg_loss=0.87288]\n","Step 1086    [0.660 sec/step, loss=0.82201, avg_loss=0.87217]\n","Step 1087    [0.661 sec/step, loss=0.82588, avg_loss=0.87172]\n","Step 1088    [0.663 sec/step, loss=0.85130, avg_loss=0.87078]\n","Step 1089    [0.664 sec/step, loss=0.83127, avg_loss=0.87075]\n","Step 1090    [0.664 sec/step, loss=0.90917, avg_loss=0.87142]\n","Step 1091    [0.663 sec/step, loss=0.83745, avg_loss=0.87058]\n","Step 1092    [0.662 sec/step, loss=0.80547, avg_loss=0.86935]\n","Step 1093    [0.663 sec/step, loss=0.86376, avg_loss=0.86919]\n","Step 1094    [0.668 sec/step, loss=0.81461, avg_loss=0.86883]\n","Step 1095    [0.672 sec/step, loss=0.83307, avg_loss=0.86800]\n","Generated 16 batches of size 16 in 2.318 sec\n","Step 1096    [0.674 sec/step, loss=0.81262, avg_loss=0.86782]\n","Step 1097    [0.674 sec/step, loss=0.85426, avg_loss=0.86699]\n","Step 1098    [0.675 sec/step, loss=0.91660, avg_loss=0.86773]\n","Step 1099    [0.668 sec/step, loss=0.86396, avg_loss=0.86717]\n","Step 1100    [0.665 sec/step, loss=0.89518, avg_loss=0.86783]\n","Step 1101    [0.664 sec/step, loss=0.90120, avg_loss=0.86827]\n","Step 1102    [0.665 sec/step, loss=0.91136, avg_loss=0.86875]\n","Step 1103    [0.665 sec/step, loss=0.90267, avg_loss=0.86898]\n","Step 1104    [0.664 sec/step, loss=0.90348, avg_loss=0.86880]\n","Step 1105    [0.663 sec/step, loss=0.82029, avg_loss=0.86861]\n","Step 1106    [0.664 sec/step, loss=0.92259, avg_loss=0.86949]\n","Step 1107    [0.663 sec/step, loss=0.87047, avg_loss=0.86984]\n","Step 1108    [0.661 sec/step, loss=0.81780, avg_loss=0.86892]\n","Step 1109    [0.661 sec/step, loss=0.86233, avg_loss=0.86825]\n","Step 1110    [0.670 sec/step, loss=0.84845, avg_loss=0.86866]\n","Generated 16 batches of size 16 in 2.243 sec\n","Step 1111    [0.674 sec/step, loss=0.83059, avg_loss=0.86824]\n","Step 1112    [0.674 sec/step, loss=0.84198, avg_loss=0.86773]\n","Step 1113    [0.675 sec/step, loss=0.89007, avg_loss=0.86833]\n","Step 1114    [0.672 sec/step, loss=0.81373, avg_loss=0.86801]\n","Step 1115    [0.666 sec/step, loss=0.86370, avg_loss=0.86759]\n","Step 1116    [0.666 sec/step, loss=0.85529, avg_loss=0.86662]\n","Step 1117    [0.665 sec/step, loss=0.81253, avg_loss=0.86589]\n","Step 1118    [0.667 sec/step, loss=0.82152, avg_loss=0.86542]\n","Step 1119    [0.667 sec/step, loss=0.83136, avg_loss=0.86504]\n","Step 1120    [0.664 sec/step, loss=0.88725, avg_loss=0.86570]\n","Step 1121    [0.664 sec/step, loss=0.81429, avg_loss=0.86555]\n","Step 1122    [0.664 sec/step, loss=0.90991, avg_loss=0.86600]\n","Step 1123    [0.664 sec/step, loss=0.85265, avg_loss=0.86517]\n","Step 1124    [0.664 sec/step, loss=0.82006, avg_loss=0.86460]\n","Step 1125    [0.661 sec/step, loss=0.79097, avg_loss=0.86393]\n","Step 1126    [0.665 sec/step, loss=0.81421, avg_loss=0.86353]\n","Generated 16 batches of size 16 in 2.144 sec\n","Step 1127    [0.673 sec/step, loss=0.83646, avg_loss=0.86260]\n","Step 1128    [0.673 sec/step, loss=0.81328, avg_loss=0.86240]\n","Step 1129    [0.678 sec/step, loss=0.81130, avg_loss=0.86219]\n","Step 1130    [0.675 sec/step, loss=0.88914, avg_loss=0.86160]\n","Step 1131    [0.669 sec/step, loss=0.83951, avg_loss=0.86153]\n","Step 1132    [0.669 sec/step, loss=0.89729, avg_loss=0.86194]\n","Step 1133    [0.671 sec/step, loss=0.85328, avg_loss=0.86185]\n","Step 1134    [0.670 sec/step, loss=0.87807, avg_loss=0.86136]\n","Step 1135    [0.669 sec/step, loss=0.84579, avg_loss=0.86128]\n","Step 1136    [0.671 sec/step, loss=0.89318, avg_loss=0.86120]\n","Step 1137    [0.670 sec/step, loss=0.87954, avg_loss=0.86125]\n","Step 1138    [0.666 sec/step, loss=0.80688, avg_loss=0.86096]\n","Step 1139    [0.661 sec/step, loss=0.82249, avg_loss=0.86092]\n","Step 1140    [0.666 sec/step, loss=0.80072, avg_loss=0.86066]\n","Step 1141    [0.667 sec/step, loss=0.85119, avg_loss=0.86071]\n","Step 1142    [0.671 sec/step, loss=0.82641, avg_loss=0.86053]\n","Step 1143    [0.675 sec/step, loss=0.80375, avg_loss=0.86044]\n","Generated 16 batches of size 16 in 2.314 sec\n","Step 1144    [0.679 sec/step, loss=0.83187, avg_loss=0.86039]\n","Step 1145    [0.679 sec/step, loss=0.82851, avg_loss=0.85953]\n","Step 1146    [0.672 sec/step, loss=0.82992, avg_loss=0.85896]\n","Step 1147    [0.667 sec/step, loss=0.89217, avg_loss=0.85919]\n","Step 1148    [0.666 sec/step, loss=0.87373, avg_loss=0.85925]\n","Step 1149    [0.667 sec/step, loss=0.91550, avg_loss=0.85930]\n","Step 1150    [0.666 sec/step, loss=0.83147, avg_loss=0.85882]\n","Step 1151    [0.667 sec/step, loss=0.87805, avg_loss=0.85819]\n","Step 1152    [0.667 sec/step, loss=0.81852, avg_loss=0.85764]\n","Step 1153    [0.666 sec/step, loss=0.80620, avg_loss=0.85639]\n","Step 1154    [0.667 sec/step, loss=0.85403, avg_loss=0.85580]\n","Step 1155    [0.668 sec/step, loss=0.84855, avg_loss=0.85576]\n","Step 1156    [0.667 sec/step, loss=0.88616, avg_loss=0.85591]\n","Step 1157    [0.663 sec/step, loss=0.81709, avg_loss=0.85565]\n","Step 1158    [0.665 sec/step, loss=0.82871, avg_loss=0.85538]\n","Generated 16 batches of size 16 in 2.238 sec\n","Step 1159    [0.675 sec/step, loss=0.80577, avg_loss=0.85429]\n","Step 1160    [0.676 sec/step, loss=0.88385, avg_loss=0.85388]\n","Step 1161    [0.675 sec/step, loss=0.86765, avg_loss=0.85334]\n","Step 1162    [0.671 sec/step, loss=0.86596, avg_loss=0.85360]\n","Step 1163    [0.667 sec/step, loss=0.79202, avg_loss=0.85247]\n","Step 1164    [0.667 sec/step, loss=0.90653, avg_loss=0.85335]\n","Step 1165    [0.665 sec/step, loss=0.82652, avg_loss=0.85282]\n","Step 1166    [0.665 sec/step, loss=0.83609, avg_loss=0.85213]\n","Step 1167    [0.665 sec/step, loss=0.80139, avg_loss=0.85166]\n","Step 1168    [0.668 sec/step, loss=0.80686, avg_loss=0.85129]\n","Step 1169    [0.668 sec/step, loss=0.79584, avg_loss=0.85093]\n","Step 1170    [0.669 sec/step, loss=0.80707, avg_loss=0.85043]\n","Step 1171    [0.669 sec/step, loss=0.87333, avg_loss=0.85033]\n","Step 1172    [0.670 sec/step, loss=0.88989, avg_loss=0.85106]\n","Step 1173    [0.671 sec/step, loss=0.82832, avg_loss=0.85048]\n","Step 1174    [0.678 sec/step, loss=0.93585, avg_loss=0.85128]\n","Generated 16 batches of size 16 in 2.186 sec\n","Step 1175    [0.681 sec/step, loss=0.83261, avg_loss=0.85087]\n","Step 1176    [0.680 sec/step, loss=0.82474, avg_loss=0.85010]\n","Step 1177    [0.679 sec/step, loss=0.86550, avg_loss=0.85044]\n","Step 1178    [0.675 sec/step, loss=0.89110, avg_loss=0.85104]\n","Step 1179    [0.672 sec/step, loss=0.85146, avg_loss=0.85095]\n","Step 1180    [0.669 sec/step, loss=0.91627, avg_loss=0.85151]\n","Step 1181    [0.669 sec/step, loss=0.83668, avg_loss=0.85070]\n","Step 1182    [0.670 sec/step, loss=0.84717, avg_loss=0.85087]\n","Step 1183    [0.671 sec/step, loss=0.82690, avg_loss=0.84981]\n","Step 1184    [0.669 sec/step, loss=0.83876, avg_loss=0.84995]\n","Step 1185    [0.669 sec/step, loss=0.88262, avg_loss=0.84973]\n","Step 1186    [0.669 sec/step, loss=0.82118, avg_loss=0.84972]\n","Step 1187    [0.672 sec/step, loss=0.79753, avg_loss=0.84944]\n","Step 1188    [0.670 sec/step, loss=0.81717, avg_loss=0.84910]\n","Step 1189    [0.671 sec/step, loss=0.84378, avg_loss=0.84922]\n","Step 1190    [0.675 sec/step, loss=0.79901, avg_loss=0.84812]\n","Step 1191    [0.680 sec/step, loss=0.84512, avg_loss=0.84820]\n","Generated 16 batches of size 16 in 2.149 sec\n","Step 1192    [0.681 sec/step, loss=0.89892, avg_loss=0.84913]\n","Step 1193    [0.681 sec/step, loss=0.87905, avg_loss=0.84928]\n","Step 1194    [0.676 sec/step, loss=0.80435, avg_loss=0.84918]\n","Step 1195    [0.671 sec/step, loss=0.88139, avg_loss=0.84966]\n","Step 1196    [0.665 sec/step, loss=0.81754, avg_loss=0.84971]\n","Step 1197    [0.665 sec/step, loss=0.82346, avg_loss=0.84940]\n","Step 1198    [0.665 sec/step, loss=0.87608, avg_loss=0.84900]\n","Step 1199    [0.669 sec/step, loss=0.80489, avg_loss=0.84841]\n","Step 1200    [0.668 sec/step, loss=0.81974, avg_loss=0.84765]\n","Step 1201    [0.669 sec/step, loss=0.82390, avg_loss=0.84688]\n","Step 1202    [0.669 sec/step, loss=0.88181, avg_loss=0.84659]\n","Step 1203    [0.670 sec/step, loss=0.82724, avg_loss=0.84583]\n","Step 1204    [0.668 sec/step, loss=0.85762, avg_loss=0.84537]\n","Step 1205    [0.669 sec/step, loss=0.86864, avg_loss=0.84586]\n","Step 1206    [0.676 sec/step, loss=0.87545, avg_loss=0.84539]\n","Generated 16 batches of size 16 in 2.145 sec\n","Step 1207    [0.682 sec/step, loss=0.83905, avg_loss=0.84507]\n","Step 1208    [0.683 sec/step, loss=0.83573, avg_loss=0.84525]\n","Step 1209    [0.683 sec/step, loss=0.89271, avg_loss=0.84555]\n","Step 1210    [0.674 sec/step, loss=0.80647, avg_loss=0.84513]\n","Step 1211    [0.668 sec/step, loss=0.78494, avg_loss=0.84468]\n","Step 1212    [0.668 sec/step, loss=0.79745, avg_loss=0.84423]\n","Step 1213    [0.668 sec/step, loss=0.80227, avg_loss=0.84335]\n","Step 1214    [0.668 sec/step, loss=0.79757, avg_loss=0.84319]\n","Step 1215    [0.666 sec/step, loss=0.80790, avg_loss=0.84264]\n","Step 1216    [0.666 sec/step, loss=0.80311, avg_loss=0.84211]\n","Step 1217    [0.666 sec/step, loss=0.80242, avg_loss=0.84201]\n","Step 1218    [0.664 sec/step, loss=0.82348, avg_loss=0.84203]\n","Step 1219    [0.664 sec/step, loss=0.87707, avg_loss=0.84249]\n","Step 1220    [0.664 sec/step, loss=0.80311, avg_loss=0.84165]\n","Step 1221    [0.664 sec/step, loss=0.82120, avg_loss=0.84172]\n","Step 1222    [0.669 sec/step, loss=0.85863, avg_loss=0.84120]\n","Step 1223    [0.672 sec/step, loss=0.77588, avg_loss=0.84044]\n","Generated 16 batches of size 16 in 2.312 sec\n","Step 1224    [0.675 sec/step, loss=0.86461, avg_loss=0.84088]\n","Step 1225    [0.675 sec/step, loss=0.79082, avg_loss=0.84088]\n","Step 1226    [0.671 sec/step, loss=0.85495, avg_loss=0.84129]\n","Step 1227    [0.663 sec/step, loss=0.83532, avg_loss=0.84128]\n","Step 1228    [0.667 sec/step, loss=0.79385, avg_loss=0.84108]\n","Step 1229    [0.663 sec/step, loss=0.81686, avg_loss=0.84114]\n","Step 1230    [0.662 sec/step, loss=0.86949, avg_loss=0.84094]\n","Step 1231    [0.662 sec/step, loss=0.79817, avg_loss=0.84053]\n","Step 1232    [0.664 sec/step, loss=0.82097, avg_loss=0.83976]\n","Step 1233    [0.662 sec/step, loss=0.77446, avg_loss=0.83898]\n","Step 1234    [0.663 sec/step, loss=0.83406, avg_loss=0.83854]\n","Step 1235    [0.662 sec/step, loss=0.83423, avg_loss=0.83842]\n","Step 1236    [0.666 sec/step, loss=0.78946, avg_loss=0.83738]\n","Step 1237    [0.665 sec/step, loss=0.78906, avg_loss=0.83648]\n","Step 1238    [0.672 sec/step, loss=0.86766, avg_loss=0.83709]\n","Generated 16 batches of size 16 in 2.130 sec\n","Step 1239    [0.678 sec/step, loss=0.86319, avg_loss=0.83749]\n","Step 1240    [0.674 sec/step, loss=0.83682, avg_loss=0.83785]\n","Step 1241    [0.672 sec/step, loss=0.81592, avg_loss=0.83750]\n","Step 1242    [0.670 sec/step, loss=0.83515, avg_loss=0.83759]\n","Step 1243    [0.667 sec/step, loss=0.87575, avg_loss=0.83831]\n","Step 1244    [0.662 sec/step, loss=0.80992, avg_loss=0.83809]\n","Step 1245    [0.663 sec/step, loss=0.86859, avg_loss=0.83849]\n","Step 1246    [0.664 sec/step, loss=0.82495, avg_loss=0.83844]\n","Step 1247    [0.663 sec/step, loss=0.86862, avg_loss=0.83820]\n","Step 1248    [0.664 sec/step, loss=0.79097, avg_loss=0.83738]\n","Step 1249    [0.663 sec/step, loss=0.80634, avg_loss=0.83629]\n","Step 1250    [0.664 sec/step, loss=0.83249, avg_loss=0.83630]\n","Step 1251    [0.664 sec/step, loss=0.84904, avg_loss=0.83601]\n","Step 1252    [0.663 sec/step, loss=0.85783, avg_loss=0.83640]\n","Step 1253    [0.663 sec/step, loss=0.82680, avg_loss=0.83660]\n","Step 1254    [0.669 sec/step, loss=0.85914, avg_loss=0.83666]\n","Generated 16 batches of size 16 in 2.271 sec\n","Step 1255    [0.677 sec/step, loss=0.79652, avg_loss=0.83614]\n","Step 1256    [0.676 sec/step, loss=0.76296, avg_loss=0.83490]\n","Step 1257    [0.679 sec/step, loss=0.81599, avg_loss=0.83489]\n","Step 1258    [0.674 sec/step, loss=0.88254, avg_loss=0.83543]\n","Step 1259    [0.663 sec/step, loss=0.80794, avg_loss=0.83545]\n","Step 1260    [0.661 sec/step, loss=0.81976, avg_loss=0.83481]\n","Step 1261    [0.662 sec/step, loss=0.79558, avg_loss=0.83409]\n","Step 1262    [0.660 sec/step, loss=0.82113, avg_loss=0.83364]\n","Step 1263    [0.660 sec/step, loss=0.80936, avg_loss=0.83382]\n","Step 1264    [0.661 sec/step, loss=0.79479, avg_loss=0.83270]\n","Step 1265    [0.663 sec/step, loss=0.85651, avg_loss=0.83300]\n","Step 1266    [0.663 sec/step, loss=0.78573, avg_loss=0.83250]\n","Step 1267    [0.663 sec/step, loss=0.81221, avg_loss=0.83260]\n","Step 1268    [0.659 sec/step, loss=0.79276, avg_loss=0.83246]\n","Step 1269    [0.664 sec/step, loss=0.79438, avg_loss=0.83245]\n","Step 1270    [0.670 sec/step, loss=0.83298, avg_loss=0.83271]\n","Step 1271    [0.673 sec/step, loss=0.84653, avg_loss=0.83244]\n","Generated 16 batches of size 16 in 2.143 sec\n","Step 1272    [0.675 sec/step, loss=0.82933, avg_loss=0.83183]\n","Step 1273    [0.675 sec/step, loss=0.89311, avg_loss=0.83248]\n","Step 1274    [0.667 sec/step, loss=0.78466, avg_loss=0.83097]\n","Step 1275    [0.663 sec/step, loss=0.85970, avg_loss=0.83124]\n","Step 1276    [0.663 sec/step, loss=0.87069, avg_loss=0.83170]\n","Step 1277    [0.664 sec/step, loss=0.82977, avg_loss=0.83134]\n","Step 1278    [0.663 sec/step, loss=0.80704, avg_loss=0.83050]\n","Step 1279    [0.662 sec/step, loss=0.78961, avg_loss=0.82988]\n","Step 1280    [0.662 sec/step, loss=0.86739, avg_loss=0.82939]\n","Step 1281    [0.664 sec/step, loss=0.81541, avg_loss=0.82918]\n","Step 1282    [0.662 sec/step, loss=0.79857, avg_loss=0.82870]\n","Step 1283    [0.663 sec/step, loss=0.87008, avg_loss=0.82913]\n","Step 1284    [0.660 sec/step, loss=0.78596, avg_loss=0.82860]\n","Step 1285    [0.659 sec/step, loss=0.85447, avg_loss=0.82832]\n","Step 1286    [0.665 sec/step, loss=0.79734, avg_loss=0.82808]\n","Step 1287    [0.666 sec/step, loss=0.85108, avg_loss=0.82861]\n","Generated 16 batches of size 16 in 2.172 sec\n","Step 1288    [0.666 sec/step, loss=0.80465, avg_loss=0.82849]\n","Step 1289    [0.664 sec/step, loss=0.79724, avg_loss=0.82802]\n","Step 1290    [0.664 sec/step, loss=0.79866, avg_loss=0.82802]\n","Step 1291    [0.660 sec/step, loss=0.82620, avg_loss=0.82783]\n","Step 1292    [0.661 sec/step, loss=0.85804, avg_loss=0.82742]\n","Step 1293    [0.659 sec/step, loss=0.79234, avg_loss=0.82656]\n","Step 1294    [0.661 sec/step, loss=0.82493, avg_loss=0.82676]\n","Step 1295    [0.662 sec/step, loss=0.81916, avg_loss=0.82614]\n","Step 1296    [0.661 sec/step, loss=0.87583, avg_loss=0.82672]\n","Step 1297    [0.661 sec/step, loss=0.78201, avg_loss=0.82631]\n","Step 1298    [0.661 sec/step, loss=0.86672, avg_loss=0.82621]\n","Step 1299    [0.656 sec/step, loss=0.78194, avg_loss=0.82598]\n","Step 1300    [0.656 sec/step, loss=0.83276, avg_loss=0.82611]\n","Step 1301    [0.656 sec/step, loss=0.79178, avg_loss=0.82579]\n","Step 1302    [0.661 sec/step, loss=0.80463, avg_loss=0.82502]\n","Step 1303    [0.663 sec/step, loss=0.77917, avg_loss=0.82454]\n","Generated 16 batches of size 16 in 2.091 sec\n","Step 1304    [0.666 sec/step, loss=0.82109, avg_loss=0.82418]\n","Step 1305    [0.670 sec/step, loss=0.78726, avg_loss=0.82336]\n","Step 1306    [0.664 sec/step, loss=0.89397, avg_loss=0.82355]\n","Step 1307    [0.659 sec/step, loss=0.80211, avg_loss=0.82318]\n","Step 1308    [0.659 sec/step, loss=0.81642, avg_loss=0.82298]\n","Step 1309    [0.660 sec/step, loss=0.82045, avg_loss=0.82226]\n","Step 1310    [0.661 sec/step, loss=0.85599, avg_loss=0.82276]\n","Step 1311    [0.663 sec/step, loss=0.81440, avg_loss=0.82305]\n","Step 1312    [0.664 sec/step, loss=0.86821, avg_loss=0.82376]\n","Step 1313    [0.668 sec/step, loss=0.78758, avg_loss=0.82361]\n","Step 1314    [0.668 sec/step, loss=0.80009, avg_loss=0.82364]\n","Step 1315    [0.669 sec/step, loss=0.85324, avg_loss=0.82409]\n","Step 1316    [0.669 sec/step, loss=0.79365, avg_loss=0.82400]\n","Step 1317    [0.669 sec/step, loss=0.78702, avg_loss=0.82384]\n","Step 1318    [0.670 sec/step, loss=0.84130, avg_loss=0.82402]\n","Step 1319    [0.675 sec/step, loss=0.77660, avg_loss=0.82302]\n","Generated 16 batches of size 16 in 2.125 sec\n","Step 1320    [0.677 sec/step, loss=0.79307, avg_loss=0.82292]\n","Step 1321    [0.676 sec/step, loss=0.85741, avg_loss=0.82328]\n","Step 1322    [0.673 sec/step, loss=0.85789, avg_loss=0.82327]\n","Step 1323    [0.671 sec/step, loss=0.80666, avg_loss=0.82358]\n","Step 1324    [0.668 sec/step, loss=0.81420, avg_loss=0.82307]\n","Step 1325    [0.669 sec/step, loss=0.78526, avg_loss=0.82302]\n","Step 1326    [0.669 sec/step, loss=0.80391, avg_loss=0.82251]\n","Step 1327    [0.668 sec/step, loss=0.79308, avg_loss=0.82209]\n","Step 1328    [0.664 sec/step, loss=0.83630, avg_loss=0.82251]\n","Step 1329    [0.665 sec/step, loss=0.86064, avg_loss=0.82295]\n","Step 1330    [0.665 sec/step, loss=0.77642, avg_loss=0.82202]\n","Step 1331    [0.666 sec/step, loss=0.82911, avg_loss=0.82233]\n","Step 1332    [0.663 sec/step, loss=0.78897, avg_loss=0.82201]\n","Step 1333    [0.666 sec/step, loss=0.80683, avg_loss=0.82233]\n","Step 1334    [0.680 sec/step, loss=0.82200, avg_loss=0.82221]\n","Generated 16 batches of size 16 in 2.057 sec\n","Step 1335    [0.681 sec/step, loss=0.78598, avg_loss=0.82173]\n","Step 1336    [0.677 sec/step, loss=0.84116, avg_loss=0.82224]\n","Step 1337    [0.678 sec/step, loss=0.83556, avg_loss=0.82271]\n","Step 1338    [0.672 sec/step, loss=0.79656, avg_loss=0.82200]\n","Step 1339    [0.667 sec/step, loss=0.80291, avg_loss=0.82140]\n","Step 1340    [0.667 sec/step, loss=0.87129, avg_loss=0.82174]\n","Step 1341    [0.666 sec/step, loss=0.78358, avg_loss=0.82142]\n","Step 1342    [0.666 sec/step, loss=0.86230, avg_loss=0.82169]\n","Step 1343    [0.666 sec/step, loss=0.80462, avg_loss=0.82098]\n","Step 1344    [0.666 sec/step, loss=0.85110, avg_loss=0.82139]\n","Step 1345    [0.666 sec/step, loss=0.78389, avg_loss=0.82054]\n","Step 1346    [0.666 sec/step, loss=0.78649, avg_loss=0.82016]\n","Step 1347    [0.666 sec/step, loss=0.82811, avg_loss=0.81975]\n","Step 1348    [0.666 sec/step, loss=0.80249, avg_loss=0.81987]\n","Step 1349    [0.665 sec/step, loss=0.83854, avg_loss=0.82019]\n","Step 1350    [0.672 sec/step, loss=0.84847, avg_loss=0.82035]\n","Generated 16 batches of size 16 in 2.075 sec\n","Step 1351    [0.676 sec/step, loss=0.86148, avg_loss=0.82047]\n","Step 1352    [0.676 sec/step, loss=0.76592, avg_loss=0.81955]\n","Step 1353    [0.680 sec/step, loss=0.79250, avg_loss=0.81921]\n","Step 1354    [0.672 sec/step, loss=0.80474, avg_loss=0.81867]\n","Step 1355    [0.666 sec/step, loss=0.80738, avg_loss=0.81878]\n","Step 1356    [0.666 sec/step, loss=0.78293, avg_loss=0.81898]\n","Step 1357    [0.664 sec/step, loss=0.81047, avg_loss=0.81892]\n","Step 1358    [0.664 sec/step, loss=0.84887, avg_loss=0.81858]\n","Step 1359    [0.664 sec/step, loss=0.78135, avg_loss=0.81832]\n","Step 1360    [0.665 sec/step, loss=0.80119, avg_loss=0.81813]\n","Step 1361    [0.666 sec/step, loss=0.83791, avg_loss=0.81856]\n","Step 1362    [0.667 sec/step, loss=0.84438, avg_loss=0.81879]\n","Step 1363    [0.666 sec/step, loss=0.82385, avg_loss=0.81893]\n","Step 1364    [0.666 sec/step, loss=0.83628, avg_loss=0.81935]\n","Step 1365    [0.669 sec/step, loss=0.77026, avg_loss=0.81849]\n","Step 1366    [0.673 sec/step, loss=0.77854, avg_loss=0.81841]\n","Step 1367    [0.677 sec/step, loss=0.75684, avg_loss=0.81786]\n","Generated 16 batches of size 16 in 2.417 sec\n","Step 1368    [0.681 sec/step, loss=0.79710, avg_loss=0.81790]\n","Step 1369    [0.678 sec/step, loss=0.80156, avg_loss=0.81798]\n","Step 1370    [0.671 sec/step, loss=0.78055, avg_loss=0.81745]\n","Step 1371    [0.668 sec/step, loss=0.85430, avg_loss=0.81753]\n","Step 1372    [0.666 sec/step, loss=0.77781, avg_loss=0.81701]\n","Step 1373    [0.665 sec/step, loss=0.77350, avg_loss=0.81582]\n","Step 1374    [0.666 sec/step, loss=0.79671, avg_loss=0.81594]\n","Step 1375    [0.667 sec/step, loss=0.78925, avg_loss=0.81523]\n","Step 1376    [0.666 sec/step, loss=0.76467, avg_loss=0.81417]\n","Step 1377    [0.668 sec/step, loss=0.78902, avg_loss=0.81377]\n","Step 1378    [0.669 sec/step, loss=0.83253, avg_loss=0.81402]\n","Step 1379    [0.669 sec/step, loss=0.76103, avg_loss=0.81373]\n","Step 1380    [0.669 sec/step, loss=0.77548, avg_loss=0.81282]\n","Step 1381    [0.668 sec/step, loss=0.80452, avg_loss=0.81271]\n","Step 1382    [0.671 sec/step, loss=0.83048, avg_loss=0.81303]\n","Generated 16 batches of size 16 in 2.244 sec\n","Step 1383    [0.682 sec/step, loss=0.76556, avg_loss=0.81198]\n","Step 1384    [0.683 sec/step, loss=0.77179, avg_loss=0.81184]\n","Step 1385    [0.683 sec/step, loss=0.76324, avg_loss=0.81093]\n","Step 1386    [0.678 sec/step, loss=0.83000, avg_loss=0.81125]\n","Step 1387    [0.673 sec/step, loss=0.74587, avg_loss=0.81020]\n","Step 1388    [0.673 sec/step, loss=0.77302, avg_loss=0.80989]\n","Step 1389    [0.673 sec/step, loss=0.83926, avg_loss=0.81031]\n","Step 1390    [0.669 sec/step, loss=0.84661, avg_loss=0.81078]\n","Step 1391    [0.670 sec/step, loss=0.77889, avg_loss=0.81031]\n","Step 1392    [0.670 sec/step, loss=0.79067, avg_loss=0.80964]\n","Step 1393    [0.670 sec/step, loss=0.75725, avg_loss=0.80929]\n","Step 1394    [0.670 sec/step, loss=0.80106, avg_loss=0.80905]\n","Step 1395    [0.669 sec/step, loss=0.77194, avg_loss=0.80858]\n","Step 1396    [0.669 sec/step, loss=0.78330, avg_loss=0.80765]\n","Step 1397    [0.668 sec/step, loss=0.75530, avg_loss=0.80738]\n","Step 1398    [0.676 sec/step, loss=0.83366, avg_loss=0.80705]\n","Generated 16 batches of size 16 in 2.154 sec\n","Step 1399    [0.681 sec/step, loss=0.76281, avg_loss=0.80686]\n","Step 1400    [0.682 sec/step, loss=0.82648, avg_loss=0.80680]\n","Step 1401    [0.686 sec/step, loss=0.77452, avg_loss=0.80663]\n","Step 1402    [0.681 sec/step, loss=0.82013, avg_loss=0.80678]\n","Step 1403    [0.677 sec/step, loss=0.75383, avg_loss=0.80653]\n","Step 1404    [0.674 sec/step, loss=0.84806, avg_loss=0.80680]\n","Step 1405    [0.669 sec/step, loss=0.80490, avg_loss=0.80697]\n","Step 1406    [0.668 sec/step, loss=0.76654, avg_loss=0.80570]\n","Step 1407    [0.668 sec/step, loss=0.76097, avg_loss=0.80529]\n","Step 1408    [0.666 sec/step, loss=0.75949, avg_loss=0.80472]\n","Step 1409    [0.666 sec/step, loss=0.79673, avg_loss=0.80448]\n","Step 1410    [0.664 sec/step, loss=0.73296, avg_loss=0.80325]\n","Step 1411    [0.663 sec/step, loss=0.77675, avg_loss=0.80288]\n","Step 1412    [0.663 sec/step, loss=0.77498, avg_loss=0.80194]\n","Step 1413    [0.661 sec/step, loss=0.79601, avg_loss=0.80203]\n","Step 1414    [0.669 sec/step, loss=0.80143, avg_loss=0.80204]\n","Generated 16 batches of size 16 in 2.232 sec\n","Step 1415    [0.671 sec/step, loss=0.83043, avg_loss=0.80181]\n","Step 1416    [0.676 sec/step, loss=0.76362, avg_loss=0.80151]\n","Step 1417    [0.676 sec/step, loss=0.75595, avg_loss=0.80120]\n","Step 1418    [0.673 sec/step, loss=0.82523, avg_loss=0.80104]\n","Step 1419    [0.669 sec/step, loss=0.82114, avg_loss=0.80149]\n","Step 1420    [0.666 sec/step, loss=0.81234, avg_loss=0.80168]\n","Step 1421    [0.666 sec/step, loss=0.76413, avg_loss=0.80075]\n","Step 1422    [0.666 sec/step, loss=0.81501, avg_loss=0.80032]\n","Step 1423    [0.663 sec/step, loss=0.73682, avg_loss=0.79962]\n","Step 1424    [0.663 sec/step, loss=0.82955, avg_loss=0.79977]\n","Step 1425    [0.664 sec/step, loss=0.82217, avg_loss=0.80014]\n","Step 1426    [0.665 sec/step, loss=0.75517, avg_loss=0.79965]\n","Step 1427    [0.664 sec/step, loss=0.76641, avg_loss=0.79939]\n","Step 1428    [0.665 sec/step, loss=0.78854, avg_loss=0.79891]\n","Step 1429    [0.666 sec/step, loss=0.78223, avg_loss=0.79813]\n","Step 1430    [0.679 sec/step, loss=0.77052, avg_loss=0.79807]\n","Generated 16 batches of size 16 in 2.070 sec\n","Step 1431    [0.679 sec/step, loss=0.77354, avg_loss=0.79751]\n","Step 1432    [0.680 sec/step, loss=0.80984, avg_loss=0.79772]\n","Step 1433    [0.678 sec/step, loss=0.77242, avg_loss=0.79738]\n","Step 1434    [0.664 sec/step, loss=0.81216, avg_loss=0.79728]\n","Step 1435    [0.666 sec/step, loss=0.78259, avg_loss=0.79724]\n","Step 1436    [0.664 sec/step, loss=0.75499, avg_loss=0.79638]\n","Step 1437    [0.663 sec/step, loss=0.76294, avg_loss=0.79566]\n","Step 1438    [0.662 sec/step, loss=0.81080, avg_loss=0.79580]\n","Step 1439    [0.662 sec/step, loss=0.75202, avg_loss=0.79529]\n","Step 1440    [0.662 sec/step, loss=0.75745, avg_loss=0.79415]\n","Step 1441    [0.662 sec/step, loss=0.81420, avg_loss=0.79446]\n","Step 1442    [0.661 sec/step, loss=0.76364, avg_loss=0.79347]\n","Step 1443    [0.662 sec/step, loss=0.78196, avg_loss=0.79324]\n","Step 1444    [0.663 sec/step, loss=0.77941, avg_loss=0.79253]\n","Step 1445    [0.663 sec/step, loss=0.76602, avg_loss=0.79235]\n","Step 1446    [0.666 sec/step, loss=0.75907, avg_loss=0.79207]\n","Generated 16 batches of size 16 in 2.105 sec\n","Step 1447    [0.672 sec/step, loss=0.78398, avg_loss=0.79163]\n","Step 1448    [0.673 sec/step, loss=0.82189, avg_loss=0.79183]\n","Step 1449    [0.674 sec/step, loss=0.82003, avg_loss=0.79164]\n","Step 1450    [0.671 sec/step, loss=0.77485, avg_loss=0.79090]\n","Step 1451    [0.666 sec/step, loss=0.73808, avg_loss=0.78967]\n","Step 1452    [0.667 sec/step, loss=0.79263, avg_loss=0.78994]\n","Step 1453    [0.663 sec/step, loss=0.80849, avg_loss=0.79010]\n","Step 1454    [0.664 sec/step, loss=0.81906, avg_loss=0.79024]\n","Step 1455    [0.663 sec/step, loss=0.78363, avg_loss=0.79000]\n","Step 1456    [0.665 sec/step, loss=0.78037, avg_loss=0.78998]\n","Step 1457    [0.666 sec/step, loss=0.82591, avg_loss=0.79013]\n","Step 1458    [0.666 sec/step, loss=0.82160, avg_loss=0.78986]\n","Step 1459    [0.666 sec/step, loss=0.78030, avg_loss=0.78985]\n","Step 1460    [0.666 sec/step, loss=0.79395, avg_loss=0.78978]\n","Step 1461    [0.666 sec/step, loss=0.75370, avg_loss=0.78893]\n","Step 1462    [0.668 sec/step, loss=0.84419, avg_loss=0.78893]\n","Step 1463    [0.675 sec/step, loss=0.77428, avg_loss=0.78844]\n","Generated 16 batches of size 16 in 2.148 sec\n","Step 1464    [0.674 sec/step, loss=0.78000, avg_loss=0.78787]\n","Step 1465    [0.670 sec/step, loss=0.81192, avg_loss=0.78829]\n","Step 1466    [0.664 sec/step, loss=0.75474, avg_loss=0.78805]\n","Step 1467    [0.661 sec/step, loss=0.82840, avg_loss=0.78877]\n","Step 1468    [0.656 sec/step, loss=0.77218, avg_loss=0.78852]\n","Step 1469    [0.657 sec/step, loss=0.80279, avg_loss=0.78853]\n","Step 1470    [0.660 sec/step, loss=0.76116, avg_loss=0.78834]\n","Step 1471    [0.660 sec/step, loss=0.75465, avg_loss=0.78734]\n","Step 1472    [0.661 sec/step, loss=0.79961, avg_loss=0.78756]\n","Step 1473    [0.661 sec/step, loss=0.82571, avg_loss=0.78808]\n","Step 1474    [0.661 sec/step, loss=0.81050, avg_loss=0.78822]\n","Step 1475    [0.661 sec/step, loss=0.77997, avg_loss=0.78813]\n","Step 1476    [0.662 sec/step, loss=0.81973, avg_loss=0.78868]\n","Step 1477    [0.659 sec/step, loss=0.75373, avg_loss=0.78832]\n","Step 1478    [0.666 sec/step, loss=0.78092, avg_loss=0.78781]\n","Generated 16 batches of size 16 in 2.183 sec\n","Step 1479    [0.670 sec/step, loss=0.78867, avg_loss=0.78808]\n","Step 1480    [0.670 sec/step, loss=0.75536, avg_loss=0.78788]\n","Step 1481    [0.670 sec/step, loss=0.80159, avg_loss=0.78785]\n","Step 1482    [0.668 sec/step, loss=0.81968, avg_loss=0.78775]\n","Step 1483    [0.657 sec/step, loss=0.76073, avg_loss=0.78770]\n","Step 1484    [0.657 sec/step, loss=0.76124, avg_loss=0.78759]\n","Step 1485    [0.657 sec/step, loss=0.72838, avg_loss=0.78724]\n","Step 1486    [0.660 sec/step, loss=0.75135, avg_loss=0.78646]\n","Step 1487    [0.662 sec/step, loss=0.79973, avg_loss=0.78700]\n","Step 1488    [0.661 sec/step, loss=0.75208, avg_loss=0.78679]\n","Step 1489    [0.662 sec/step, loss=0.80999, avg_loss=0.78649]\n","Step 1490    [0.661 sec/step, loss=0.76136, avg_loss=0.78564]\n","Step 1491    [0.664 sec/step, loss=0.76043, avg_loss=0.78546]\n","Step 1492    [0.662 sec/step, loss=0.83679, avg_loss=0.78592]\n","Step 1493    [0.661 sec/step, loss=0.73243, avg_loss=0.78567]\n","Step 1494    [0.665 sec/step, loss=0.79665, avg_loss=0.78563]\n","Generated 16 batches of size 16 in 2.016 sec\n","Step 1495    [0.671 sec/step, loss=0.75819, avg_loss=0.78549]\n","Step 1496    [0.673 sec/step, loss=0.81972, avg_loss=0.78585]\n","Step 1497    [0.675 sec/step, loss=0.78023, avg_loss=0.78610]\n","Step 1498    [0.666 sec/step, loss=0.75664, avg_loss=0.78533]\n","Step 1499    [0.665 sec/step, loss=0.78767, avg_loss=0.78558]\n","Step 1500    [0.665 sec/step, loss=0.79846, avg_loss=0.78530]\n","Saving audio and alignment...\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12625 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50864 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47532 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44032 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45208 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50500 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50556 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54624 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48169 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54693 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47568 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51080 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12625 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50864 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47532 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44032 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45208 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50500 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50556 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54624 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48169 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54693 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47568 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51080 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/train-step-000001500-align000.png\n","100% 1/1 [00:02<00:00,  2.15s/it]\n","Test finished for step 1500.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/test-step-000001500-align000.png\n"," 50% 1/2 [00:02<00:02,  2.20s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/test-step-000001500-align001.png\n","100% 2/2 [00:04<00:00,  2.17s/it]\n","Test finished for step 1500.\n","Step 1501    [0.661 sec/step, loss=0.76657, avg_loss=0.78522]\n","Step 1502    [0.660 sec/step, loss=0.81782, avg_loss=0.78520]\n","Step 1503    [0.660 sec/step, loss=0.74205, avg_loss=0.78508]\n","Step 1504    [0.662 sec/step, loss=0.77497, avg_loss=0.78435]\n","Step 1505    [0.664 sec/step, loss=0.79191, avg_loss=0.78422]\n","Step 1506    [0.664 sec/step, loss=0.75193, avg_loss=0.78407]\n","Step 1507    [0.664 sec/step, loss=0.80755, avg_loss=0.78454]\n","Step 1508    [0.669 sec/step, loss=0.75960, avg_loss=0.78454]\n","Step 1509    [0.675 sec/step, loss=0.77842, avg_loss=0.78436]\n","Generated 16 batches of size 16 in 2.166 sec\n","Step 1510    [0.682 sec/step, loss=0.81203, avg_loss=0.78515]\n","Step 1511    [0.682 sec/step, loss=0.74856, avg_loss=0.78486]\n","Step 1512    [0.681 sec/step, loss=0.75921, avg_loss=0.78471]\n","Step 1513    [0.679 sec/step, loss=0.76789, avg_loss=0.78443]\n","Step 1514    [0.670 sec/step, loss=0.79703, avg_loss=0.78438]\n","Step 1515    [0.666 sec/step, loss=0.73202, avg_loss=0.78340]\n","Step 1516    [0.662 sec/step, loss=0.75635, avg_loss=0.78333]\n","Step 1517    [0.663 sec/step, loss=0.78352, avg_loss=0.78360]\n","Step 1518    [0.662 sec/step, loss=0.78645, avg_loss=0.78321]\n","Step 1519    [0.661 sec/step, loss=0.75437, avg_loss=0.78255]\n","Step 1520    [0.663 sec/step, loss=0.81028, avg_loss=0.78252]\n","Step 1521    [0.664 sec/step, loss=0.76620, avg_loss=0.78255]\n","Step 1522    [0.663 sec/step, loss=0.74309, avg_loss=0.78183]\n","Step 1523    [0.664 sec/step, loss=0.80865, avg_loss=0.78254]\n","Step 1524    [0.665 sec/step, loss=0.80650, avg_loss=0.78231]\n","Step 1525    [0.669 sec/step, loss=0.75675, avg_loss=0.78166]\n","Step 1526    [0.672 sec/step, loss=0.78408, avg_loss=0.78195]\n","Generated 16 batches of size 16 in 2.336 sec\n","Step 1527    [0.673 sec/step, loss=0.73064, avg_loss=0.78159]\n","Step 1528    [0.673 sec/step, loss=0.74511, avg_loss=0.78116]\n","Step 1529    [0.673 sec/step, loss=0.77816, avg_loss=0.78112]\n","Step 1530    [0.660 sec/step, loss=0.79579, avg_loss=0.78137]\n","Step 1531    [0.659 sec/step, loss=0.77020, avg_loss=0.78134]\n","Step 1532    [0.659 sec/step, loss=0.82777, avg_loss=0.78151]\n","Step 1533    [0.662 sec/step, loss=0.76367, avg_loss=0.78143]\n","Step 1534    [0.662 sec/step, loss=0.75586, avg_loss=0.78086]\n","Step 1535    [0.660 sec/step, loss=0.76167, avg_loss=0.78065]\n","Step 1536    [0.660 sec/step, loss=0.73924, avg_loss=0.78050]\n","Step 1537    [0.660 sec/step, loss=0.73763, avg_loss=0.78024]\n","Step 1538    [0.661 sec/step, loss=0.80911, avg_loss=0.78023]\n","Step 1539    [0.661 sec/step, loss=0.80559, avg_loss=0.78076]\n","Step 1540    [0.662 sec/step, loss=0.76927, avg_loss=0.78088]\n","Step 1541    [0.667 sec/step, loss=0.78080, avg_loss=0.78055]\n","Generated 16 batches of size 16 in 2.088 sec\n","Step 1542    [0.673 sec/step, loss=0.78564, avg_loss=0.78077]\n","Step 1543    [0.673 sec/step, loss=0.75458, avg_loss=0.78049]\n","Step 1544    [0.673 sec/step, loss=0.77902, avg_loss=0.78049]\n","Step 1545    [0.674 sec/step, loss=0.83088, avg_loss=0.78114]\n","Step 1546    [0.670 sec/step, loss=0.81968, avg_loss=0.78174]\n","Step 1547    [0.667 sec/step, loss=0.76568, avg_loss=0.78156]\n","Step 1548    [0.668 sec/step, loss=0.78171, avg_loss=0.78116]\n","Step 1549    [0.666 sec/step, loss=0.79441, avg_loss=0.78090]\n","Step 1550    [0.661 sec/step, loss=0.78835, avg_loss=0.78104]\n","Step 1551    [0.664 sec/step, loss=0.77956, avg_loss=0.78145]\n","Step 1552    [0.664 sec/step, loss=0.81506, avg_loss=0.78168]\n","Step 1553    [0.663 sec/step, loss=0.75640, avg_loss=0.78116]\n","Step 1554    [0.662 sec/step, loss=0.74592, avg_loss=0.78043]\n","Step 1555    [0.662 sec/step, loss=0.78297, avg_loss=0.78042]\n","Step 1556    [0.659 sec/step, loss=0.75195, avg_loss=0.78013]\n","Step 1557    [0.663 sec/step, loss=0.76926, avg_loss=0.77957]\n","Generated 16 batches of size 16 in 2.108 sec\n","Step 1558    [0.668 sec/step, loss=0.82125, avg_loss=0.77956]\n","Step 1559    [0.672 sec/step, loss=0.75456, avg_loss=0.77931]\n","Step 1560    [0.673 sec/step, loss=0.79682, avg_loss=0.77934]\n","Step 1561    [0.673 sec/step, loss=0.77884, avg_loss=0.77959]\n","Step 1562    [0.670 sec/step, loss=0.82190, avg_loss=0.77936]\n","Step 1563    [0.666 sec/step, loss=0.77991, avg_loss=0.77942]\n","Step 1564    [0.666 sec/step, loss=0.77439, avg_loss=0.77936]\n","Step 1565    [0.666 sec/step, loss=0.73546, avg_loss=0.77860]\n","Step 1566    [0.667 sec/step, loss=0.74515, avg_loss=0.77850]\n","Step 1567    [0.666 sec/step, loss=0.81503, avg_loss=0.77837]\n","Step 1568    [0.667 sec/step, loss=0.80800, avg_loss=0.77873]\n","Step 1569    [0.668 sec/step, loss=0.76576, avg_loss=0.77836]\n","Step 1570    [0.664 sec/step, loss=0.78999, avg_loss=0.77865]\n","Step 1571    [0.665 sec/step, loss=0.77795, avg_loss=0.77888]\n","Step 1572    [0.665 sec/step, loss=0.75402, avg_loss=0.77842]\n","Step 1573    [0.672 sec/step, loss=0.75626, avg_loss=0.77773]\n","Generated 16 batches of size 16 in 2.160 sec\n","Step 1574    [0.678 sec/step, loss=0.76060, avg_loss=0.77723]\n","Step 1575    [0.675 sec/step, loss=0.74680, avg_loss=0.77690]\n","Step 1576    [0.675 sec/step, loss=0.74450, avg_loss=0.77615]\n","Step 1577    [0.679 sec/step, loss=0.76344, avg_loss=0.77624]\n","Step 1578    [0.670 sec/step, loss=0.80484, avg_loss=0.77648]\n","Step 1579    [0.668 sec/step, loss=0.80571, avg_loss=0.77665]\n","Step 1580    [0.667 sec/step, loss=0.73840, avg_loss=0.77648]\n","Step 1581    [0.667 sec/step, loss=0.75875, avg_loss=0.77605]\n","Step 1582    [0.666 sec/step, loss=0.78351, avg_loss=0.77569]\n","Step 1583    [0.667 sec/step, loss=0.76686, avg_loss=0.77575]\n","Step 1584    [0.667 sec/step, loss=0.74650, avg_loss=0.77561]\n","Step 1585    [0.668 sec/step, loss=0.80872, avg_loss=0.77641]\n","Step 1586    [0.664 sec/step, loss=0.74972, avg_loss=0.77639]\n","Step 1587    [0.664 sec/step, loss=0.79694, avg_loss=0.77637]\n","Step 1588    [0.664 sec/step, loss=0.74874, avg_loss=0.77633]\n","Step 1589    [0.668 sec/step, loss=0.73005, avg_loss=0.77553]\n","Step 1590    [0.672 sec/step, loss=0.71942, avg_loss=0.77511]\n","Generated 16 batches of size 16 in 2.106 sec\n","Step 1591    [0.673 sec/step, loss=0.75744, avg_loss=0.77508]\n","Step 1592    [0.674 sec/step, loss=0.79862, avg_loss=0.77470]\n","Step 1593    [0.676 sec/step, loss=0.77242, avg_loss=0.77510]\n","Step 1594    [0.671 sec/step, loss=0.78211, avg_loss=0.77496]\n","Step 1595    [0.666 sec/step, loss=0.79366, avg_loss=0.77531]\n","Step 1596    [0.665 sec/step, loss=0.74127, avg_loss=0.77453]\n","Step 1597    [0.666 sec/step, loss=0.76735, avg_loss=0.77440]\n","Step 1598    [0.666 sec/step, loss=0.78396, avg_loss=0.77467]\n","Step 1599    [0.664 sec/step, loss=0.80160, avg_loss=0.77481]\n","Step 1600    [0.663 sec/step, loss=0.73271, avg_loss=0.77415]\n","Step 1601    [0.664 sec/step, loss=0.75064, avg_loss=0.77399]\n","Step 1602    [0.668 sec/step, loss=0.74054, avg_loss=0.77322]\n","Step 1603    [0.671 sec/step, loss=0.75166, avg_loss=0.77332]\n","Step 1604    [0.669 sec/step, loss=0.76344, avg_loss=0.77320]\n","Step 1605    [0.675 sec/step, loss=0.78038, avg_loss=0.77309]\n","Step 1606    [0.679 sec/step, loss=0.74432, avg_loss=0.77301]\n","Generated 16 batches of size 16 in 2.152 sec\n","Step 1607    [0.679 sec/step, loss=0.74052, avg_loss=0.77234]\n","Step 1608    [0.676 sec/step, loss=0.79106, avg_loss=0.77266]\n","Step 1609    [0.669 sec/step, loss=0.79274, avg_loss=0.77280]\n","Step 1610    [0.663 sec/step, loss=0.76718, avg_loss=0.77235]\n","Step 1611    [0.663 sec/step, loss=0.73242, avg_loss=0.77219]\n","Step 1612    [0.664 sec/step, loss=0.75592, avg_loss=0.77216]\n","Step 1613    [0.663 sec/step, loss=0.71295, avg_loss=0.77161]\n","Step 1614    [0.664 sec/step, loss=0.78848, avg_loss=0.77152]\n","Step 1615    [0.664 sec/step, loss=0.77176, avg_loss=0.77192]\n","Step 1616    [0.663 sec/step, loss=0.72834, avg_loss=0.77164]\n","Step 1617    [0.667 sec/step, loss=0.74279, avg_loss=0.77123]\n","Step 1618    [0.667 sec/step, loss=0.73669, avg_loss=0.77073]\n","Step 1619    [0.668 sec/step, loss=0.80067, avg_loss=0.77120]\n","Step 1620    [0.667 sec/step, loss=0.74338, avg_loss=0.77053]\n","Step 1621    [0.669 sec/step, loss=0.72885, avg_loss=0.77015]\n","Step 1622    [0.675 sec/step, loss=0.74470, avg_loss=0.77017]\n","Generated 16 batches of size 16 in 2.209 sec\n","Step 1623    [0.675 sec/step, loss=0.74598, avg_loss=0.76954]\n","Step 1624    [0.675 sec/step, loss=0.79525, avg_loss=0.76943]\n","Step 1625    [0.671 sec/step, loss=0.76014, avg_loss=0.76946]\n","Step 1626    [0.668 sec/step, loss=0.80588, avg_loss=0.76968]\n","Step 1627    [0.669 sec/step, loss=0.75694, avg_loss=0.76995]\n","Step 1628    [0.670 sec/step, loss=0.76149, avg_loss=0.77011]\n","Step 1629    [0.668 sec/step, loss=0.75254, avg_loss=0.76985]\n","Step 1630    [0.668 sec/step, loss=0.79276, avg_loss=0.76982]\n","Step 1631    [0.668 sec/step, loss=0.75916, avg_loss=0.76971]\n","Step 1632    [0.668 sec/step, loss=0.72935, avg_loss=0.76873]\n","Step 1633    [0.663 sec/step, loss=0.76640, avg_loss=0.76876]\n","Step 1634    [0.663 sec/step, loss=0.73413, avg_loss=0.76854]\n","Step 1635    [0.664 sec/step, loss=0.74680, avg_loss=0.76839]\n","Step 1636    [0.665 sec/step, loss=0.75281, avg_loss=0.76852]\n","Step 1637    [0.674 sec/step, loss=0.75463, avg_loss=0.76869]\n","Generated 16 batches of size 16 in 2.216 sec\n","Step 1638    [0.678 sec/step, loss=0.72360, avg_loss=0.76784]\n","Step 1639    [0.678 sec/step, loss=0.78232, avg_loss=0.76761]\n","Step 1640    [0.678 sec/step, loss=0.74170, avg_loss=0.76733]\n","Step 1641    [0.672 sec/step, loss=0.71629, avg_loss=0.76669]\n","Step 1642    [0.670 sec/step, loss=0.74097, avg_loss=0.76624]\n","Step 1643    [0.671 sec/step, loss=0.78261, avg_loss=0.76652]\n","Step 1644    [0.669 sec/step, loss=0.77158, avg_loss=0.76645]\n","Step 1645    [0.668 sec/step, loss=0.73727, avg_loss=0.76551]\n","Step 1646    [0.668 sec/step, loss=0.73836, avg_loss=0.76470]\n","Step 1647    [0.668 sec/step, loss=0.73535, avg_loss=0.76439]\n","Step 1648    [0.666 sec/step, loss=0.76390, avg_loss=0.76421]\n","Step 1649    [0.667 sec/step, loss=0.75280, avg_loss=0.76380]\n","Step 1650    [0.668 sec/step, loss=0.73899, avg_loss=0.76330]\n","Step 1651    [0.667 sec/step, loss=0.74220, avg_loss=0.76293]\n","Step 1652    [0.666 sec/step, loss=0.71988, avg_loss=0.76198]\n","Step 1653    [0.671 sec/step, loss=0.72000, avg_loss=0.76162]\n","Step 1654    [0.676 sec/step, loss=0.77628, avg_loss=0.76192]\n","Generated 16 batches of size 16 in 2.075 sec\n","Step 1655    [0.677 sec/step, loss=0.78648, avg_loss=0.76195]\n","Step 1656    [0.679 sec/step, loss=0.74332, avg_loss=0.76187]\n","Step 1657    [0.674 sec/step, loss=0.74343, avg_loss=0.76161]\n","Step 1658    [0.669 sec/step, loss=0.72811, avg_loss=0.76068]\n","Step 1659    [0.664 sec/step, loss=0.73171, avg_loss=0.76045]\n","Step 1660    [0.665 sec/step, loss=0.79748, avg_loss=0.76046]\n","Step 1661    [0.665 sec/step, loss=0.77399, avg_loss=0.76041]\n","Step 1662    [0.668 sec/step, loss=0.74235, avg_loss=0.75961]\n","Step 1663    [0.666 sec/step, loss=0.73693, avg_loss=0.75918]\n","Step 1664    [0.666 sec/step, loss=0.69839, avg_loss=0.75842]\n","Step 1665    [0.667 sec/step, loss=0.76794, avg_loss=0.75875]\n","Step 1666    [0.668 sec/step, loss=0.78154, avg_loss=0.75911]\n","Step 1667    [0.669 sec/step, loss=0.75030, avg_loss=0.75846]\n","Step 1668    [0.669 sec/step, loss=0.78057, avg_loss=0.75819]\n","Step 1669    [0.671 sec/step, loss=0.72492, avg_loss=0.75778]\n","Generated 16 batches of size 16 in 2.058 sec\n","Step 1670    [0.676 sec/step, loss=0.73433, avg_loss=0.75722]\n","Step 1671    [0.678 sec/step, loss=0.73914, avg_loss=0.75684]\n","Step 1672    [0.677 sec/step, loss=0.78261, avg_loss=0.75712]\n","Step 1673    [0.670 sec/step, loss=0.73412, avg_loss=0.75690]\n","Step 1674    [0.665 sec/step, loss=0.72126, avg_loss=0.75651]\n","Step 1675    [0.665 sec/step, loss=0.73055, avg_loss=0.75635]\n","Step 1676    [0.665 sec/step, loss=0.76611, avg_loss=0.75656]\n","Step 1677    [0.662 sec/step, loss=0.73633, avg_loss=0.75629]\n","Step 1678    [0.663 sec/step, loss=0.72731, avg_loss=0.75551]\n","Step 1679    [0.661 sec/step, loss=0.70752, avg_loss=0.75453]\n","Step 1680    [0.663 sec/step, loss=0.72835, avg_loss=0.75443]\n","Step 1681    [0.663 sec/step, loss=0.72298, avg_loss=0.75407]\n","Step 1682    [0.662 sec/step, loss=0.76996, avg_loss=0.75394]\n","Step 1683    [0.662 sec/step, loss=0.77954, avg_loss=0.75407]\n","Step 1684    [0.662 sec/step, loss=0.74008, avg_loss=0.75400]\n","Step 1685    [0.675 sec/step, loss=0.73733, avg_loss=0.75329]\n","Generated 16 batches of size 16 in 2.146 sec\n","Step 1686    [0.676 sec/step, loss=0.71919, avg_loss=0.75298]\n","Step 1687    [0.676 sec/step, loss=0.74063, avg_loss=0.75242]\n","Step 1688    [0.676 sec/step, loss=0.71982, avg_loss=0.75213]\n","Step 1689    [0.673 sec/step, loss=0.77676, avg_loss=0.75260]\n","Step 1690    [0.670 sec/step, loss=0.75508, avg_loss=0.75295]\n","Step 1691    [0.666 sec/step, loss=0.73751, avg_loss=0.75275]\n","Step 1692    [0.664 sec/step, loss=0.75704, avg_loss=0.75234]\n","Step 1693    [0.663 sec/step, loss=0.71889, avg_loss=0.75180]\n","Step 1694    [0.663 sec/step, loss=0.71943, avg_loss=0.75118]\n","Step 1695    [0.663 sec/step, loss=0.73761, avg_loss=0.75062]\n","Step 1696    [0.668 sec/step, loss=0.72126, avg_loss=0.75042]\n","Step 1697    [0.665 sec/step, loss=0.70729, avg_loss=0.74982]\n","Step 1698    [0.667 sec/step, loss=0.73784, avg_loss=0.74935]\n","Step 1699    [0.667 sec/step, loss=0.76666, avg_loss=0.74900]\n","Step 1700    [0.669 sec/step, loss=0.77617, avg_loss=0.74944]\n","Step 1701    [0.672 sec/step, loss=0.75291, avg_loss=0.74946]\n","Step 1702    [0.672 sec/step, loss=0.77997, avg_loss=0.74986]\n","Generated 16 batches of size 16 in 2.176 sec\n","Step 1703    [0.671 sec/step, loss=0.72221, avg_loss=0.74956]\n","Step 1704    [0.670 sec/step, loss=0.70147, avg_loss=0.74894]\n","Step 1705    [0.664 sec/step, loss=0.72111, avg_loss=0.74835]\n","Step 1706    [0.660 sec/step, loss=0.72211, avg_loss=0.74813]\n","Step 1707    [0.660 sec/step, loss=0.75945, avg_loss=0.74832]\n","Step 1708    [0.659 sec/step, loss=0.74358, avg_loss=0.74784]\n","Step 1709    [0.662 sec/step, loss=0.74408, avg_loss=0.74736]\n","Step 1710    [0.661 sec/step, loss=0.72871, avg_loss=0.74697]\n","Step 1711    [0.666 sec/step, loss=0.73450, avg_loss=0.74699]\n","Step 1712    [0.666 sec/step, loss=0.72685, avg_loss=0.74670]\n","Step 1713    [0.667 sec/step, loss=0.77434, avg_loss=0.74731]\n","Step 1714    [0.667 sec/step, loss=0.76708, avg_loss=0.74710]\n","Step 1715    [0.670 sec/step, loss=0.74003, avg_loss=0.74678]\n","Step 1716    [0.670 sec/step, loss=0.74992, avg_loss=0.74700]\n","Step 1717    [0.670 sec/step, loss=0.70586, avg_loss=0.74663]\n","Step 1718    [0.675 sec/step, loss=0.72194, avg_loss=0.74648]\n","Generated 16 batches of size 16 in 2.135 sec\n","Step 1719    [0.675 sec/step, loss=0.77242, avg_loss=0.74620]\n","Step 1720    [0.676 sec/step, loss=0.73562, avg_loss=0.74612]\n","Step 1721    [0.673 sec/step, loss=0.75058, avg_loss=0.74634]\n","Step 1722    [0.667 sec/step, loss=0.72067, avg_loss=0.74610]\n","Step 1723    [0.667 sec/step, loss=0.70822, avg_loss=0.74572]\n","Step 1724    [0.667 sec/step, loss=0.75485, avg_loss=0.74532]\n","Step 1725    [0.666 sec/step, loss=0.70955, avg_loss=0.74481]\n","Step 1726    [0.668 sec/step, loss=0.73777, avg_loss=0.74413]\n","Step 1727    [0.670 sec/step, loss=0.72738, avg_loss=0.74383]\n","Step 1728    [0.669 sec/step, loss=0.71629, avg_loss=0.74338]\n","Step 1729    [0.669 sec/step, loss=0.77392, avg_loss=0.74360]\n","Step 1730    [0.671 sec/step, loss=0.76731, avg_loss=0.74334]\n","Step 1731    [0.670 sec/step, loss=0.71829, avg_loss=0.74293]\n","Step 1732    [0.670 sec/step, loss=0.75469, avg_loss=0.74319]\n","Step 1733    [0.678 sec/step, loss=0.72555, avg_loss=0.74278]\n","Generated 16 batches of size 16 in 2.069 sec\n","Step 1734    [0.682 sec/step, loss=0.70332, avg_loss=0.74247]\n","Step 1735    [0.682 sec/step, loss=0.73597, avg_loss=0.74236]\n","Step 1736    [0.680 sec/step, loss=0.69755, avg_loss=0.74181]\n","Step 1737    [0.672 sec/step, loss=0.78198, avg_loss=0.74208]\n","Step 1738    [0.668 sec/step, loss=0.73614, avg_loss=0.74221]\n","Step 1739    [0.667 sec/step, loss=0.71232, avg_loss=0.74151]\n","Step 1740    [0.667 sec/step, loss=0.73435, avg_loss=0.74144]\n","Step 1741    [0.668 sec/step, loss=0.76308, avg_loss=0.74190]\n","Step 1742    [0.663 sec/step, loss=0.71739, avg_loss=0.74167]\n","Step 1743    [0.664 sec/step, loss=0.76519, avg_loss=0.74149]\n","Step 1744    [0.664 sec/step, loss=0.77202, avg_loss=0.74150]\n","Step 1745    [0.665 sec/step, loss=0.73101, avg_loss=0.74143]\n","Step 1746    [0.669 sec/step, loss=0.73324, avg_loss=0.74138]\n","Step 1747    [0.666 sec/step, loss=0.78594, avg_loss=0.74189]\n","Step 1748    [0.668 sec/step, loss=0.73340, avg_loss=0.74158]\n","Step 1749    [0.672 sec/step, loss=0.72227, avg_loss=0.74128]\n","Step 1750    [0.677 sec/step, loss=0.74023, avg_loss=0.74129]\n","Generated 16 batches of size 16 in 2.204 sec\n","Step 1751    [0.677 sec/step, loss=0.75231, avg_loss=0.74139]\n","Step 1752    [0.678 sec/step, loss=0.77794, avg_loss=0.74197]\n","Step 1753    [0.674 sec/step, loss=0.73892, avg_loss=0.74216]\n","Step 1754    [0.668 sec/step, loss=0.71745, avg_loss=0.74157]\n","Step 1755    [0.667 sec/step, loss=0.70997, avg_loss=0.74081]\n","Step 1756    [0.665 sec/step, loss=0.75862, avg_loss=0.74096]\n","Step 1757    [0.664 sec/step, loss=0.72800, avg_loss=0.74081]\n","Step 1758    [0.669 sec/step, loss=0.74780, avg_loss=0.74100]\n","Step 1759    [0.668 sec/step, loss=0.69898, avg_loss=0.74068]\n","Step 1760    [0.667 sec/step, loss=0.76609, avg_loss=0.74036]\n","Step 1761    [0.668 sec/step, loss=0.74320, avg_loss=0.74006]\n","Step 1762    [0.665 sec/step, loss=0.76996, avg_loss=0.74033]\n","Step 1763    [0.666 sec/step, loss=0.73056, avg_loss=0.74027]\n","Step 1764    [0.666 sec/step, loss=0.70993, avg_loss=0.74038]\n","Step 1765    [0.669 sec/step, loss=0.72601, avg_loss=0.73996]\n","Step 1766    [0.672 sec/step, loss=0.72108, avg_loss=0.73936]\n","Generated 16 batches of size 16 in 2.135 sec\n","Step 1767    [0.673 sec/step, loss=0.75023, avg_loss=0.73936]\n","Step 1768    [0.674 sec/step, loss=0.75075, avg_loss=0.73906]\n","Step 1769    [0.669 sec/step, loss=0.72155, avg_loss=0.73903]\n","Step 1770    [0.664 sec/step, loss=0.76414, avg_loss=0.73932]\n","Step 1771    [0.663 sec/step, loss=0.75633, avg_loss=0.73950]\n","Step 1772    [0.663 sec/step, loss=0.77893, avg_loss=0.73946]\n","Step 1773    [0.664 sec/step, loss=0.73099, avg_loss=0.73943]\n","Step 1774    [0.665 sec/step, loss=0.77079, avg_loss=0.73992]\n","Step 1775    [0.670 sec/step, loss=0.73898, avg_loss=0.74001]\n","Step 1776    [0.671 sec/step, loss=0.72314, avg_loss=0.73958]\n","Step 1777    [0.670 sec/step, loss=0.75207, avg_loss=0.73974]\n","Step 1778    [0.670 sec/step, loss=0.73491, avg_loss=0.73981]\n","Step 1779    [0.671 sec/step, loss=0.76747, avg_loss=0.74041]\n","Step 1780    [0.671 sec/step, loss=0.74496, avg_loss=0.74058]\n","Step 1781    [0.679 sec/step, loss=0.73180, avg_loss=0.74067]\n","Generated 16 batches of size 16 in 2.122 sec\n","Step 1782    [0.683 sec/step, loss=0.73869, avg_loss=0.74035]\n","Step 1783    [0.682 sec/step, loss=0.75434, avg_loss=0.74010]\n","Step 1784    [0.682 sec/step, loss=0.75648, avg_loss=0.74026]\n","Step 1785    [0.669 sec/step, loss=0.69530, avg_loss=0.73984]\n","Step 1786    [0.668 sec/step, loss=0.76587, avg_loss=0.74031]\n","Step 1787    [0.667 sec/step, loss=0.73268, avg_loss=0.74023]\n","Step 1788    [0.667 sec/step, loss=0.71573, avg_loss=0.74019]\n","Step 1789    [0.666 sec/step, loss=0.74959, avg_loss=0.73992]\n","Step 1790    [0.666 sec/step, loss=0.73525, avg_loss=0.73972]\n","Step 1791    [0.666 sec/step, loss=0.73240, avg_loss=0.73967]\n","Step 1792    [0.667 sec/step, loss=0.75195, avg_loss=0.73962]\n","Step 1793    [0.668 sec/step, loss=0.74456, avg_loss=0.73988]\n","Step 1794    [0.669 sec/step, loss=0.71029, avg_loss=0.73978]\n","Step 1795    [0.669 sec/step, loss=0.78095, avg_loss=0.74022]\n","Step 1796    [0.665 sec/step, loss=0.72601, avg_loss=0.74027]\n","Step 1797    [0.670 sec/step, loss=0.71354, avg_loss=0.74033]\n","Generated 16 batches of size 16 in 2.077 sec\n","Step 1798    [0.675 sec/step, loss=0.76926, avg_loss=0.74064]\n","Step 1799    [0.674 sec/step, loss=0.74240, avg_loss=0.74040]\n","Step 1800    [0.673 sec/step, loss=0.72273, avg_loss=0.73987]\n","Step 1801    [0.668 sec/step, loss=0.79843, avg_loss=0.74032]\n","Step 1802    [0.664 sec/step, loss=0.77239, avg_loss=0.74024]\n","Step 1803    [0.667 sec/step, loss=0.74641, avg_loss=0.74049]\n","Step 1804    [0.667 sec/step, loss=0.75743, avg_loss=0.74105]\n","Step 1805    [0.666 sec/step, loss=0.74522, avg_loss=0.74129]\n","Step 1806    [0.667 sec/step, loss=0.76061, avg_loss=0.74167]\n","Step 1807    [0.667 sec/step, loss=0.78418, avg_loss=0.74192]\n","Step 1808    [0.666 sec/step, loss=0.74957, avg_loss=0.74198]\n","Step 1809    [0.665 sec/step, loss=0.75583, avg_loss=0.74210]\n","Step 1810    [0.665 sec/step, loss=0.72663, avg_loss=0.74208]\n","Step 1811    [0.661 sec/step, loss=0.71271, avg_loss=0.74186]\n","Step 1812    [0.660 sec/step, loss=0.71927, avg_loss=0.74178]\n","Step 1813    [0.666 sec/step, loss=0.76109, avg_loss=0.74165]\n","Generated 16 batches of size 16 in 2.069 sec\n","Step 1814    [0.669 sec/step, loss=0.69754, avg_loss=0.74095]\n","Step 1815    [0.668 sec/step, loss=0.73888, avg_loss=0.74094]\n","Step 1816    [0.668 sec/step, loss=0.71224, avg_loss=0.74057]\n","Step 1817    [0.666 sec/step, loss=0.76123, avg_loss=0.74112]\n","Step 1818    [0.662 sec/step, loss=0.76913, avg_loss=0.74159]\n","Step 1819    [0.664 sec/step, loss=0.74244, avg_loss=0.74129]\n","Step 1820    [0.664 sec/step, loss=0.72348, avg_loss=0.74117]\n","Step 1821    [0.663 sec/step, loss=0.73606, avg_loss=0.74103]\n","Step 1822    [0.663 sec/step, loss=0.77216, avg_loss=0.74154]\n","Step 1823    [0.662 sec/step, loss=0.69965, avg_loss=0.74145]\n","Step 1824    [0.662 sec/step, loss=0.71612, avg_loss=0.74107]\n","Step 1825    [0.662 sec/step, loss=0.73993, avg_loss=0.74137]\n","Step 1826    [0.660 sec/step, loss=0.76311, avg_loss=0.74162]\n","Step 1827    [0.656 sec/step, loss=0.71964, avg_loss=0.74155]\n","Step 1828    [0.656 sec/step, loss=0.76198, avg_loss=0.74200]\n","Step 1829    [0.661 sec/step, loss=0.72746, avg_loss=0.74154]\n","Generated 16 batches of size 16 in 2.276 sec\n","Step 1830    [0.670 sec/step, loss=0.74034, avg_loss=0.74127]\n","Step 1831    [0.672 sec/step, loss=0.76395, avg_loss=0.74173]\n","Step 1832    [0.671 sec/step, loss=0.73802, avg_loss=0.74156]\n","Step 1833    [0.666 sec/step, loss=0.78276, avg_loss=0.74213]\n","Step 1834    [0.662 sec/step, loss=0.77830, avg_loss=0.74288]\n","Step 1835    [0.661 sec/step, loss=0.72766, avg_loss=0.74280]\n","Step 1836    [0.661 sec/step, loss=0.72193, avg_loss=0.74304]\n","Step 1837    [0.663 sec/step, loss=0.74582, avg_loss=0.74268]\n","Step 1838    [0.662 sec/step, loss=0.67593, avg_loss=0.74208]\n","Step 1839    [0.663 sec/step, loss=0.73788, avg_loss=0.74233]\n","Step 1840    [0.663 sec/step, loss=0.75999, avg_loss=0.74259]\n","Step 1841    [0.663 sec/step, loss=0.70912, avg_loss=0.74205]\n","Step 1842    [0.667 sec/step, loss=0.74290, avg_loss=0.74231]\n","Step 1843    [0.665 sec/step, loss=0.72311, avg_loss=0.74189]\n","Step 1844    [0.667 sec/step, loss=0.74143, avg_loss=0.74158]\n","Step 1845    [0.670 sec/step, loss=0.70804, avg_loss=0.74135]\n","Step 1846    [0.671 sec/step, loss=0.71317, avg_loss=0.74115]\n","Generated 16 batches of size 16 in 2.199 sec\n","Step 1847    [0.671 sec/step, loss=0.72567, avg_loss=0.74055]\n","Step 1848    [0.669 sec/step, loss=0.75414, avg_loss=0.74075]\n","Step 1849    [0.666 sec/step, loss=0.75777, avg_loss=0.74111]\n","Step 1850    [0.661 sec/step, loss=0.74175, avg_loss=0.74112]\n","Step 1851    [0.660 sec/step, loss=0.71422, avg_loss=0.74074]\n","Step 1852    [0.660 sec/step, loss=0.77160, avg_loss=0.74068]\n","Step 1853    [0.661 sec/step, loss=0.71807, avg_loss=0.74047]\n","Step 1854    [0.661 sec/step, loss=0.73780, avg_loss=0.74067]\n","Step 1855    [0.661 sec/step, loss=0.68250, avg_loss=0.74040]\n","Step 1856    [0.662 sec/step, loss=0.73803, avg_loss=0.74019]\n","Step 1857    [0.662 sec/step, loss=0.70224, avg_loss=0.73994]\n","Step 1858    [0.657 sec/step, loss=0.69581, avg_loss=0.73942]\n","Step 1859    [0.659 sec/step, loss=0.71413, avg_loss=0.73957]\n","Step 1860    [0.659 sec/step, loss=0.68851, avg_loss=0.73879]\n","Step 1861    [0.662 sec/step, loss=0.71239, avg_loss=0.73848]\n","Generated 16 batches of size 16 in 2.179 sec\n","Step 1862    [0.669 sec/step, loss=0.72237, avg_loss=0.73801]\n","Step 1863    [0.669 sec/step, loss=0.70761, avg_loss=0.73778]\n","Step 1864    [0.670 sec/step, loss=0.72876, avg_loss=0.73797]\n","Step 1865    [0.666 sec/step, loss=0.71116, avg_loss=0.73782]\n","Step 1866    [0.661 sec/step, loss=0.70041, avg_loss=0.73761]\n","Step 1867    [0.662 sec/step, loss=0.74335, avg_loss=0.73754]\n","Step 1868    [0.664 sec/step, loss=0.72842, avg_loss=0.73732]\n","Step 1869    [0.665 sec/step, loss=0.75445, avg_loss=0.73765]\n","Step 1870    [0.664 sec/step, loss=0.69534, avg_loss=0.73696]\n","Step 1871    [0.668 sec/step, loss=0.71172, avg_loss=0.73651]\n","Step 1872    [0.668 sec/step, loss=0.71422, avg_loss=0.73587]\n","Step 1873    [0.668 sec/step, loss=0.71573, avg_loss=0.73572]\n","Step 1874    [0.669 sec/step, loss=0.71857, avg_loss=0.73519]\n","Step 1875    [0.666 sec/step, loss=0.73900, avg_loss=0.73519]\n","Step 1876    [0.665 sec/step, loss=0.68913, avg_loss=0.73485]\n","Step 1877    [0.671 sec/step, loss=0.71525, avg_loss=0.73448]\n","Generated 16 batches of size 16 in 2.141 sec\n","Step 1878    [0.675 sec/step, loss=0.70669, avg_loss=0.73420]\n","Step 1879    [0.674 sec/step, loss=0.69737, avg_loss=0.73350]\n","Step 1880    [0.672 sec/step, loss=0.68983, avg_loss=0.73295]\n","Step 1881    [0.663 sec/step, loss=0.68629, avg_loss=0.73250]\n","Step 1882    [0.660 sec/step, loss=0.73202, avg_loss=0.73243]\n","Step 1883    [0.660 sec/step, loss=0.71556, avg_loss=0.73204]\n","Step 1884    [0.660 sec/step, loss=0.70603, avg_loss=0.73154]\n","Step 1885    [0.661 sec/step, loss=0.71557, avg_loss=0.73174]\n","Step 1886    [0.661 sec/step, loss=0.71485, avg_loss=0.73123]\n","Step 1887    [0.666 sec/step, loss=0.70219, avg_loss=0.73092]\n","Step 1888    [0.666 sec/step, loss=0.70558, avg_loss=0.73082]\n","Step 1889    [0.665 sec/step, loss=0.69172, avg_loss=0.73024]\n","Step 1890    [0.665 sec/step, loss=0.68995, avg_loss=0.72979]\n","Step 1891    [0.662 sec/step, loss=0.68671, avg_loss=0.72933]\n","Step 1892    [0.661 sec/step, loss=0.69184, avg_loss=0.72873]\n","Step 1893    [0.668 sec/step, loss=0.76345, avg_loss=0.72892]\n","Step 1894    [0.671 sec/step, loss=0.71807, avg_loss=0.72900]\n","Generated 16 batches of size 16 in 2.216 sec\n","Step 1895    [0.670 sec/step, loss=0.68347, avg_loss=0.72802]\n","Step 1896    [0.670 sec/step, loss=0.73452, avg_loss=0.72811]\n","Step 1897    [0.666 sec/step, loss=0.70171, avg_loss=0.72799]\n","Step 1898    [0.660 sec/step, loss=0.71985, avg_loss=0.72750]\n","Step 1899    [0.661 sec/step, loss=0.74261, avg_loss=0.72750]\n","Step 1900    [0.660 sec/step, loss=0.73415, avg_loss=0.72761]\n","Step 1901    [0.663 sec/step, loss=0.71017, avg_loss=0.72673]\n","Step 1902    [0.662 sec/step, loss=0.68579, avg_loss=0.72586]\n","Step 1903    [0.657 sec/step, loss=0.69327, avg_loss=0.72533]\n","Step 1904    [0.658 sec/step, loss=0.73754, avg_loss=0.72513]\n","Step 1905    [0.659 sec/step, loss=0.72241, avg_loss=0.72491]\n","Step 1906    [0.658 sec/step, loss=0.67905, avg_loss=0.72409]\n","Step 1907    [0.658 sec/step, loss=0.70898, avg_loss=0.72334]\n","Step 1908    [0.659 sec/step, loss=0.72419, avg_loss=0.72308]\n","Step 1909    [0.664 sec/step, loss=0.69481, avg_loss=0.72247]\n","Generated 16 batches of size 16 in 2.037 sec\n","Step 1910    [0.668 sec/step, loss=0.70487, avg_loss=0.72226]\n","Step 1911    [0.670 sec/step, loss=0.74065, avg_loss=0.72254]\n","Step 1912    [0.672 sec/step, loss=0.71050, avg_loss=0.72245]\n","Step 1913    [0.668 sec/step, loss=0.70575, avg_loss=0.72190]\n","Step 1914    [0.668 sec/step, loss=0.71352, avg_loss=0.72206]\n","Step 1915    [0.667 sec/step, loss=0.71158, avg_loss=0.72178]\n","Step 1916    [0.667 sec/step, loss=0.73378, avg_loss=0.72200]\n","Step 1917    [0.664 sec/step, loss=0.70295, avg_loss=0.72141]\n","Step 1918    [0.666 sec/step, loss=0.70786, avg_loss=0.72080]\n","Step 1919    [0.662 sec/step, loss=0.71011, avg_loss=0.72048]\n","Step 1920    [0.662 sec/step, loss=0.71408, avg_loss=0.72038]\n","Step 1921    [0.663 sec/step, loss=0.71407, avg_loss=0.72016]\n","Step 1922    [0.664 sec/step, loss=0.70466, avg_loss=0.71949]\n","Step 1923    [0.664 sec/step, loss=0.70276, avg_loss=0.71952]\n","Step 1924    [0.663 sec/step, loss=0.69084, avg_loss=0.71927]\n","Step 1925    [0.667 sec/step, loss=0.70337, avg_loss=0.71890]\n","Generated 16 batches of size 16 in 2.142 sec\n","Step 1926    [0.678 sec/step, loss=0.72064, avg_loss=0.71848]\n","Step 1927    [0.678 sec/step, loss=0.74090, avg_loss=0.71869]\n","Step 1928    [0.678 sec/step, loss=0.73793, avg_loss=0.71845]\n","Step 1929    [0.674 sec/step, loss=0.75806, avg_loss=0.71876]\n","Step 1930    [0.664 sec/step, loss=0.72157, avg_loss=0.71857]\n","Step 1931    [0.662 sec/step, loss=0.68218, avg_loss=0.71775]\n","Step 1932    [0.662 sec/step, loss=0.74462, avg_loss=0.71782]\n","Step 1933    [0.660 sec/step, loss=0.68642, avg_loss=0.71685]\n","Step 1934    [0.660 sec/step, loss=0.69340, avg_loss=0.71600]\n","Step 1935    [0.661 sec/step, loss=0.71914, avg_loss=0.71592]\n","Step 1936    [0.662 sec/step, loss=0.74216, avg_loss=0.71612]\n","Step 1937    [0.660 sec/step, loss=0.69171, avg_loss=0.71558]\n","Step 1938    [0.660 sec/step, loss=0.71318, avg_loss=0.71595]\n","Step 1939    [0.660 sec/step, loss=0.72850, avg_loss=0.71586]\n","Step 1940    [0.659 sec/step, loss=0.67822, avg_loss=0.71504]\n","Step 1941    [0.666 sec/step, loss=0.73207, avg_loss=0.71527]\n","Generated 16 batches of size 16 in 2.026 sec\n","Step 1942    [0.667 sec/step, loss=0.71730, avg_loss=0.71501]\n","Step 1943    [0.667 sec/step, loss=0.68138, avg_loss=0.71460]\n","Step 1944    [0.665 sec/step, loss=0.73287, avg_loss=0.71451]\n","Step 1945    [0.662 sec/step, loss=0.69824, avg_loss=0.71441]\n","Step 1946    [0.658 sec/step, loss=0.71491, avg_loss=0.71443]\n","Step 1947    [0.660 sec/step, loss=0.72158, avg_loss=0.71439]\n","Step 1948    [0.660 sec/step, loss=0.67486, avg_loss=0.71360]\n","Step 1949    [0.661 sec/step, loss=0.71206, avg_loss=0.71314]\n","Step 1950    [0.661 sec/step, loss=0.71171, avg_loss=0.71284]\n","Step 1951    [0.661 sec/step, loss=0.67720, avg_loss=0.71247]\n","Step 1952    [0.661 sec/step, loss=0.70786, avg_loss=0.71183]\n","Step 1953    [0.661 sec/step, loss=0.71201, avg_loss=0.71177]\n","Step 1954    [0.661 sec/step, loss=0.67068, avg_loss=0.71110]\n","Step 1955    [0.662 sec/step, loss=0.69078, avg_loss=0.71118]\n","Step 1956    [0.662 sec/step, loss=0.70873, avg_loss=0.71089]\n","Step 1957    [0.666 sec/step, loss=0.66840, avg_loss=0.71055]\n","Step 1958    [0.672 sec/step, loss=0.72192, avg_loss=0.71081]\n","Generated 16 batches of size 16 in 2.155 sec\n","Step 1959    [0.670 sec/step, loss=0.69710, avg_loss=0.71064]\n","Step 1960    [0.675 sec/step, loss=0.72371, avg_loss=0.71099]\n","Step 1961    [0.671 sec/step, loss=0.70013, avg_loss=0.71087]\n","Step 1962    [0.665 sec/step, loss=0.72997, avg_loss=0.71095]\n","Step 1963    [0.666 sec/step, loss=0.70346, avg_loss=0.71091]\n","Step 1964    [0.665 sec/step, loss=0.70703, avg_loss=0.71069]\n","Step 1965    [0.665 sec/step, loss=0.68781, avg_loss=0.71046]\n","Step 1966    [0.664 sec/step, loss=0.65890, avg_loss=0.71004]\n","Step 1967    [0.664 sec/step, loss=0.68884, avg_loss=0.70950]\n","Step 1968    [0.659 sec/step, loss=0.66546, avg_loss=0.70887]\n","Step 1969    [0.659 sec/step, loss=0.70944, avg_loss=0.70842]\n","Step 1970    [0.659 sec/step, loss=0.68765, avg_loss=0.70834]\n","Step 1971    [0.656 sec/step, loss=0.73455, avg_loss=0.70857]\n","Step 1972    [0.655 sec/step, loss=0.68389, avg_loss=0.70826]\n","Step 1973    [0.657 sec/step, loss=0.70468, avg_loss=0.70815]\n","Generated 16 batches of size 16 in 2.113 sec\n","Step 1974    [0.664 sec/step, loss=0.68299, avg_loss=0.70780]\n","Step 1975    [0.662 sec/step, loss=0.66565, avg_loss=0.70706]\n","Step 1976    [0.663 sec/step, loss=0.70253, avg_loss=0.70720]\n","Step 1977    [0.656 sec/step, loss=0.67067, avg_loss=0.70675]\n","Step 1978    [0.653 sec/step, loss=0.70733, avg_loss=0.70676]\n","Step 1979    [0.655 sec/step, loss=0.68866, avg_loss=0.70667]\n","Step 1980    [0.660 sec/step, loss=0.70095, avg_loss=0.70678]\n","Step 1981    [0.660 sec/step, loss=0.71488, avg_loss=0.70707]\n","Step 1982    [0.659 sec/step, loss=0.65878, avg_loss=0.70634]\n","Step 1983    [0.659 sec/step, loss=0.65834, avg_loss=0.70576]\n","Step 1984    [0.659 sec/step, loss=0.68398, avg_loss=0.70554]\n","Step 1985    [0.657 sec/step, loss=0.69869, avg_loss=0.70537]\n","Step 1986    [0.658 sec/step, loss=0.71666, avg_loss=0.70539]\n","Step 1987    [0.655 sec/step, loss=0.69168, avg_loss=0.70529]\n","Step 1988    [0.655 sec/step, loss=0.70893, avg_loss=0.70532]\n","Step 1989    [0.662 sec/step, loss=0.69695, avg_loss=0.70537]\n","Generated 16 batches of size 16 in 2.120 sec\n","Step 1990    [0.667 sec/step, loss=0.68302, avg_loss=0.70530]\n","Step 1991    [0.671 sec/step, loss=0.69736, avg_loss=0.70541]\n","Step 1992    [0.672 sec/step, loss=0.68591, avg_loss=0.70535]\n","Step 1993    [0.664 sec/step, loss=0.68464, avg_loss=0.70456]\n","Step 1994    [0.660 sec/step, loss=0.65356, avg_loss=0.70392]\n","Step 1995    [0.660 sec/step, loss=0.72131, avg_loss=0.70430]\n","Step 1996    [0.661 sec/step, loss=0.69310, avg_loss=0.70388]\n","Step 1997    [0.662 sec/step, loss=0.70466, avg_loss=0.70391]\n","Step 1998    [0.661 sec/step, loss=0.69156, avg_loss=0.70363]\n","Step 1999    [0.659 sec/step, loss=0.67792, avg_loss=0.70298]\n","Step 2000    [0.660 sec/step, loss=0.70669, avg_loss=0.70271]\n","Saving checkpoint to: logdir-tacotron/moon_2021-02-03_23-08-12/model.ckpt-2000\n","Saving audio and alignment...\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12628 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48148 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47196 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44536 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51656 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47928 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49436 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49352 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51089 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44192 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12628 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48148 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47196 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44536 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51656 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47928 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49436 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49352 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51089 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44192 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/train-step-000002000-align000.png\n","100% 1/1 [00:02<00:00,  2.18s/it]\n","Test finished for step 2000.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/test-step-000002000-align000.png\n"," 50% 1/2 [00:02<00:02,  2.32s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/test-step-000002000-align001.png\n","100% 2/2 [00:04<00:00,  2.25s/it]\n","Test finished for step 2000.\n","Step 2001    [0.658 sec/step, loss=0.67230, avg_loss=0.70233]\n","Step 2002    [0.662 sec/step, loss=0.69281, avg_loss=0.70240]\n","Step 2003    [0.662 sec/step, loss=0.65218, avg_loss=0.70199]\n","Step 2004    [0.667 sec/step, loss=0.72025, avg_loss=0.70182]\n","Generated 16 batches of size 16 in 2.178 sec\n","Step 2005    [0.674 sec/step, loss=0.68208, avg_loss=0.70141]\n","Step 2006    [0.676 sec/step, loss=0.68816, avg_loss=0.70150]\n","Step 2007    [0.676 sec/step, loss=0.68134, avg_loss=0.70123]\n","Step 2008    [0.677 sec/step, loss=0.71443, avg_loss=0.70113]\n","Step 2009    [0.671 sec/step, loss=0.68370, avg_loss=0.70102]\n","Step 2010    [0.667 sec/step, loss=0.67735, avg_loss=0.70074]\n","Step 2011    [0.666 sec/step, loss=0.71281, avg_loss=0.70046]\n","Step 2012    [0.663 sec/step, loss=0.67952, avg_loss=0.70015]\n","Step 2013    [0.662 sec/step, loss=0.69994, avg_loss=0.70010]\n","Step 2014    [0.657 sec/step, loss=0.65277, avg_loss=0.69949]\n","Step 2015    [0.656 sec/step, loss=0.66869, avg_loss=0.69906]\n","Step 2016    [0.658 sec/step, loss=0.73617, avg_loss=0.69908]\n","Step 2017    [0.659 sec/step, loss=0.69360, avg_loss=0.69899]\n","Step 2018    [0.656 sec/step, loss=0.69998, avg_loss=0.69891]\n","Step 2019    [0.657 sec/step, loss=0.70437, avg_loss=0.69885]\n","Step 2020    [0.659 sec/step, loss=0.69075, avg_loss=0.69862]\n","Step 2021    [0.666 sec/step, loss=0.70493, avg_loss=0.69853]\n","Generated 16 batches of size 16 in 2.222 sec\n","Step 2022    [0.668 sec/step, loss=0.69868, avg_loss=0.69847]\n","Step 2023    [0.667 sec/step, loss=0.68062, avg_loss=0.69825]\n","Step 2024    [0.668 sec/step, loss=0.71445, avg_loss=0.69848]\n","Step 2025    [0.665 sec/step, loss=0.69241, avg_loss=0.69837]\n","Step 2026    [0.659 sec/step, loss=0.71068, avg_loss=0.69828]\n","Step 2027    [0.660 sec/step, loss=0.67899, avg_loss=0.69766]\n","Step 2028    [0.659 sec/step, loss=0.66218, avg_loss=0.69690]\n","Step 2029    [0.658 sec/step, loss=0.70136, avg_loss=0.69633]\n","Step 2030    [0.658 sec/step, loss=0.69029, avg_loss=0.69602]\n","Step 2031    [0.659 sec/step, loss=0.65761, avg_loss=0.69577]\n","Step 2032    [0.659 sec/step, loss=0.65650, avg_loss=0.69489]\n","Step 2033    [0.660 sec/step, loss=0.72012, avg_loss=0.69523]\n","Step 2034    [0.661 sec/step, loss=0.69739, avg_loss=0.69527]\n","Step 2035    [0.661 sec/step, loss=0.67312, avg_loss=0.69481]\n","Step 2036    [0.663 sec/step, loss=0.67953, avg_loss=0.69418]\n","Generated 16 batches of size 16 in 2.211 sec\n","Step 2037    [0.671 sec/step, loss=0.67577, avg_loss=0.69402]\n","Step 2038    [0.672 sec/step, loss=0.68136, avg_loss=0.69370]\n","Step 2039    [0.672 sec/step, loss=0.71421, avg_loss=0.69356]\n","Step 2040    [0.672 sec/step, loss=0.68924, avg_loss=0.69367]\n","Step 2041    [0.665 sec/step, loss=0.65172, avg_loss=0.69287]\n","Step 2042    [0.661 sec/step, loss=0.69162, avg_loss=0.69261]\n","Step 2043    [0.665 sec/step, loss=0.69320, avg_loss=0.69273]\n","Step 2044    [0.667 sec/step, loss=0.69612, avg_loss=0.69236]\n","Step 2045    [0.666 sec/step, loss=0.67218, avg_loss=0.69210]\n","Step 2046    [0.668 sec/step, loss=0.68414, avg_loss=0.69179]\n","Step 2047    [0.666 sec/step, loss=0.72767, avg_loss=0.69186]\n","Step 2048    [0.665 sec/step, loss=0.68095, avg_loss=0.69192]\n","Step 2049    [0.668 sec/step, loss=0.68718, avg_loss=0.69167]\n","Step 2050    [0.667 sec/step, loss=0.68226, avg_loss=0.69137]\n","Step 2051    [0.667 sec/step, loss=0.72029, avg_loss=0.69180]\n","Step 2052    [0.670 sec/step, loss=0.64850, avg_loss=0.69121]\n","Step 2053    [0.673 sec/step, loss=0.66970, avg_loss=0.69079]\n","Generated 16 batches of size 16 in 2.307 sec\n","Step 2054    [0.676 sec/step, loss=0.67822, avg_loss=0.69086]\n","Step 2055    [0.676 sec/step, loss=0.69508, avg_loss=0.69091]\n","Step 2056    [0.677 sec/step, loss=0.67668, avg_loss=0.69058]\n","Step 2057    [0.673 sec/step, loss=0.69620, avg_loss=0.69086]\n","Step 2058    [0.668 sec/step, loss=0.71302, avg_loss=0.69077]\n","Step 2059    [0.668 sec/step, loss=0.66480, avg_loss=0.69045]\n","Step 2060    [0.664 sec/step, loss=0.68305, avg_loss=0.69004]\n","Step 2061    [0.662 sec/step, loss=0.68375, avg_loss=0.68988]\n","Step 2062    [0.662 sec/step, loss=0.68556, avg_loss=0.68944]\n","Step 2063    [0.660 sec/step, loss=0.65870, avg_loss=0.68899]\n","Step 2064    [0.661 sec/step, loss=0.69714, avg_loss=0.68889]\n","Step 2065    [0.661 sec/step, loss=0.64409, avg_loss=0.68845]\n","Step 2066    [0.661 sec/step, loss=0.71957, avg_loss=0.68906]\n","Step 2067    [0.661 sec/step, loss=0.66059, avg_loss=0.68878]\n","Step 2068    [0.670 sec/step, loss=0.69573, avg_loss=0.68908]\n","Generated 16 batches of size 16 in 2.142 sec\n","Step 2069    [0.675 sec/step, loss=0.67311, avg_loss=0.68872]\n","Step 2070    [0.674 sec/step, loss=0.67563, avg_loss=0.68860]\n","Step 2071    [0.673 sec/step, loss=0.65994, avg_loss=0.68785]\n","Step 2072    [0.673 sec/step, loss=0.67078, avg_loss=0.68772]\n","Step 2073    [0.670 sec/step, loss=0.68938, avg_loss=0.68757]\n","Step 2074    [0.663 sec/step, loss=0.70847, avg_loss=0.68782]\n","Step 2075    [0.664 sec/step, loss=0.69644, avg_loss=0.68813]\n","Step 2076    [0.667 sec/step, loss=0.68544, avg_loss=0.68796]\n","Step 2077    [0.667 sec/step, loss=0.67510, avg_loss=0.68800]\n","Step 2078    [0.666 sec/step, loss=0.69029, avg_loss=0.68783]\n","Step 2079    [0.664 sec/step, loss=0.65504, avg_loss=0.68750]\n","Step 2080    [0.664 sec/step, loss=0.68189, avg_loss=0.68730]\n","Step 2081    [0.664 sec/step, loss=0.70147, avg_loss=0.68717]\n","Step 2082    [0.665 sec/step, loss=0.67974, avg_loss=0.68738]\n","Step 2083    [0.666 sec/step, loss=0.66399, avg_loss=0.68744]\n","Step 2084    [0.672 sec/step, loss=0.67975, avg_loss=0.68739]\n","Generated 16 batches of size 16 in 2.221 sec\n","Step 2085    [0.680 sec/step, loss=0.67504, avg_loss=0.68716]\n","Step 2086    [0.680 sec/step, loss=0.68540, avg_loss=0.68685]\n","Step 2087    [0.679 sec/step, loss=0.66926, avg_loss=0.68662]\n","Step 2088    [0.678 sec/step, loss=0.65163, avg_loss=0.68605]\n","Step 2089    [0.672 sec/step, loss=0.71082, avg_loss=0.68619]\n","Step 2090    [0.667 sec/step, loss=0.65542, avg_loss=0.68591]\n","Step 2091    [0.664 sec/step, loss=0.68904, avg_loss=0.68583]\n","Step 2092    [0.664 sec/step, loss=0.70971, avg_loss=0.68607]\n","Step 2093    [0.664 sec/step, loss=0.70303, avg_loss=0.68625]\n","Step 2094    [0.666 sec/step, loss=0.71015, avg_loss=0.68682]\n","Step 2095    [0.666 sec/step, loss=0.65010, avg_loss=0.68610]\n","Step 2096    [0.666 sec/step, loss=0.69665, avg_loss=0.68614]\n","Step 2097    [0.663 sec/step, loss=0.67817, avg_loss=0.68587]\n","Step 2098    [0.664 sec/step, loss=0.65640, avg_loss=0.68552]\n","Step 2099    [0.669 sec/step, loss=0.69539, avg_loss=0.68570]\n","Step 2100    [0.674 sec/step, loss=0.69657, avg_loss=0.68560]\n","Generated 16 batches of size 16 in 2.078 sec\n","Step 2101    [0.680 sec/step, loss=0.70234, avg_loss=0.68590]\n","Step 2102    [0.677 sec/step, loss=0.67539, avg_loss=0.68572]\n","Step 2103    [0.677 sec/step, loss=0.69053, avg_loss=0.68611]\n","Step 2104    [0.671 sec/step, loss=0.65242, avg_loss=0.68543]\n","Step 2105    [0.666 sec/step, loss=0.68318, avg_loss=0.68544]\n","Step 2106    [0.664 sec/step, loss=0.67045, avg_loss=0.68526]\n","Step 2107    [0.663 sec/step, loss=0.65309, avg_loss=0.68498]\n","Step 2108    [0.662 sec/step, loss=0.67845, avg_loss=0.68462]\n","Step 2109    [0.666 sec/step, loss=0.67130, avg_loss=0.68450]\n","Step 2110    [0.667 sec/step, loss=0.67952, avg_loss=0.68452]\n","Step 2111    [0.668 sec/step, loss=0.70786, avg_loss=0.68447]\n","Step 2112    [0.669 sec/step, loss=0.69426, avg_loss=0.68461]\n","Step 2113    [0.668 sec/step, loss=0.69413, avg_loss=0.68456]\n","Step 2114    [0.669 sec/step, loss=0.66185, avg_loss=0.68465]\n","Step 2115    [0.668 sec/step, loss=0.65855, avg_loss=0.68455]\n","Step 2116    [0.673 sec/step, loss=0.67327, avg_loss=0.68392]\n","Step 2117    [0.677 sec/step, loss=0.73542, avg_loss=0.68434]\n","Generated 16 batches of size 16 in 2.391 sec\n","Step 2118    [0.679 sec/step, loss=0.66880, avg_loss=0.68402]\n","Step 2119    [0.678 sec/step, loss=0.65065, avg_loss=0.68349]\n","Step 2120    [0.677 sec/step, loss=0.67651, avg_loss=0.68334]\n","Step 2121    [0.671 sec/step, loss=0.67970, avg_loss=0.68309]\n","Step 2122    [0.668 sec/step, loss=0.69353, avg_loss=0.68304]\n","Step 2123    [0.669 sec/step, loss=0.68110, avg_loss=0.68304]\n","Step 2124    [0.668 sec/step, loss=0.66989, avg_loss=0.68260]\n","Step 2125    [0.669 sec/step, loss=0.70200, avg_loss=0.68269]\n","Step 2126    [0.666 sec/step, loss=0.67700, avg_loss=0.68236]\n","Step 2127    [0.664 sec/step, loss=0.65773, avg_loss=0.68215]\n","Step 2128    [0.663 sec/step, loss=0.64765, avg_loss=0.68200]\n","Step 2129    [0.664 sec/step, loss=0.67884, avg_loss=0.68178]\n","Step 2130    [0.664 sec/step, loss=0.70558, avg_loss=0.68193]\n","Step 2131    [0.666 sec/step, loss=0.67992, avg_loss=0.68215]\n","Step 2132    [0.670 sec/step, loss=0.68640, avg_loss=0.68245]\n","Step 2133    [0.673 sec/step, loss=0.67802, avg_loss=0.68203]\n","Generated 16 batches of size 16 in 2.130 sec\n","Step 2134    [0.678 sec/step, loss=0.69095, avg_loss=0.68196]\n","Step 2135    [0.678 sec/step, loss=0.67186, avg_loss=0.68195]\n","Step 2136    [0.675 sec/step, loss=0.66448, avg_loss=0.68180]\n","Step 2137    [0.666 sec/step, loss=0.66614, avg_loss=0.68171]\n","Step 2138    [0.666 sec/step, loss=0.69012, avg_loss=0.68179]\n","Step 2139    [0.666 sec/step, loss=0.71373, avg_loss=0.68179]\n","Step 2140    [0.667 sec/step, loss=0.68337, avg_loss=0.68173]\n","Step 2141    [0.668 sec/step, loss=0.68083, avg_loss=0.68202]\n","Step 2142    [0.667 sec/step, loss=0.67517, avg_loss=0.68186]\n","Step 2143    [0.662 sec/step, loss=0.69479, avg_loss=0.68187]\n","Step 2144    [0.661 sec/step, loss=0.67598, avg_loss=0.68167]\n","Step 2145    [0.661 sec/step, loss=0.68328, avg_loss=0.68178]\n","Step 2146    [0.659 sec/step, loss=0.65173, avg_loss=0.68146]\n","Step 2147    [0.657 sec/step, loss=0.63138, avg_loss=0.68049]\n","Step 2148    [0.662 sec/step, loss=0.66312, avg_loss=0.68032]\n","Step 2149    [0.662 sec/step, loss=0.64109, avg_loss=0.67986]\n","Generated 16 batches of size 16 in 2.296 sec\n","Step 2150    [0.663 sec/step, loss=0.67404, avg_loss=0.67977]\n","Step 2151    [0.663 sec/step, loss=0.65817, avg_loss=0.67915]\n","Step 2152    [0.662 sec/step, loss=0.68271, avg_loss=0.67949]\n","Step 2153    [0.660 sec/step, loss=0.70220, avg_loss=0.67982]\n","Step 2154    [0.658 sec/step, loss=0.68884, avg_loss=0.67993]\n","Step 2155    [0.661 sec/step, loss=0.69395, avg_loss=0.67991]\n","Step 2156    [0.659 sec/step, loss=0.67604, avg_loss=0.67991]\n","Step 2157    [0.659 sec/step, loss=0.67876, avg_loss=0.67973]\n","Step 2158    [0.659 sec/step, loss=0.66332, avg_loss=0.67924]\n","Step 2159    [0.660 sec/step, loss=0.69670, avg_loss=0.67956]\n","Step 2160    [0.664 sec/step, loss=0.67094, avg_loss=0.67943]\n","Step 2161    [0.666 sec/step, loss=0.66116, avg_loss=0.67921]\n","Step 2162    [0.665 sec/step, loss=0.68105, avg_loss=0.67916]\n","Step 2163    [0.665 sec/step, loss=0.65772, avg_loss=0.67915]\n","Step 2164    [0.669 sec/step, loss=0.64199, avg_loss=0.67860]\n","Generated 16 batches of size 16 in 2.074 sec\n","Step 2165    [0.675 sec/step, loss=0.67619, avg_loss=0.67892]\n","Step 2166    [0.675 sec/step, loss=0.69754, avg_loss=0.67870]\n","Step 2167    [0.676 sec/step, loss=0.67173, avg_loss=0.67881]\n","Step 2168    [0.666 sec/step, loss=0.64096, avg_loss=0.67827]\n","Step 2169    [0.662 sec/step, loss=0.64688, avg_loss=0.67800]\n","Step 2170    [0.661 sec/step, loss=0.66456, avg_loss=0.67789]\n","Step 2171    [0.662 sec/step, loss=0.67029, avg_loss=0.67800]\n","Step 2172    [0.665 sec/step, loss=0.67559, avg_loss=0.67804]\n","Step 2173    [0.669 sec/step, loss=0.69808, avg_loss=0.67813]\n","Step 2174    [0.667 sec/step, loss=0.63544, avg_loss=0.67740]\n","Step 2175    [0.667 sec/step, loss=0.70379, avg_loss=0.67747]\n","Step 2176    [0.662 sec/step, loss=0.66910, avg_loss=0.67731]\n","Step 2177    [0.663 sec/step, loss=0.68297, avg_loss=0.67739]\n","Step 2178    [0.664 sec/step, loss=0.66965, avg_loss=0.67718]\n","Step 2179    [0.665 sec/step, loss=0.69823, avg_loss=0.67762]\n","Step 2180    [0.667 sec/step, loss=0.67441, avg_loss=0.67754]\n","Generated 16 batches of size 16 in 2.090 sec\n","Step 2181    [0.672 sec/step, loss=0.68608, avg_loss=0.67739]\n","Step 2182    [0.671 sec/step, loss=0.65211, avg_loss=0.67711]\n","Step 2183    [0.670 sec/step, loss=0.64017, avg_loss=0.67687]\n","Step 2184    [0.665 sec/step, loss=0.67100, avg_loss=0.67678]\n","Step 2185    [0.660 sec/step, loss=0.66914, avg_loss=0.67673]\n","Step 2186    [0.660 sec/step, loss=0.65762, avg_loss=0.67645]\n","Step 2187    [0.660 sec/step, loss=0.65948, avg_loss=0.67635]\n","Step 2188    [0.659 sec/step, loss=0.66392, avg_loss=0.67647]\n","Step 2189    [0.659 sec/step, loss=0.69198, avg_loss=0.67628]\n","Step 2190    [0.659 sec/step, loss=0.66791, avg_loss=0.67641]\n","Step 2191    [0.659 sec/step, loss=0.63828, avg_loss=0.67590]\n","Step 2192    [0.659 sec/step, loss=0.67193, avg_loss=0.67552]\n","Step 2193    [0.659 sec/step, loss=0.64175, avg_loss=0.67491]\n","Step 2194    [0.658 sec/step, loss=0.65858, avg_loss=0.67440]\n","Step 2195    [0.659 sec/step, loss=0.65321, avg_loss=0.67443]\n","Step 2196    [0.665 sec/step, loss=0.69272, avg_loss=0.67439]\n","Generated 16 batches of size 16 in 2.253 sec\n","Step 2197    [0.674 sec/step, loss=0.69282, avg_loss=0.67453]\n","Step 2198    [0.677 sec/step, loss=0.66351, avg_loss=0.67461]\n","Step 2199    [0.672 sec/step, loss=0.66777, avg_loss=0.67433]\n","Step 2200    [0.667 sec/step, loss=0.68476, avg_loss=0.67421]\n","Step 2201    [0.661 sec/step, loss=0.66922, avg_loss=0.67388]\n","Step 2202    [0.660 sec/step, loss=0.69859, avg_loss=0.67411]\n","Step 2203    [0.660 sec/step, loss=0.66906, avg_loss=0.67390]\n","Step 2204    [0.660 sec/step, loss=0.64612, avg_loss=0.67383]\n","Step 2205    [0.658 sec/step, loss=0.65457, avg_loss=0.67355]\n","Step 2206    [0.658 sec/step, loss=0.63750, avg_loss=0.67322]\n","Step 2207    [0.660 sec/step, loss=0.69745, avg_loss=0.67366]\n","Step 2208    [0.659 sec/step, loss=0.67424, avg_loss=0.67362]\n","Step 2209    [0.655 sec/step, loss=0.69079, avg_loss=0.67381]\n","Step 2210    [0.654 sec/step, loss=0.64463, avg_loss=0.67347]\n","Step 2211    [0.653 sec/step, loss=0.66971, avg_loss=0.67308]\n","Step 2212    [0.658 sec/step, loss=0.67831, avg_loss=0.67292]\n","Generated 16 batches of size 16 in 2.210 sec\n","Step 2213    [0.667 sec/step, loss=0.68647, avg_loss=0.67285]\n","Step 2214    [0.667 sec/step, loss=0.67588, avg_loss=0.67299]\n","Step 2215    [0.668 sec/step, loss=0.67284, avg_loss=0.67313]\n","Step 2216    [0.667 sec/step, loss=0.68605, avg_loss=0.67326]\n","Step 2217    [0.662 sec/step, loss=0.64381, avg_loss=0.67234]\n","Step 2218    [0.662 sec/step, loss=0.67022, avg_loss=0.67236]\n","Step 2219    [0.662 sec/step, loss=0.67173, avg_loss=0.67257]\n","Step 2220    [0.659 sec/step, loss=0.67025, avg_loss=0.67251]\n","Step 2221    [0.659 sec/step, loss=0.64875, avg_loss=0.67220]\n","Step 2222    [0.658 sec/step, loss=0.65968, avg_loss=0.67186]\n","Step 2223    [0.659 sec/step, loss=0.67969, avg_loss=0.67184]\n","Step 2224    [0.661 sec/step, loss=0.68923, avg_loss=0.67204]\n","Step 2225    [0.662 sec/step, loss=0.67565, avg_loss=0.67177]\n","Step 2226    [0.661 sec/step, loss=0.64652, avg_loss=0.67147]\n","Step 2227    [0.662 sec/step, loss=0.68842, avg_loss=0.67178]\n","Step 2228    [0.667 sec/step, loss=0.65345, avg_loss=0.67183]\n","Step 2229    [0.671 sec/step, loss=0.67979, avg_loss=0.67184]\n","Generated 16 batches of size 16 in 2.147 sec\n","Step 2230    [0.671 sec/step, loss=0.63917, avg_loss=0.67118]\n","Step 2231    [0.668 sec/step, loss=0.68501, avg_loss=0.67123]\n","Step 2232    [0.666 sec/step, loss=0.68296, avg_loss=0.67120]\n","Step 2233    [0.662 sec/step, loss=0.70227, avg_loss=0.67144]\n","Step 2234    [0.662 sec/step, loss=0.69952, avg_loss=0.67152]\n","Step 2235    [0.661 sec/step, loss=0.64408, avg_loss=0.67125]\n","Step 2236    [0.661 sec/step, loss=0.69615, avg_loss=0.67156]\n","Step 2237    [0.662 sec/step, loss=0.63395, avg_loss=0.67124]\n","Step 2238    [0.661 sec/step, loss=0.65874, avg_loss=0.67093]\n","Step 2239    [0.662 sec/step, loss=0.68962, avg_loss=0.67069]\n","Step 2240    [0.661 sec/step, loss=0.67248, avg_loss=0.67058]\n","Step 2241    [0.664 sec/step, loss=0.68778, avg_loss=0.67065]\n","Step 2242    [0.666 sec/step, loss=0.67566, avg_loss=0.67065]\n","Step 2243    [0.667 sec/step, loss=0.65497, avg_loss=0.67025]\n","Step 2244    [0.670 sec/step, loss=0.64642, avg_loss=0.66996]\n","Generated 16 batches of size 16 in 2.004 sec\n","Step 2245    [0.676 sec/step, loss=0.68777, avg_loss=0.67000]\n","Step 2246    [0.678 sec/step, loss=0.66720, avg_loss=0.67016]\n","Step 2247    [0.679 sec/step, loss=0.66423, avg_loss=0.67049]\n","Step 2248    [0.676 sec/step, loss=0.72384, avg_loss=0.67109]\n","Step 2249    [0.672 sec/step, loss=0.70142, avg_loss=0.67170]\n","Step 2250    [0.670 sec/step, loss=0.66615, avg_loss=0.67162]\n","Step 2251    [0.670 sec/step, loss=0.65027, avg_loss=0.67154]\n","Step 2252    [0.668 sec/step, loss=0.65657, avg_loss=0.67128]\n","Step 2253    [0.669 sec/step, loss=0.68114, avg_loss=0.67107]\n","Step 2254    [0.673 sec/step, loss=0.69266, avg_loss=0.67110]\n","Step 2255    [0.668 sec/step, loss=0.64179, avg_loss=0.67058]\n","Step 2256    [0.670 sec/step, loss=0.72022, avg_loss=0.67102]\n","Step 2257    [0.670 sec/step, loss=0.64641, avg_loss=0.67070]\n","Step 2258    [0.672 sec/step, loss=0.66084, avg_loss=0.67068]\n","Step 2259    [0.671 sec/step, loss=0.69870, avg_loss=0.67070]\n","Step 2260    [0.671 sec/step, loss=0.65968, avg_loss=0.67058]\n","Step 2261    [0.674 sec/step, loss=0.65073, avg_loss=0.67048]\n","Generated 16 batches of size 16 in 2.018 sec\n","Step 2262    [0.675 sec/step, loss=0.68272, avg_loss=0.67050]\n","Step 2263    [0.676 sec/step, loss=0.68285, avg_loss=0.67075]\n","Step 2264    [0.670 sec/step, loss=0.67041, avg_loss=0.67103]\n","Step 2265    [0.664 sec/step, loss=0.65827, avg_loss=0.67085]\n","Step 2266    [0.664 sec/step, loss=0.64509, avg_loss=0.67033]\n","Step 2267    [0.664 sec/step, loss=0.67193, avg_loss=0.67033]\n","Step 2268    [0.664 sec/step, loss=0.67856, avg_loss=0.67071]\n","Step 2269    [0.664 sec/step, loss=0.62790, avg_loss=0.67052]\n","Step 2270    [0.666 sec/step, loss=0.67000, avg_loss=0.67057]\n","Step 2271    [0.670 sec/step, loss=0.70301, avg_loss=0.67090]\n","Step 2272    [0.668 sec/step, loss=0.66576, avg_loss=0.67080]\n","Step 2273    [0.664 sec/step, loss=0.69239, avg_loss=0.67074]\n","Step 2274    [0.665 sec/step, loss=0.69850, avg_loss=0.67137]\n","Step 2275    [0.666 sec/step, loss=0.66893, avg_loss=0.67102]\n","Step 2276    [0.670 sec/step, loss=0.64412, avg_loss=0.67077]\n","Step 2277    [0.675 sec/step, loss=0.67891, avg_loss=0.67073]\n","Generated 16 batches of size 16 in 2.224 sec\n","Step 2278    [0.675 sec/step, loss=0.64213, avg_loss=0.67046]\n","Step 2279    [0.673 sec/step, loss=0.65374, avg_loss=0.67001]\n","Step 2280    [0.670 sec/step, loss=0.66500, avg_loss=0.66992]\n","Step 2281    [0.664 sec/step, loss=0.68634, avg_loss=0.66992]\n","Step 2282    [0.664 sec/step, loss=0.66269, avg_loss=0.67003]\n","Step 2283    [0.665 sec/step, loss=0.67497, avg_loss=0.67038]\n","Step 2284    [0.664 sec/step, loss=0.63710, avg_loss=0.67004]\n","Step 2285    [0.663 sec/step, loss=0.67922, avg_loss=0.67014]\n","Step 2286    [0.663 sec/step, loss=0.65813, avg_loss=0.67014]\n","Step 2287    [0.663 sec/step, loss=0.64707, avg_loss=0.67002]\n","Step 2288    [0.664 sec/step, loss=0.66451, avg_loss=0.67003]\n","Step 2289    [0.664 sec/step, loss=0.65646, avg_loss=0.66967]\n","Step 2290    [0.664 sec/step, loss=0.66069, avg_loss=0.66960]\n","Step 2291    [0.664 sec/step, loss=0.64327, avg_loss=0.66965]\n","Step 2292    [0.668 sec/step, loss=0.65512, avg_loss=0.66948]\n","Step 2293    [0.673 sec/step, loss=0.64830, avg_loss=0.66955]\n","Generated 16 batches of size 16 in 2.182 sec\n","Step 2294    [0.676 sec/step, loss=0.67003, avg_loss=0.66966]\n","Step 2295    [0.674 sec/step, loss=0.68320, avg_loss=0.66996]\n","Step 2296    [0.668 sec/step, loss=0.64595, avg_loss=0.66949]\n","Step 2297    [0.663 sec/step, loss=0.70193, avg_loss=0.66958]\n","Step 2298    [0.660 sec/step, loss=0.63062, avg_loss=0.66925]\n","Step 2299    [0.662 sec/step, loss=0.69098, avg_loss=0.66949]\n","Step 2300    [0.662 sec/step, loss=0.67739, avg_loss=0.66941]\n","Step 2301    [0.662 sec/step, loss=0.63412, avg_loss=0.66906]\n","Step 2302    [0.661 sec/step, loss=0.64760, avg_loss=0.66855]\n","Step 2303    [0.663 sec/step, loss=0.67135, avg_loss=0.66857]\n","Step 2304    [0.663 sec/step, loss=0.62851, avg_loss=0.66840]\n","Step 2305    [0.663 sec/step, loss=0.67187, avg_loss=0.66857]\n","Step 2306    [0.663 sec/step, loss=0.65026, avg_loss=0.66870]\n","Step 2307    [0.662 sec/step, loss=0.66711, avg_loss=0.66840]\n","Step 2308    [0.668 sec/step, loss=0.66537, avg_loss=0.66831]\n","Generated 16 batches of size 16 in 1.983 sec\n","Step 2309    [0.672 sec/step, loss=0.62890, avg_loss=0.66769]\n","Step 2310    [0.674 sec/step, loss=0.65972, avg_loss=0.66784]\n","Step 2311    [0.673 sec/step, loss=0.65252, avg_loss=0.66767]\n","Step 2312    [0.668 sec/step, loss=0.64253, avg_loss=0.66731]\n","Step 2313    [0.662 sec/step, loss=0.64682, avg_loss=0.66691]\n","Step 2314    [0.665 sec/step, loss=0.67067, avg_loss=0.66686]\n","Step 2315    [0.665 sec/step, loss=0.67157, avg_loss=0.66685]\n","Step 2316    [0.662 sec/step, loss=0.67200, avg_loss=0.66671]\n","Step 2317    [0.663 sec/step, loss=0.64048, avg_loss=0.66667]\n","Step 2318    [0.661 sec/step, loss=0.64071, avg_loss=0.66638]\n","Step 2319    [0.661 sec/step, loss=0.63429, avg_loss=0.66600]\n","Step 2320    [0.662 sec/step, loss=0.65101, avg_loss=0.66581]\n","Step 2321    [0.662 sec/step, loss=0.64003, avg_loss=0.66572]\n","Step 2322    [0.663 sec/step, loss=0.64864, avg_loss=0.66561]\n","Step 2323    [0.662 sec/step, loss=0.66087, avg_loss=0.66543]\n","Step 2324    [0.668 sec/step, loss=0.66929, avg_loss=0.66523]\n","Generated 16 batches of size 16 in 2.170 sec\n","Step 2325    [0.669 sec/step, loss=0.62220, avg_loss=0.66469]\n","Step 2326    [0.669 sec/step, loss=0.61856, avg_loss=0.66441]\n","Step 2327    [0.669 sec/step, loss=0.65063, avg_loss=0.66403]\n","Step 2328    [0.667 sec/step, loss=0.65791, avg_loss=0.66408]\n","Step 2329    [0.662 sec/step, loss=0.66184, avg_loss=0.66390]\n","Step 2330    [0.666 sec/step, loss=0.67726, avg_loss=0.66428]\n","Step 2331    [0.666 sec/step, loss=0.61200, avg_loss=0.66355]\n","Step 2332    [0.665 sec/step, loss=0.65888, avg_loss=0.66331]\n","Step 2333    [0.664 sec/step, loss=0.59407, avg_loss=0.66223]\n","Step 2334    [0.660 sec/step, loss=0.66370, avg_loss=0.66187]\n","Step 2335    [0.661 sec/step, loss=0.67589, avg_loss=0.66219]\n","Step 2336    [0.662 sec/step, loss=0.65088, avg_loss=0.66174]\n","Step 2337    [0.661 sec/step, loss=0.65069, avg_loss=0.66190]\n","Step 2338    [0.661 sec/step, loss=0.63429, avg_loss=0.66166]\n","Step 2339    [0.661 sec/step, loss=0.65926, avg_loss=0.66135]\n","Step 2340    [0.671 sec/step, loss=0.63820, avg_loss=0.66101]\n","Generated 16 batches of size 16 in 2.205 sec\n","Step 2341    [0.670 sec/step, loss=0.64127, avg_loss=0.66055]\n","Step 2342    [0.673 sec/step, loss=0.66217, avg_loss=0.66041]\n","Step 2343    [0.671 sec/step, loss=0.63324, avg_loss=0.66019]\n","Step 2344    [0.667 sec/step, loss=0.64060, avg_loss=0.66014]\n","Step 2345    [0.661 sec/step, loss=0.66723, avg_loss=0.65993]\n","Step 2346    [0.660 sec/step, loss=0.64247, avg_loss=0.65968]\n","Step 2347    [0.660 sec/step, loss=0.63029, avg_loss=0.65934]\n","Step 2348    [0.658 sec/step, loss=0.66087, avg_loss=0.65871]\n","Step 2349    [0.660 sec/step, loss=0.63577, avg_loss=0.65806]\n","Step 2350    [0.660 sec/step, loss=0.67527, avg_loss=0.65815]\n","Step 2351    [0.660 sec/step, loss=0.61380, avg_loss=0.65778]\n","Step 2352    [0.660 sec/step, loss=0.64530, avg_loss=0.65767]\n","Step 2353    [0.658 sec/step, loss=0.62858, avg_loss=0.65715]\n","Step 2354    [0.655 sec/step, loss=0.66513, avg_loss=0.65687]\n","Step 2355    [0.655 sec/step, loss=0.63484, avg_loss=0.65680]\n","Step 2356    [0.660 sec/step, loss=0.65262, avg_loss=0.65613]\n","Step 2357    [0.663 sec/step, loss=0.64067, avg_loss=0.65607]\n","Generated 16 batches of size 16 in 2.215 sec\n","Step 2358    [0.662 sec/step, loss=0.66187, avg_loss=0.65608]\n","Step 2359    [0.665 sec/step, loss=0.65030, avg_loss=0.65559]\n","Step 2360    [0.660 sec/step, loss=0.63332, avg_loss=0.65533]\n","Step 2361    [0.660 sec/step, loss=0.66833, avg_loss=0.65551]\n","Step 2362    [0.659 sec/step, loss=0.64479, avg_loss=0.65513]\n","Step 2363    [0.658 sec/step, loss=0.62073, avg_loss=0.65451]\n","Step 2364    [0.658 sec/step, loss=0.63683, avg_loss=0.65417]\n","Step 2365    [0.658 sec/step, loss=0.62365, avg_loss=0.65382]\n","Step 2366    [0.658 sec/step, loss=0.66739, avg_loss=0.65405]\n","Step 2367    [0.658 sec/step, loss=0.64739, avg_loss=0.65380]\n","Step 2368    [0.659 sec/step, loss=0.62843, avg_loss=0.65330]\n","Step 2369    [0.660 sec/step, loss=0.62893, avg_loss=0.65331]\n","Step 2370    [0.659 sec/step, loss=0.64593, avg_loss=0.65307]\n","Step 2371    [0.654 sec/step, loss=0.65115, avg_loss=0.65255]\n","Step 2372    [0.667 sec/step, loss=0.66501, avg_loss=0.65254]\n","Generated 16 batches of size 16 in 2.173 sec\n","Step 2373    [0.668 sec/step, loss=0.61061, avg_loss=0.65173]\n","Step 2374    [0.666 sec/step, loss=0.61574, avg_loss=0.65090]\n","Step 2375    [0.667 sec/step, loss=0.66349, avg_loss=0.65084]\n","Step 2376    [0.664 sec/step, loss=0.66383, avg_loss=0.65104]\n","Step 2377    [0.660 sec/step, loss=0.63691, avg_loss=0.65062]\n","Step 2378    [0.659 sec/step, loss=0.64032, avg_loss=0.65060]\n","Step 2379    [0.661 sec/step, loss=0.65336, avg_loss=0.65060]\n","Step 2380    [0.660 sec/step, loss=0.66920, avg_loss=0.65064]\n","Step 2381    [0.659 sec/step, loss=0.62998, avg_loss=0.65008]\n","Step 2382    [0.663 sec/step, loss=0.65839, avg_loss=0.65003]\n","Step 2383    [0.663 sec/step, loss=0.64577, avg_loss=0.64974]\n","Step 2384    [0.664 sec/step, loss=0.64228, avg_loss=0.64979]\n","Step 2385    [0.663 sec/step, loss=0.64058, avg_loss=0.64941]\n","Step 2386    [0.665 sec/step, loss=0.64858, avg_loss=0.64931]\n","Step 2387    [0.666 sec/step, loss=0.65815, avg_loss=0.64942]\n","Step 2388    [0.669 sec/step, loss=0.61840, avg_loss=0.64896]\n","Step 2389    [0.673 sec/step, loss=0.65819, avg_loss=0.64898]\n","Generated 16 batches of size 16 in 2.281 sec\n","Step 2390    [0.674 sec/step, loss=0.64658, avg_loss=0.64884]\n","Step 2391    [0.675 sec/step, loss=0.64307, avg_loss=0.64884]\n","Step 2392    [0.670 sec/step, loss=0.63351, avg_loss=0.64862]\n","Step 2393    [0.666 sec/step, loss=0.61520, avg_loss=0.64829]\n","Step 2394    [0.663 sec/step, loss=0.64413, avg_loss=0.64803]\n","Step 2395    [0.663 sec/step, loss=0.61680, avg_loss=0.64737]\n","Step 2396    [0.664 sec/step, loss=0.64467, avg_loss=0.64735]\n","Step 2397    [0.662 sec/step, loss=0.63904, avg_loss=0.64672]\n","Step 2398    [0.662 sec/step, loss=0.63326, avg_loss=0.64675]\n","Step 2399    [0.662 sec/step, loss=0.67340, avg_loss=0.64658]\n","Step 2400    [0.661 sec/step, loss=0.63120, avg_loss=0.64611]\n","Step 2401    [0.662 sec/step, loss=0.65691, avg_loss=0.64634]\n","Step 2402    [0.662 sec/step, loss=0.61594, avg_loss=0.64602]\n","Step 2403    [0.665 sec/step, loss=0.67095, avg_loss=0.64602]\n","Step 2404    [0.670 sec/step, loss=0.60266, avg_loss=0.64576]\n","Step 2405    [0.675 sec/step, loss=0.63585, avg_loss=0.64540]\n","Generated 16 batches of size 16 in 2.078 sec\n","Step 2406    [0.676 sec/step, loss=0.62921, avg_loss=0.64519]\n","Step 2407    [0.675 sec/step, loss=0.62887, avg_loss=0.64481]\n","Step 2408    [0.670 sec/step, loss=0.63668, avg_loss=0.64452]\n","Step 2409    [0.665 sec/step, loss=0.64699, avg_loss=0.64470]\n","Step 2410    [0.663 sec/step, loss=0.61299, avg_loss=0.64424]\n","Step 2411    [0.664 sec/step, loss=0.64204, avg_loss=0.64413]\n","Step 2412    [0.664 sec/step, loss=0.63521, avg_loss=0.64406]\n","Step 2413    [0.662 sec/step, loss=0.62614, avg_loss=0.64385]\n","Step 2414    [0.658 sec/step, loss=0.61300, avg_loss=0.64327]\n","Step 2415    [0.658 sec/step, loss=0.65560, avg_loss=0.64311]\n","Step 2416    [0.657 sec/step, loss=0.64668, avg_loss=0.64286]\n","Step 2417    [0.657 sec/step, loss=0.61981, avg_loss=0.64265]\n","Step 2418    [0.658 sec/step, loss=0.65774, avg_loss=0.64282]\n","Step 2419    [0.662 sec/step, loss=0.66415, avg_loss=0.64312]\n","Step 2420    [0.669 sec/step, loss=0.66833, avg_loss=0.64330]\n","Generated 16 batches of size 16 in 2.166 sec\n","Step 2421    [0.674 sec/step, loss=0.62372, avg_loss=0.64313]\n","Step 2422    [0.672 sec/step, loss=0.63117, avg_loss=0.64296]\n","Step 2423    [0.674 sec/step, loss=0.64371, avg_loss=0.64279]\n","Step 2424    [0.667 sec/step, loss=0.64231, avg_loss=0.64252]\n","Step 2425    [0.664 sec/step, loss=0.62824, avg_loss=0.64258]\n","Step 2426    [0.666 sec/step, loss=0.62240, avg_loss=0.64262]\n","Step 2427    [0.665 sec/step, loss=0.62144, avg_loss=0.64232]\n","Step 2428    [0.662 sec/step, loss=0.61567, avg_loss=0.64190]\n","Step 2429    [0.662 sec/step, loss=0.61169, avg_loss=0.64140]\n","Step 2430    [0.659 sec/step, loss=0.65391, avg_loss=0.64117]\n","Step 2431    [0.659 sec/step, loss=0.60163, avg_loss=0.64106]\n","Step 2432    [0.659 sec/step, loss=0.62150, avg_loss=0.64069]\n","Step 2433    [0.659 sec/step, loss=0.59092, avg_loss=0.64066]\n","Step 2434    [0.658 sec/step, loss=0.59952, avg_loss=0.64002]\n","Step 2435    [0.658 sec/step, loss=0.63765, avg_loss=0.63963]\n","Step 2436    [0.662 sec/step, loss=0.63605, avg_loss=0.63949]\n","Step 2437    [0.667 sec/step, loss=0.61781, avg_loss=0.63916]\n","Generated 16 batches of size 16 in 2.154 sec\n","Step 2438    [0.668 sec/step, loss=0.64074, avg_loss=0.63922]\n","Step 2439    [0.668 sec/step, loss=0.63029, avg_loss=0.63893]\n","Step 2440    [0.662 sec/step, loss=0.66022, avg_loss=0.63915]\n","Step 2441    [0.659 sec/step, loss=0.64191, avg_loss=0.63916]\n","Step 2442    [0.655 sec/step, loss=0.61920, avg_loss=0.63873]\n","Step 2443    [0.658 sec/step, loss=0.64332, avg_loss=0.63883]\n","Step 2444    [0.659 sec/step, loss=0.62296, avg_loss=0.63865]\n","Step 2445    [0.658 sec/step, loss=0.61560, avg_loss=0.63814]\n","Step 2446    [0.656 sec/step, loss=0.61777, avg_loss=0.63789]\n","Step 2447    [0.658 sec/step, loss=0.60863, avg_loss=0.63767]\n","Step 2448    [0.658 sec/step, loss=0.62002, avg_loss=0.63726]\n","Step 2449    [0.658 sec/step, loss=0.64296, avg_loss=0.63734]\n","Step 2450    [0.657 sec/step, loss=0.60116, avg_loss=0.63660]\n","Step 2451    [0.658 sec/step, loss=0.64130, avg_loss=0.63687]\n","Step 2452    [0.663 sec/step, loss=0.63368, avg_loss=0.63675]\n","Generated 16 batches of size 16 in 2.129 sec\n","Step 2453    [0.669 sec/step, loss=0.64897, avg_loss=0.63696]\n","Step 2454    [0.668 sec/step, loss=0.63580, avg_loss=0.63666]\n","Step 2455    [0.668 sec/step, loss=0.61015, avg_loss=0.63642]\n","Step 2456    [0.661 sec/step, loss=0.59784, avg_loss=0.63587]\n","Step 2457    [0.661 sec/step, loss=0.64036, avg_loss=0.63587]\n","Step 2458    [0.659 sec/step, loss=0.62517, avg_loss=0.63550]\n","Step 2459    [0.657 sec/step, loss=0.63461, avg_loss=0.63534]\n","Step 2460    [0.662 sec/step, loss=0.65644, avg_loss=0.63557]\n","Step 2461    [0.657 sec/step, loss=0.66017, avg_loss=0.63549]\n","Step 2462    [0.657 sec/step, loss=0.66383, avg_loss=0.63568]\n","Step 2463    [0.660 sec/step, loss=0.62636, avg_loss=0.63574]\n","Step 2464    [0.661 sec/step, loss=0.63354, avg_loss=0.63571]\n","Step 2465    [0.665 sec/step, loss=0.65534, avg_loss=0.63602]\n","Step 2466    [0.667 sec/step, loss=0.61390, avg_loss=0.63549]\n","Step 2467    [0.665 sec/step, loss=0.61594, avg_loss=0.63517]\n","Step 2468    [0.668 sec/step, loss=0.60933, avg_loss=0.63498]\n","Step 2469    [0.673 sec/step, loss=0.62034, avg_loss=0.63490]\n","Generated 16 batches of size 16 in 2.164 sec\n","Step 2470    [0.673 sec/step, loss=0.59619, avg_loss=0.63440]\n","Step 2471    [0.674 sec/step, loss=0.64249, avg_loss=0.63431]\n","Step 2472    [0.662 sec/step, loss=0.65509, avg_loss=0.63421]\n","Step 2473    [0.661 sec/step, loss=0.61356, avg_loss=0.63424]\n","Step 2474    [0.661 sec/step, loss=0.59553, avg_loss=0.63404]\n","Step 2475    [0.661 sec/step, loss=0.63615, avg_loss=0.63377]\n","Step 2476    [0.660 sec/step, loss=0.63556, avg_loss=0.63349]\n","Step 2477    [0.660 sec/step, loss=0.65597, avg_loss=0.63368]\n","Step 2478    [0.661 sec/step, loss=0.60743, avg_loss=0.63335]\n","Step 2479    [0.659 sec/step, loss=0.62977, avg_loss=0.63311]\n","Step 2480    [0.663 sec/step, loss=0.67456, avg_loss=0.63316]\n","Step 2481    [0.663 sec/step, loss=0.61852, avg_loss=0.63305]\n","Step 2482    [0.659 sec/step, loss=0.62278, avg_loss=0.63269]\n","Step 2483    [0.658 sec/step, loss=0.61171, avg_loss=0.63235]\n","Step 2484    [0.664 sec/step, loss=0.66633, avg_loss=0.63259]\n","Generated 16 batches of size 16 in 2.174 sec\n","Step 2485    [0.670 sec/step, loss=0.63940, avg_loss=0.63258]\n","Step 2486    [0.668 sec/step, loss=0.63956, avg_loss=0.63249]\n","Step 2487    [0.668 sec/step, loss=0.65607, avg_loss=0.63247]\n","Step 2488    [0.665 sec/step, loss=0.63284, avg_loss=0.63262]\n","Step 2489    [0.661 sec/step, loss=0.61923, avg_loss=0.63223]\n","Step 2490    [0.659 sec/step, loss=0.62590, avg_loss=0.63202]\n","Step 2491    [0.659 sec/step, loss=0.63069, avg_loss=0.63189]\n","Step 2492    [0.658 sec/step, loss=0.59290, avg_loss=0.63149]\n","Step 2493    [0.661 sec/step, loss=0.65350, avg_loss=0.63187]\n","Step 2494    [0.660 sec/step, loss=0.61054, avg_loss=0.63154]\n","Step 2495    [0.661 sec/step, loss=0.64224, avg_loss=0.63179]\n","Step 2496    [0.660 sec/step, loss=0.62913, avg_loss=0.63163]\n","Step 2497    [0.658 sec/step, loss=0.60841, avg_loss=0.63133]\n","Step 2498    [0.658 sec/step, loss=0.61710, avg_loss=0.63117]\n","Step 2499    [0.662 sec/step, loss=0.66554, avg_loss=0.63109]\n","Step 2500    [0.670 sec/step, loss=0.62166, avg_loss=0.63099]\n","Saving audio and alignment...\n","Generated 16 batches of size 16 in 1.878 sec\n","  0% 0/1 [00:00<?, ?it/s]Training korean : Use jamo\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12634 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51068 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44288 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46104 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44172 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52628 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44396 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54664 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45912 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46972 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51077 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12634 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51068 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44288 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46104 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44172 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52628 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44396 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54664 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45912 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46972 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51077 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/train-step-000002500-align000.png\n","100% 1/1 [00:02<00:00,  2.15s/it]\n","Test finished for step 2500.\n","  0% 0/2 [00:00<?, ?it/s]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/test-step-000002500-align000.png\n"," 50% 1/2 [00:02<00:02,  2.17s/it]Training korean : Use jamo\n"," [*] Plot saved: logdir-tacotron/moon_2021-02-03_23-08-12/test-step-000002500-align001.png\n","100% 2/2 [00:04<00:00,  2.18s/it]\n","Test finished for step 2500.\n","Step 2501    [0.670 sec/step, loss=0.62365, avg_loss=0.63066]\n","Step 2502    [0.670 sec/step, loss=0.63460, avg_loss=0.63085]\n","Step 2503    [0.666 sec/step, loss=0.60626, avg_loss=0.63020]\n","Step 2504    [0.662 sec/step, loss=0.64588, avg_loss=0.63063]\n","Step 2505    [0.658 sec/step, loss=0.65967, avg_loss=0.63087]\n","Step 2506    [0.658 sec/step, loss=0.64692, avg_loss=0.63105]\n","Step 2507    [0.658 sec/step, loss=0.60866, avg_loss=0.63085]\n","Step 2508    [0.657 sec/step, loss=0.63275, avg_loss=0.63081]\n","Step 2509    [0.657 sec/step, loss=0.58899, avg_loss=0.63023]\n","Step 2510    [0.662 sec/step, loss=0.66020, avg_loss=0.63070]\n","Step 2511    [0.661 sec/step, loss=0.62989, avg_loss=0.63058]\n","Step 2512    [0.662 sec/step, loss=0.63068, avg_loss=0.63053]\n","Step 2513    [0.663 sec/step, loss=0.63697, avg_loss=0.63064]\n","Step 2514    [0.662 sec/step, loss=0.61851, avg_loss=0.63069]\n","Step 2515    [0.665 sec/step, loss=0.61139, avg_loss=0.63025]\n","Step 2516    [0.672 sec/step, loss=0.62188, avg_loss=0.63000]\n","Generated 16 batches of size 16 in 2.178 sec\n","Step 2517    [0.672 sec/step, loss=0.62726, avg_loss=0.63008]\n","Step 2518    [0.674 sec/step, loss=0.63544, avg_loss=0.62986]\n","Step 2519    [0.670 sec/step, loss=0.61676, avg_loss=0.62938]\n","Step 2520    [0.664 sec/step, loss=0.64608, avg_loss=0.62916]\n","Step 2521    [0.660 sec/step, loss=0.63506, avg_loss=0.62927]\n","Step 2522    [0.660 sec/step, loss=0.59929, avg_loss=0.62895]\n","Step 2523    [0.657 sec/step, loss=0.62280, avg_loss=0.62875]\n","Step 2524    [0.658 sec/step, loss=0.60972, avg_loss=0.62842]\n","Step 2525    [0.662 sec/step, loss=0.64728, avg_loss=0.62861]\n","Step 2526    [0.661 sec/step, loss=0.61814, avg_loss=0.62857]\n","Step 2527    [0.662 sec/step, loss=0.61488, avg_loss=0.62850]\n","Step 2528    [0.662 sec/step, loss=0.59263, avg_loss=0.62827]\n","Step 2529    [0.662 sec/step, loss=0.60874, avg_loss=0.62824]\n","Step 2530    [0.663 sec/step, loss=0.61934, avg_loss=0.62790]\n","Step 2531    [0.666 sec/step, loss=0.60703, avg_loss=0.62795]\n","Step 2532    [0.671 sec/step, loss=0.64364, avg_loss=0.62817]\n","Generated 16 batches of size 16 in 2.354 sec\n","Step 2533    [0.673 sec/step, loss=0.60795, avg_loss=0.62834]\n","Step 2534    [0.675 sec/step, loss=0.63932, avg_loss=0.62874]\n","Step 2535    [0.673 sec/step, loss=0.61190, avg_loss=0.62848]\n","Step 2536    [0.668 sec/step, loss=0.59856, avg_loss=0.62811]\n","Step 2537    [0.664 sec/step, loss=0.62829, avg_loss=0.62821]\n","Step 2538    [0.664 sec/step, loss=0.63492, avg_loss=0.62815]\n","Step 2539    [0.664 sec/step, loss=0.61350, avg_loss=0.62799]\n","Step 2540    [0.660 sec/step, loss=0.61885, avg_loss=0.62757]\n","Step 2541    [0.660 sec/step, loss=0.58560, avg_loss=0.62701]\n","Step 2542    [0.662 sec/step, loss=0.62753, avg_loss=0.62709]\n","Step 2543    [0.659 sec/step, loss=0.61371, avg_loss=0.62680]\n","Step 2544    [0.658 sec/step, loss=0.62870, avg_loss=0.62685]\n","Step 2545    [0.659 sec/step, loss=0.63018, avg_loss=0.62700]\n","Step 2546    [0.664 sec/step, loss=0.64772, avg_loss=0.62730]\n","Step 2547    [0.668 sec/step, loss=0.62558, avg_loss=0.62747]\n","Step 2548    [0.671 sec/step, loss=0.60109, avg_loss=0.62728]\n","Generated 16 batches of size 16 in 2.275 sec\n","Step 2549    [0.672 sec/step, loss=0.62151, avg_loss=0.62706]\n","Step 2550    [0.672 sec/step, loss=0.58921, avg_loss=0.62695]\n","Step 2551    [0.673 sec/step, loss=0.60847, avg_loss=0.62662]\n","Step 2552    [0.668 sec/step, loss=0.63615, avg_loss=0.62664]\n","Step 2553    [0.662 sec/step, loss=0.59310, avg_loss=0.62608]\n","Step 2554    [0.662 sec/step, loss=0.60703, avg_loss=0.62579]\n","Step 2555    [0.662 sec/step, loss=0.60773, avg_loss=0.62577]\n","Step 2556    [0.663 sec/step, loss=0.64355, avg_loss=0.62623]\n","Step 2557    [0.665 sec/step, loss=0.64468, avg_loss=0.62627]\n","Step 2558    [0.665 sec/step, loss=0.60152, avg_loss=0.62603]\n","Step 2559    [0.664 sec/step, loss=0.60790, avg_loss=0.62577]\n","Step 2560    [0.660 sec/step, loss=0.60701, avg_loss=0.62527]\n","Step 2561    [0.660 sec/step, loss=0.63646, avg_loss=0.62504]\n","Step 2562    [0.660 sec/step, loss=0.63284, avg_loss=0.62473]\n","Step 2563    [0.666 sec/step, loss=0.65513, avg_loss=0.62501]\n","Generated 16 batches of size 16 in 2.132 sec\n","Step 2564    [0.669 sec/step, loss=0.64063, avg_loss=0.62508]\n","Step 2565    [0.666 sec/step, loss=0.61668, avg_loss=0.62470]\n","Step 2566    [0.664 sec/step, loss=0.59081, avg_loss=0.62447]\n","Step 2567    [0.666 sec/step, loss=0.61345, avg_loss=0.62444]\n","Step 2568    [0.664 sec/step, loss=0.62550, avg_loss=0.62460]\n","Step 2569    [0.660 sec/step, loss=0.62065, avg_loss=0.62461]\n","Step 2570    [0.659 sec/step, loss=0.60367, avg_loss=0.62468]\n","Step 2571    [0.658 sec/step, loss=0.59320, avg_loss=0.62419]\n","Step 2572    [0.656 sec/step, loss=0.63540, avg_loss=0.62399]\n","Step 2573    [0.656 sec/step, loss=0.62243, avg_loss=0.62408]\n","Step 2574    [0.657 sec/step, loss=0.59551, avg_loss=0.62408]\n","Step 2575    [0.656 sec/step, loss=0.59435, avg_loss=0.62366]\n","Step 2576    [0.656 sec/step, loss=0.62633, avg_loss=0.62357]\n","Step 2577    [0.658 sec/step, loss=0.62963, avg_loss=0.62331]\n","Step 2578    [0.657 sec/step, loss=0.59059, avg_loss=0.62314]\n","Step 2579    [0.663 sec/step, loss=0.62212, avg_loss=0.62306]\n","Generated 16 batches of size 16 in 2.238 sec\n","Step 2580    [0.668 sec/step, loss=0.63275, avg_loss=0.62264]\n","Step 2581    [0.668 sec/step, loss=0.57960, avg_loss=0.62225]\n","Step 2582    [0.668 sec/step, loss=0.65275, avg_loss=0.62255]\n","Step 2583    [0.668 sec/step, loss=0.59463, avg_loss=0.62238]\n","Step 2584    [0.662 sec/step, loss=0.61140, avg_loss=0.62183]\n","Step 2585    [0.656 sec/step, loss=0.63306, avg_loss=0.62177]\n","Step 2586    [0.657 sec/step, loss=0.63403, avg_loss=0.62172]\n","Step 2587    [0.657 sec/step, loss=0.61952, avg_loss=0.62135]\n","Step 2588    [0.656 sec/step, loss=0.60301, avg_loss=0.62105]\n","Step 2589    [0.656 sec/step, loss=0.61397, avg_loss=0.62100]\n","Step 2590    [0.657 sec/step, loss=0.59125, avg_loss=0.62065]\n","Step 2591    [0.656 sec/step, loss=0.58659, avg_loss=0.62021]\n","Step 2592    [0.657 sec/step, loss=0.64665, avg_loss=0.62075]\n","Step 2593    [0.655 sec/step, loss=0.65854, avg_loss=0.62080]\n","Step 2594    [0.657 sec/step, loss=0.60211, avg_loss=0.62072]\n","Step 2595    [0.662 sec/step, loss=0.62876, avg_loss=0.62058]\n","Generated 16 batches of size 16 in 2.083 sec\n","Step 2596    [0.667 sec/step, loss=0.65168, avg_loss=0.62081]\n","Step 2597    [0.668 sec/step, loss=0.60773, avg_loss=0.62080]\n","Step 2598    [0.667 sec/step, loss=0.60860, avg_loss=0.62071]\n","Step 2599    [0.663 sec/step, loss=0.61701, avg_loss=0.62023]\n","Step 2600    [0.655 sec/step, loss=0.62831, avg_loss=0.62030]\n","Step 2601    [0.654 sec/step, loss=0.61053, avg_loss=0.62016]\n","Step 2602    [0.656 sec/step, loss=0.61930, avg_loss=0.62001]\n","Step 2603    [0.661 sec/step, loss=0.64750, avg_loss=0.62042]\n","Step 2604    [0.660 sec/step, loss=0.58654, avg_loss=0.61983]\n","Step 2605    [0.658 sec/step, loss=0.60121, avg_loss=0.61925]\n","Step 2606    [0.660 sec/step, loss=0.61531, avg_loss=0.61893]\n","Step 2607    [0.660 sec/step, loss=0.62214, avg_loss=0.61906]\n","Step 2608    [0.660 sec/step, loss=0.60941, avg_loss=0.61883]\n","Step 2609    [0.661 sec/step, loss=0.59424, avg_loss=0.61888]\n","Step 2610    [0.657 sec/step, loss=0.64636, avg_loss=0.61875]\n","Step 2611    [0.661 sec/step, loss=0.63900, avg_loss=0.61884]\n","Step 2612    [0.666 sec/step, loss=0.63750, avg_loss=0.61890]\n","Generated 16 batches of size 16 in 2.156 sec\n","Step 2613    [0.665 sec/step, loss=0.60356, avg_loss=0.61857]\n","Step 2614    [0.670 sec/step, loss=0.64641, avg_loss=0.61885]\n","Step 2615    [0.667 sec/step, loss=0.64700, avg_loss=0.61921]\n","Step 2616    [0.661 sec/step, loss=0.62700, avg_loss=0.61926]\n","Step 2617    [0.661 sec/step, loss=0.59917, avg_loss=0.61898]\n","Step 2618    [0.659 sec/step, loss=0.61503, avg_loss=0.61877]\n","Step 2619    [0.660 sec/step, loss=0.63710, avg_loss=0.61898]\n","Step 2620    [0.659 sec/step, loss=0.59268, avg_loss=0.61844]\n","Step 2621    [0.659 sec/step, loss=0.62575, avg_loss=0.61835]\n","Step 2622    [0.663 sec/step, loss=0.64597, avg_loss=0.61882]\n","Step 2623    [0.666 sec/step, loss=0.63389, avg_loss=0.61893]\n","Step 2624    [0.665 sec/step, loss=0.60558, avg_loss=0.61888]\n","Step 2625    [0.660 sec/step, loss=0.60513, avg_loss=0.61846]\n","Step 2626    [0.659 sec/step, loss=0.62467, avg_loss=0.61853]\n","Step 2627    [0.663 sec/step, loss=0.61086, avg_loss=0.61849]\n","Generated 16 batches of size 16 in 2.024 sec\n","Step 2628    [0.670 sec/step, loss=0.65370, avg_loss=0.61910]\n","Step 2629    [0.671 sec/step, loss=0.61587, avg_loss=0.61917]\n","Step 2630    [0.669 sec/step, loss=0.65266, avg_loss=0.61950]\n","Step 2631    [0.668 sec/step, loss=0.66520, avg_loss=0.62009]\n","Step 2632    [0.662 sec/step, loss=0.63630, avg_loss=0.62001]\n","Step 2633    [0.661 sec/step, loss=0.63959, avg_loss=0.62033]\n","Step 2634    [0.660 sec/step, loss=0.61265, avg_loss=0.62006]\n","Step 2635    [0.660 sec/step, loss=0.60490, avg_loss=0.61999]\n","Step 2636    [0.662 sec/step, loss=0.67045, avg_loss=0.62071]\n","Step 2637    [0.662 sec/step, loss=0.60318, avg_loss=0.62046]\n","Step 2638    [0.666 sec/step, loss=0.64772, avg_loss=0.62059]\n","Step 2639    [0.665 sec/step, loss=0.59256, avg_loss=0.62038]\n","Step 2640    [0.665 sec/step, loss=0.65068, avg_loss=0.62070]\n","Step 2641    [0.665 sec/step, loss=0.59936, avg_loss=0.62083]\n","Step 2642    [0.663 sec/step, loss=0.62274, avg_loss=0.62079]\n","Step 2643    [0.667 sec/step, loss=0.64465, avg_loss=0.62110]\n","Step 2644    [0.672 sec/step, loss=0.62882, avg_loss=0.62110]\n","Generated 16 batches of size 16 in 2.256 sec\n","Step 2645    [0.673 sec/step, loss=0.58857, avg_loss=0.62068]\n","Step 2646    [0.668 sec/step, loss=0.60768, avg_loss=0.62028]\n","Step 2647    [0.664 sec/step, loss=0.61057, avg_loss=0.62013]\n","Step 2648    [0.663 sec/step, loss=0.62495, avg_loss=0.62037]\n","Step 2649    [0.661 sec/step, loss=0.62240, avg_loss=0.62038]\n","Step 2650    [0.661 sec/step, loss=0.59598, avg_loss=0.62045]\n","Step 2651    [0.661 sec/step, loss=0.64714, avg_loss=0.62083]\n","Step 2652    [0.659 sec/step, loss=0.63562, avg_loss=0.62083]\n","Step 2653    [0.660 sec/step, loss=0.60874, avg_loss=0.62098]\n","Step 2654    [0.660 sec/step, loss=0.59251, avg_loss=0.62084]\n","Step 2655    [0.660 sec/step, loss=0.60793, avg_loss=0.62084]\n","Step 2656    [0.661 sec/step, loss=0.62468, avg_loss=0.62065]\n","Step 2657    [0.658 sec/step, loss=0.60525, avg_loss=0.62026]\n","Step 2658    [0.658 sec/step, loss=0.60086, avg_loss=0.62025]\n","Step 2659    [0.666 sec/step, loss=0.62237, avg_loss=0.62039]\n","Generated 16 batches of size 16 in 2.129 sec\n","Step 2660    [0.671 sec/step, loss=0.59483, avg_loss=0.62027]\n","Step 2661    [0.675 sec/step, loss=0.64294, avg_loss=0.62034]\n","Step 2662    [0.674 sec/step, loss=0.59511, avg_loss=0.61996]\n","Step 2663    [0.667 sec/step, loss=0.61872, avg_loss=0.61960]\n","Step 2664    [0.663 sec/step, loss=0.62172, avg_loss=0.61941]\n","Step 2665    [0.663 sec/step, loss=0.58645, avg_loss=0.61910]\n","Step 2666    [0.663 sec/step, loss=0.62523, avg_loss=0.61945]\n","Step 2667    [0.662 sec/step, loss=0.61219, avg_loss=0.61944]\n","Step 2668    [0.664 sec/step, loss=0.63460, avg_loss=0.61953]\n","Step 2669    [0.664 sec/step, loss=0.61616, avg_loss=0.61948]\n","Step 2670    [0.665 sec/step, loss=0.60673, avg_loss=0.61951]\n","Step 2671    [0.665 sec/step, loss=0.58398, avg_loss=0.61942]\n","Step 2672    [0.666 sec/step, loss=0.59304, avg_loss=0.61900]\n","Step 2673    [0.664 sec/step, loss=0.60105, avg_loss=0.61878]\n","Step 2674    [0.665 sec/step, loss=0.60667, avg_loss=0.61890]\n","Step 2675    [0.669 sec/step, loss=0.63061, avg_loss=0.61926]\n","Step 2676    [0.672 sec/step, loss=0.60707, avg_loss=0.61907]\n","Generated 16 batches of size 16 in 2.305 sec\n","Step 2677    [0.674 sec/step, loss=0.65291, avg_loss=0.61930]\n","Step 2678    [0.675 sec/step, loss=0.63673, avg_loss=0.61976]\n","Step 2679    [0.669 sec/step, loss=0.63068, avg_loss=0.61984]\n","Step 2680    [0.659 sec/step, loss=0.59446, avg_loss=0.61946]\n","Step 2681    [0.660 sec/step, loss=0.59611, avg_loss=0.61963]\n","Step 2682    [0.661 sec/step, loss=0.59948, avg_loss=0.61909]\n","Step 2683    [0.664 sec/step, loss=0.60924, avg_loss=0.61924]\n","Step 2684    [0.663 sec/step, loss=0.58907, avg_loss=0.61902]\n","Step 2685    [0.663 sec/step, loss=0.61782, avg_loss=0.61886]\n","Step 2686    [0.663 sec/step, loss=0.59959, avg_loss=0.61852]\n","Step 2687    [0.662 sec/step, loss=0.63144, avg_loss=0.61864]\n","Step 2688    [0.664 sec/step, loss=0.60695, avg_loss=0.61868]\n","Step 2689    [0.665 sec/step, loss=0.61753, avg_loss=0.61871]\n","Step 2690    [0.667 sec/step, loss=0.59302, avg_loss=0.61873]\n","Step 2691    [0.671 sec/step, loss=0.59915, avg_loss=0.61886]\n","Step 2692    [0.674 sec/step, loss=0.57433, avg_loss=0.61813]\n","Generated 16 batches of size 16 in 2.112 sec\n","Step 2693    [0.676 sec/step, loss=0.62431, avg_loss=0.61779]\n","Step 2694    [0.675 sec/step, loss=0.60947, avg_loss=0.61787]\n","Step 2695    [0.669 sec/step, loss=0.58320, avg_loss=0.61741]\n","Step 2696    [0.668 sec/step, loss=0.63776, avg_loss=0.61727]\n","Step 2697    [0.668 sec/step, loss=0.62380, avg_loss=0.61743]\n","Step 2698    [0.668 sec/step, loss=0.58741, avg_loss=0.61722]\n","Step 2699    [0.667 sec/step, loss=0.58514, avg_loss=0.61690]\n","Step 2700    [0.671 sec/step, loss=0.61985, avg_loss=0.61682]\n","Step 2701    [0.672 sec/step, loss=0.61662, avg_loss=0.61688]\n","Step 2702    [0.669 sec/step, loss=0.57186, avg_loss=0.61640]\n","Step 2703    [0.665 sec/step, loss=0.62971, avg_loss=0.61622]\n","Step 2704    [0.666 sec/step, loss=0.57631, avg_loss=0.61612]\n","Step 2705    [0.667 sec/step, loss=0.62740, avg_loss=0.61638]\n","Step 2706    [0.667 sec/step, loss=0.60728, avg_loss=0.61630]\n","Step 2707    [0.670 sec/step, loss=0.59928, avg_loss=0.61608]\n","Generated 16 batches of size 16 in 2.021 sec\n","Step 2708    [0.677 sec/step, loss=0.61098, avg_loss=0.61609]\n","Step 2709    [0.679 sec/step, loss=0.57610, avg_loss=0.61591]\n","Step 2710    [0.678 sec/step, loss=0.56780, avg_loss=0.61512]\n","Step 2711    [0.674 sec/step, loss=0.61741, avg_loss=0.61491]\n","Step 2712    [0.669 sec/step, loss=0.59939, avg_loss=0.61453]\n","Step 2713    [0.669 sec/step, loss=0.61890, avg_loss=0.61468]\n","Step 2714    [0.664 sec/step, loss=0.57043, avg_loss=0.61392]\n","Step 2715    [0.664 sec/step, loss=0.59394, avg_loss=0.61339]\n","Step 2716    [0.663 sec/step, loss=0.59922, avg_loss=0.61311]\n","Step 2717    [0.664 sec/step, loss=0.60815, avg_loss=0.61320]\n","Step 2718    [0.664 sec/step, loss=0.58070, avg_loss=0.61286]\n","Step 2719    [0.663 sec/step, loss=0.59528, avg_loss=0.61244]\n","Step 2720    [0.663 sec/step, loss=0.55837, avg_loss=0.61210]\n","Step 2721    [0.663 sec/step, loss=0.62780, avg_loss=0.61212]\n","Step 2722    [0.662 sec/step, loss=0.60693, avg_loss=0.61173]\n","Step 2723    [0.667 sec/step, loss=0.58389, avg_loss=0.61123]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJQUMUfB5SYn","executionInfo":{"status":"ok","timestamp":1612393171270,"user_tz":-540,"elapsed":25198,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"c9918d39-8880-461a-bd3e-b925b05f2ec4"},"source":["!python synthesizer.py --load_path logdir-tacotron/moon_2021-02-03_23-08-12 --num_speakers 1 --speaker_id 0 --text \"고려대 이성환 교수님 연구실 화이팅!\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From synthesizer.py:27: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n","\n","WARNING:tensorflow:From synthesizer.py:27: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n","\n"," [*] Found lastest checkpoint: logdir-tacotron/moon_2021-02-02_13-18-57/model.ckpt-130000\n","Constructing model: tacotron\n","========================================\n"," model_type: single\n","========================================\n","Initialized Tacotron model. Dimensions: \n","    embedding:                256\n","    speaker embedding:        None\n","    prenet out:               128\n","    encoder out:              256\n","    attention out:            256\n","    concat attn & out:        512\n","    decoder cell out:         256\n","    decoder out (5 frames):  400\n","    decoder out (1 frame):    80\n","    postnet out:              256\n","    linear out:               1025\n","Loading checkpoint: logdir-tacotron/moon_2021-02-02_13-18-57/model.ckpt-130000\n","2021-02-03 22:59:19.384712: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n","2021-02-03 22:59:19.384885: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7798700 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-02-03 22:59:19.384916: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-02-03 22:59:19.388825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-02-03 22:59:19.526855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 22:59:19.527576: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x77988c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-02-03 22:59:19.527611: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2021-02-03 22:59:19.527840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 22:59:19.528398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n","pciBusID: 0000:00:04.0\n","2021-02-03 22:59:19.530987: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-02-03 22:59:19.533044: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-02-03 22:59:19.535115: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-02-03 22:59:19.535774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-02-03 22:59:19.537731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-02-03 22:59:19.538585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-02-03 22:59:19.541986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-02-03 22:59:19.542113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 22:59:19.542679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 22:59:19.543163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-02-03 22:59:19.543229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-02-03 22:59:19.544486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-02-03 22:59:19.544513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-02-03 22:59:19.544525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-02-03 22:59:19.544646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 22:59:19.545211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-02-03 22:59:19.545739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n","2021-02-03 22:59:22.628258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-02-03 22:59:22.982375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","plot_graph_and_save_audio:   0% 0/1 [00:00<?, ?it/s]findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12593 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12631 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12601 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12629 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12599 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12624 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12615 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12643 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12613 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12627 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12622 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12632 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12596 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12635 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12636 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12609 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12642 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12620 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12610 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 12623 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47140 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45824 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49457 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54872 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44368 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49688 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45784 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50672 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44396 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49892 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51032 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44608 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54788 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50864 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54868 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54021 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51077 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45768 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54728 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12593 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12631 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12601 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12629 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12599 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12624 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12615 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12643 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12613 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12627 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12622 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12632 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12596 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12635 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12636 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12609 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12642 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12620 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12610 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 12623 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47140 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45824 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49457 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54872 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44368 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49688 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45784 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50672 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44396 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49892 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51032 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44608 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54788 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50864 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54868 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54021 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51077 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45768 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54728 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"," [*] Plot saved: logdir-tacotron/generate/2021-02-03_22-59-26.png\n","plot_graph_and_save_audio: 100% 1/1 [00:02<00:00,  2.14s/it]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7es5VwjEEBp","executionInfo":{"status":"ok","timestamp":1612309578740,"user_tz":-540,"elapsed":17202,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfnXgOWxG2tX-vPqbxzfqqeNW5PunWL6suf4s7=s64","userId":"14544536004560997153"}},"outputId":"887c00ac-b1c2-4d90-b2c5-1fbfeee4e8c0"},"source":["!python synthesizer.py --load_path logdir-tacotron/moon+son_2018-12-25_19-03-21 --num_speakers 2 --speaker_id 0 --text \"고려대 인공지능 대학원 이성환 교수님 연구실 김현우 화이팅.\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From synthesizer.py:27: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n","\n","WARNING:tensorflow:From synthesizer.py:27: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n","\n","Constructing model: tacotron\n","Traceback (most recent call last):\n","  File \"synthesizer.py\", line 385, in <module>\n","    synthesizer.load(config.load_path, config.num_speakers, config.checkpoint_step)\n","  File \"synthesizer.py\", line 52, in load\n","    load_hparams(hparams, load_path)\n","  File \"/content/drive/My Drive/AI대학원/tacotron1/utils/__init__.py\", line 160, in load_hparams\n","    new_hparams = load_json(path)\n","  File \"/content/drive/My Drive/AI대학원/tacotron1/utils/__init__.py\", line 174, in load_json\n","    with open(path,encoding=encoding) as f:\n","FileNotFoundError: [Errno 2] No such file or directory: 'logdir-tacotron/params.json'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U5O6oC-_I4B6"},"source":[""],"execution_count":null,"outputs":[]}]}